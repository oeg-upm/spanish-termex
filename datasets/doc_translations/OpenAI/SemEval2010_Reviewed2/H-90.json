{
    "id": "H-90",
    "original_text": "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1. INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents. From a single query, however, the retrieval system can only have very limited clue about the users information need. An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available. Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2]. There are many kinds of context that we can exploit. Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy. However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents. Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information. Thus the effectiveness of relevance feedback may be limited in real applications. For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12]. In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8]. For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied. In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading). We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results. A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort. For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia. As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island. However, any particular user is unlikely searching for both types of documents. Such an ambiguity can be resolved by exploiting history information. For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for. Implicit feedback was studied in several previous works. In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people. In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated. In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user. Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6]. While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval. Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy. We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information. We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model. One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation. We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models. To the best of our knowledge, this is the first test set for implicit feedback. We evaluate the proposed models using this data set. The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user. The remaining sections are organized as follows. In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later. In Section 3, we propose several implicit feedback models based on statistical language models. In Section 4, we describe how we create the data set for implicit feedback experiments. In Section 5, we evaluate different implicit feedback models on the created data set. Section 6 is our conclusions and future work. 2. PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback. One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session. A session can be considered as a period consisting of all interactions for the same information need. The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context. Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search. In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session. The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time. Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session. In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context. In a single search session, a user may interact with the search system several times. During interactions, the user would continuously modify the query. Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session. Note that we assume that the session boundaries are known in this paper. In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16]. Traditionally, the retrieval system only uses the current query Qk to do retrieval. But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section. Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy. In addition to the query history, there may be other short-term context information available. For example, a user would presumably frequently click some documents to view. We refer to data associated with these actions as clickthrough history. The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document. Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need. Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval. In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1. We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk. Previous work has also shown positive results using similar clickthrough information [11, 17]. Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them. In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3. LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk. An important research question is how we can exploit such information effectively. We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method. According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document. One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model. Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk. Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history. Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally. Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC . We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k). We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text. We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ). Then we linearly interpolate these two history models to obtain the history model p(w|H). Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk). These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information. If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries. But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history. To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator. The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ). We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length. Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±. Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries. This means that all previous queries are treated equally and so are all clicked summaries. However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better. Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable. Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries. Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way. In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents. In order to rank documents, the system must have some model for the users information need. In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query. A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general. Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary). To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior. We use Dirichlet prior because it is a conjugate prior for multinomial distributions. With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation. In general, we assume that the retrieval system maintains a current query model Ï†i at any moment. As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model. Initially, before we see any user query, we may already have some information about the user. For example, we may have some information about what documents the user has viewed in the past. We use such information to define a prior on the query model, which is denoted by Ï†0. After we observe the first query Q1, we can update the query model based on the new observed data Q1. The updated query model Ï†1 can then be used for ranking documents in response to Q1. As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1. As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2. In general, we may repeat such an updating process to iteratively update the query model. Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci. In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data. Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary. If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci). On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model. Thus the model remains the same as if we do not observe any new text evidence. In general, the parameters Âµi and Î½i may have different values for different i. For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi. But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents. This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen. To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight. This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document. One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries. The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries. As in OnlineUp, we set all Âµis and Î½is to the same value. And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4. DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic. Since there is no such data set available to us, we have to create one. There are two choices. One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine). But the problem is that we have no relevance judgments on such data. The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file. Unfortunately, there are no query history and clickthrough history data. We decide to augment a TREC data set by collecting query history and clickthrough history data. We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments. There are altogether 242918 news articles and the average document length is 416 words. Most articles have titles. If not, we select the first sentence of the text as the title. For the preprocessing, we only do case folding and do not do stopword removal or stemming. We select 30 relatively difficult topics from TREC topics 1-150. These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20]. The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user. In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well. We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles. We use 3 subjects to do experiments to collect query history and clickthrough history data. Each subject is assigned 10 topics and given the topic descriptions provided by TREC. For each topic, the first query is the title of the topic given in the original TREC topic description. After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject. The subject will browse the results and maybe click one or more results to browse the full text of article(s). The subject may also modify the query to do another search. For each topic, the subject composes at least 4 queries. In our experiment, only the first 4 queries for each topic are used. The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study. We use a relational database to store user interactions, including the submitted queries and clicked documents. For each query, we store the query terms and the associated result pages. And for each clicked document, we store the summary as shown on the search result page. The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing). Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words. Altogether there are 91 documents clicked to view. So on average, there are around 3 clicks per topic. The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words. Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file. This data set is publicly available 1 . 5. EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy. In particular, the search context can provide extra information to help us estimate a better query model than using just the current query. So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context. Since we collected four versions of queries for each topic, we make such comparisons for each version of queries. We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents. In all cases, the reported figure is the average over all of the 30 topics. We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp). Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others). Note that Âµ and Î½ may need to be interpreted differently for different methods. We vary these parameters and identify the optimal performance for each method. We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1. A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context. We can make several observations from this table: 1. Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best. Users generally formulate better and better queries. 2. Using search context generally has positive effect, especially when the context is rich. This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2. Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse. When the search context is rich, the performance improvement can be quite substantial. For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3. Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp. Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense. The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries. Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying. While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient. Overall, BatchUp appears to be the best method when we vary the parameter settings. We have two different kinds of search context - query history and clickthrough data. We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately. This allows us to evaluate the effect of using query history alone. We use the same parameter setting for query history as in Table 1. The results are shown in Table 2. Here we see that in general, the benefit of using query history is very limited with mixed results. This is different from what is reported in a previous study [15], where using query history is consistently helpful. Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4. This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations. Yet another observation is that when using query history only, the BayesInt model appears to be better than other models. Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm. The displayed results thus reflect the variation caused by parameter Âµ. A smaller setting of 2.0 is seen better than a larger value of 5.0. A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ. The value of Âµ can be interpreted as how many words we regard the query history is worth. A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich. Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2. As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information. This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have. The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query. The results are shown in Table 4. We see that the benefit of using clickthrough information is much more significant than that of using query history. We see an overall positive effect, often with significant improvement over the baseline. It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve. Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial. These results show that the clicked summary text is in general quite useful for inferring a users information need. Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant. Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement. However, such improvement is really not beneficial for the user as the user has already seen these relevant documents. To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file. The results are shown in Table 5. Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results. From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly. Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant. To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4. The results are shown in Table 6. We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries. These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs. In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5. FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking. Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information. In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model. In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others. BatchUp has two parameters Âµ and Î½. We first look at Âµ. When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query. If we increase Âµ, we will gradually incorporate more information from the previous queries. In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved. We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2. The pattern is also similar when we set Î½ to other values. In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4. The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query. Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone. We now turn to the other parameter Î½. When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query. With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9. We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15. This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable. Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6. CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance. Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp. We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models. Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort. The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information. It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history. For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query. Second, the proposed models can be implemented in any practical systems. We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms. We will also do a user study to evaluate effectiveness of these models in the real web search. Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7. ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472. We thank the anonymous reviewers for their useful comments. 8. REFERENCES [1] E. Adar and D. Karger. Haystack: Per-user information environments. In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al. Challenges in information retrieval and language modeling. Workshop at University of Amherst, 2002. [3] K. Bharat. Searchpad: Explicit capture of search context to support web search. In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko. Relevance feedback and personalization: A language modeling perspective. In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit queries (IQ) for contextualized search (demo description). In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search in context: The concept revisited. In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang. Query session based term suggestion for interactive web search. In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic web log session identification with statistical language models. Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom. Scaling personalized web search. In Proceeding of WWW 2003, 2003. [11] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin. Display time as implicit feedback: Understanding task effects. In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan. Implicit feedback for inferring user preference. SIGIR Forum, 32(2), 2003. [14] J. Rocchio. Relevance feedback information retrieval. In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen and C. Zhai. Exploiting query history for document ranking in interactive information retrieval (poster). In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai. A session-based search engine (poster). In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based on user profile constructed without any effort from users. In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven. A simulated study of implicit feedback models. In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty. Model-based feedback in the KL-divergence retrieval model. In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad-hoc information retrieval. In Proceedings of SIGIR 2001, 2001.",
    "original_translation": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001.",
    "original_sentences": [
        "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
        "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
        "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
        "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
        "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
        "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
        "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
        "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
        "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
        "There are many kinds of context that we can exploit.",
        "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
        "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
        "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
        "Thus the effectiveness of relevance feedback may be limited in real applications.",
        "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
        "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
        "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
        "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
        "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
        "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
        "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
        "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
        "However, any particular user is unlikely searching for both types of documents.",
        "Such an ambiguity can be resolved by exploiting history information.",
        "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
        "Implicit feedback was studied in several previous works.",
        "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
        "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
        "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
        "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
        "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
        "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
        "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
        "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
        "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
        "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
        "To the best of our knowledge, this is the first test set for implicit feedback.",
        "We evaluate the proposed models using this data set.",
        "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
        "The remaining sections are organized as follows.",
        "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
        "In Section 3, we propose several implicit feedback models based on statistical language models.",
        "In Section 4, we describe how we create the data set for implicit feedback experiments.",
        "In Section 5, we evaluate different implicit feedback models on the created data set.",
        "Section 6 is our conclusions and future work. 2.",
        "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
        "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
        "A session can be considered as a period consisting of all interactions for the same information need.",
        "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
        "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
        "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
        "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
        "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
        "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
        "In a single search session, a user may interact with the search system several times.",
        "During interactions, the user would continuously modify the query.",
        "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
        "Note that we assume that the session boundaries are known in this paper.",
        "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
        "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
        "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
        "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
        "In addition to the query history, there may be other short-term context information available.",
        "For example, a user would presumably frequently click some documents to view.",
        "We refer to data associated with these actions as clickthrough history.",
        "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
        "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
        "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
        "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
        "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
        "Previous work has also shown positive results using similar clickthrough information [11, 17].",
        "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
        "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
        "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
        "An important research question is how we can exploit such information effectively.",
        "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
        "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
        "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
        "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
        "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
        "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
        "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
        "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
        "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
        "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
        "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
        "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
        "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
        "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
        "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
        "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
        "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
        "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
        "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
        "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
        "This means that all previous queries are treated equally and so are all clicked summaries.",
        "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
        "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
        "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
        "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
        "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
        "In order to rank documents, the system must have some model for the users information need.",
        "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
        "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
        "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
        "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
        "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
        "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
        "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
        "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
        "Initially, before we see any user query, we may already have some information about the user.",
        "For example, we may have some information about what documents the user has viewed in the past.",
        "We use such information to define a prior on the query model, which is denoted by Ï†0.",
        "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
        "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
        "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
        "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
        "In general, we may repeat such an updating process to iteratively update the query model.",
        "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
        "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
        "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
        "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
        "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
        "Thus the model remains the same as if we do not observe any new text evidence.",
        "In general, the parameters Âµi and Î½i may have different values for different i.",
        "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
        "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
        "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
        "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
        "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
        "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
        "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
        "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
        "As in OnlineUp, we set all Âµis and Î½is to the same value.",
        "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
        "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
        "Since there is no such data set available to us, we have to create one.",
        "There are two choices.",
        "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
        "But the problem is that we have no relevance judgments on such data.",
        "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
        "Unfortunately, there are no query history and clickthrough history data.",
        "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
        "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
        "There are altogether 242918 news articles and the average document length is 416 words.",
        "Most articles have titles.",
        "If not, we select the first sentence of the text as the title.",
        "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
        "We select 30 relatively difficult topics from TREC topics 1-150.",
        "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
        "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
        "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
        "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
        "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
        "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
        "For each topic, the first query is the title of the topic given in the original TREC topic description.",
        "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
        "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
        "The subject may also modify the query to do another search.",
        "For each topic, the subject composes at least 4 queries.",
        "In our experiment, only the first 4 queries for each topic are used.",
        "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
        "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
        "For each query, we store the query terms and the associated result pages.",
        "And for each clicked document, we store the summary as shown on the search result page.",
        "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
        "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
        "Altogether there are 91 documents clicked to view.",
        "So on average, there are around 3 clicks per topic.",
        "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
        "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
        "This data set is publicly available 1 . 5.",
        "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
        "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
        "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
        "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
        "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
        "In all cases, the reported figure is the average over all of the 30 topics.",
        "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
        "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
        "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
        "We vary these parameters and identify the optimal performance for each method.",
        "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
        "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
        "We can make several observations from this table: 1.",
        "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
        "Users generally formulate better and better queries. 2.",
        "Using search context generally has positive effect, especially when the context is rich.",
        "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
        "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
        "When the search context is rich, the performance improvement can be quite substantial.",
        "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
        "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
        "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
        "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
        "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
        "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
        "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
        "We have two different kinds of search context - query history and clickthrough data.",
        "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
        "This allows us to evaluate the effect of using query history alone.",
        "We use the same parameter setting for query history as in Table 1.",
        "The results are shown in Table 2.",
        "Here we see that in general, the benefit of using query history is very limited with mixed results.",
        "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
        "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
        "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
        "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
        "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
        "The displayed results thus reflect the variation caused by parameter Âµ.",
        "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
        "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
        "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
        "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
        "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
        "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
        "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
        "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
        "The results are shown in Table 4.",
        "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
        "We see an overall positive effect, often with significant improvement over the baseline.",
        "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
        "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
        "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
        "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
        "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
        "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
        "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
        "The results are shown in Table 5.",
        "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
        "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
        "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
        "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
        "The results are shown in Table 6.",
        "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
        "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
        "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
        "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
        "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
        "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
        "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
        "BatchUp has two parameters Âµ and Î½.",
        "We first look at Âµ.",
        "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
        "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
        "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
        "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
        "The pattern is also similar when we set Î½ to other values.",
        "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
        "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
        "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
        "We now turn to the other parameter Î½.",
        "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
        "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
        "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
        "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
        "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
        "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
        "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
        "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
        "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
        "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
        "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
        "Second, the proposed models can be implemented in any practical systems.",
        "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
        "We will also do a user study to evaluate effectiveness of these models in the real web search.",
        "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
        "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
        "We thank the anonymous reviewers for their useful comments. 8.",
        "REFERENCES [1] E. Adar and D. Karger.",
        "Haystack: Per-user information environments.",
        "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
        "Challenges in information retrieval and language modeling.",
        "Workshop at University of Amherst, 2002. [3] K. Bharat.",
        "Searchpad: Explicit capture of search context to support web search.",
        "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
        "Relevance feedback and personalization: A language modeling perspective.",
        "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
        "Nie, and W.-Y.",
        "Ma.",
        "Probabilistic query expansion using query logs.",
        "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
        "Implicit queries (IQ) for contextualized search (demo description).",
        "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
        "Placing search in context: The concept revisited.",
        "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
        "Query session based term suggestion for interactive web search.",
        "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
        "An, and D. Schuurmans.",
        "Dynamic web log session identification with statistical language models.",
        "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
        "Scaling personalized web search.",
        "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
        "Display time as implicit feedback: Understanding task effects.",
        "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
        "Implicit feedback for inferring user preference.",
        "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
        "Relevance feedback information retrieval.",
        "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
        "Prentice-Hall. [15] X. Shen and C. Zhai.",
        "Exploiting query history for document ranking in interactive information retrieval (poster).",
        "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
        "A session-based search engine (poster).",
        "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
        "Adaptive web search based on user profile constructed without any effort from users.",
        "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
        "A simulated study of implicit feedback models.",
        "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
        "Model-based feedback in the KL-divergence retrieval model.",
        "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
        "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
        "In Proceedings of SIGIR 2001, 2001."
    ],
    "translated_text_sentences": [
        "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida.",
        "En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva.",
        "Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos.",
        "Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas.",
        "Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n.",
        "CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1.",
        "En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos.",
        "A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario.",
        "Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible.",
        "De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2].",
        "Hay muchos tipos de contexto que podemos aprovechar.",
        "La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n.",
        "Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes.",
        "Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n.",
        "Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales.",
        "Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12].",
        "En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8].",
        "Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha.",
        "En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo).",
        "Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n.",
        "Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario.",
        "Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia.",
        "Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla.",
        "Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos.",
        "Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica.",
        "Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando.",
        "El feedback implÃ­cito fue estudiado en varios trabajos previos.",
        "En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas.",
        "En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics.",
        "En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario.",
        "Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6].",
        "Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n.",
        "EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n.",
        "Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda.",
        "Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL.",
        "Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n.",
        "Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita.",
        "Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita.",
        "Evaluamos los modelos propuestos utilizando este conjunto de datos.",
        "Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario.",
        "Las secciones restantes estÃ¡n organizadas de la siguiente manera.",
        "En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante.",
        "En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje.",
        "En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita.",
        "En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado.",
        "La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2.",
        "DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita.",
        "Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n.",
        "Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n.",
        "La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo.",
        "Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual.",
        "En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente.",
        "El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo.",
        "El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular.",
        "En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo.",
        "En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces.",
        "Durante las interacciones, el usuario modificarÃ­a continuamente la consulta.",
        "Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual.",
        "Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos.",
        "En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16].",
        "Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n.",
        "Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior.",
        "De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n.",
        "AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible.",
        "Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos.",
        "Nos referimos a los datos asociados con estas acciones como historial de clics.",
        "Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic.",
        "Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario.",
        "Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n.",
        "En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1.",
        "TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk.",
        "Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17].",
        "Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos.",
        "En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos.",
        "Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk.",
        "Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva.",
        "Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n.",
        "SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento.",
        "Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta.",
        "Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk.",
        "Sea HC = (C1, ..., Ckâˆ’1) el historial de clics.",
        "Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual.",
        "Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC.",
        "Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k).",
        "Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto.",
        "Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC).",
        "Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H).",
        "Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk).",
        "Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica.",
        "Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas.",
        "Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial.",
        "Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano.",
        "El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ).",
        "Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta.",
        "De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta.",
        "MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados.",
        "Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados.",
        "Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s.",
        "Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable.",
        "Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores.",
        "Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general.",
        "En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos.",
        "Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios.",
        "En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva).",
        "Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general.",
        "Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado).",
        "Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior.",
        "Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales.",
        "Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana.",
        "En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento.",
        "Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta.",
        "Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario.",
        "Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado.",
        "Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0.",
        "DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1.",
        "El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1.",
        "A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1.",
        "Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2.",
        "En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta.",
        "Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci.",
        "En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados.",
        "AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado.",
        "Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci).",
        "Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo.",
        "Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual.",
        "En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i.",
        "Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande.",
        "Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
        "Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos.",
        "Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto.",
        "Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo.",
        "Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic.",
        "Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores.",
        "Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados.",
        "En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor.",
        "Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4.",
        "RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema.",
        "Dado que no contamos con un conjunto de datos disponible, debemos crear uno.",
        "Hay dos opciones.",
        "Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda).",
        "Pero el problema es que no tenemos juicios de relevancia sobre esos datos.",
        "La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia.",
        "Lamentablemente, no hay datos de historial de consultas e historial de clics.",
        "Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics.",
        "Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos.",
        "Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras.",
        "La mayorÃ­a de los artÃ­culos tienen tÃ­tulos.",
        "Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo.",
        "Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming.",
        "Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150.",
        "Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20].",
        "La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario.",
        "En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real.",
        "Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP.",
        "Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics.",
        "A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC.",
        "Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC.",
        "DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto.",
        "El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos.",
        "El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda.",
        "Para cada tema, el sujeto compone al menos 4 consultas.",
        "En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema.",
        "El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio.",
        "Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados.",
        "Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas.",
        "Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda.",
        "El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano).",
        "De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras.",
        "En total hay 91 documentos clicados para ver.",
        "En promedio, hay alrededor de 3 clics por tema.",
        "La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras.",
        "De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC.",
        "Este conjunto de datos estÃ¡ disponible pÃºblicamente.",
        "EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda.",
        "En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual.",
        "Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda.",
        "Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas.",
        "Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos.",
        "En todos los casos, la cifra reportada es el promedio de los 30 temas.",
        "Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp).",
        "Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s).",
        "Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos.",
        "Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo.",
        "TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1.",
        "Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda.",
        "Podemos hacer varias observaciones a partir de esta tabla: 1.",
        "Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor.",
        "Los usuarios generalmente formulan consultas cada vez mejores.",
        "El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico.",
        "Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2.",
        "De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso.",
        "Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial.",
        "Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3.",
        "Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp.",
        "Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido.",
        "La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados.",
        "Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo.",
        "Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento.",
        "En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros.",
        "Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics.",
        "Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente.",
        "Esto nos permite evaluar el efecto de utilizar solo el historial de consultas.",
        "Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1.",
        "Los resultados se muestran en la Tabla 2.",
        "AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos.",
        "Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil.",
        "Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4.",
        "Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores.",
        "Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos.",
        "Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo.",
        "Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ.",
        "Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0.",
        "Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ.",
        "El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta.",
        "Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante.",
        "Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2.",
        "Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica.",
        "Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos.",
        "Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual.",
        "Los resultados se muestran en la Tabla 4.",
        "Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas.",
        "Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida.",
        "TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados.",
        "Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial.",
        "Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario.",
        "Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes.",
        "Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento.",
        "Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes.",
        "Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio.",
        "Los resultados se muestran en la Tabla 5.",
        "Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados.",
        "Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos.",
        "La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante.",
        "Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4.",
        "Los resultados se muestran en la Tabla 6.",
        "Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas.",
        "Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes.",
        "En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5.",
        "Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos.",
        "El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics.",
        "En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro.",
        "En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros.",
        "BatchUp tiene dos parÃ¡metros Âµ y Î½.",
        "Primero miramos a Âµ.",
        "Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual.",
        "Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores.",
        "En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp.",
        "Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2.",
        "El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores.",
        "AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4.",
        "El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual.",
        "Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado.",
        "Ahora pasamos al otro parÃ¡metro Î½.",
        "Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual.",
        "Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9.",
        "Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15.",
        "Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa.",
        "En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6.",
        "CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n.",
        "Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp.",
        "Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita.",
        "Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario.",
        "El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita.",
        "SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics.",
        "Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior.",
        "Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico.",
        "Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos.",
        "TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real.",
        "Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7.",
        "AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472.",
        "Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8.",
        "REFERENCIAS [1] E. Adar y D. Karger.",
        "Haystack: Entornos de informaciÃ³n por usuario.",
        "En Actas de CIKM 1999, 1999. [2] J. Allan y et al.",
        "DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje.",
        "Taller en la Universidad de Amherst, 2002. [3] K. Bharat.",
        "Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web.",
        "En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko.",
        "RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje.",
        "En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
        "Nie, and W.-Y. -> Nie, y W.-Y.",
        "This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish.",
        "ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta.",
        "En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz.",
        "Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n).",
        "En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin.",
        "Colocando la bÃºsqueda en contexto: El concepto revisado.",
        "En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang.",
        "Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web.",
        "En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
        "An, y D. Schuurmans.",
        "IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico.",
        "Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom.",
        "Escalando la bÃºsqueda web personalizada.",
        "En Actas de WWW 2003, 2003. [11] T. Joachims.",
        "OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics.",
        "En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin.",
        "Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea.",
        "En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan.",
        "RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario.",
        "Foro SIGIR, 32(2), 2003. [14] J. Rocchio.",
        "RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia.",
        "En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971.",
        "Prentice-Hall. [15] X. Shen y C. Zhai.",
        "Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster).",
        "En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai.",
        "Un motor de bÃºsqueda basado en sesiones (pÃ³ster).",
        "En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa.",
        "BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios.",
        "En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven.",
        "Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita.",
        "En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty.",
        "RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler.",
        "En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty.",
        "Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc.",
        "En Actas de SIGIR 2001, 2001."
    ],
    "error_count": 1,
    "keys": {
        "retrieval accuracy": {
            "translated_key": "precisiÃ³n de recuperaciÃ³n",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve <br>retrieval accuracy</br> in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve <br>retrieval accuracy</br>, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving <br>retrieval accuracy</br>.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the <br>retrieval accuracy</br> without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve <br>retrieval accuracy</br>.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving <br>retrieval accuracy</br>.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve <br>retrieval accuracy</br> in an interactive information retrieval setting.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve <br>retrieval accuracy</br>, whenever it is available.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving <br>retrieval accuracy</br>.",
                "A major advantage of implicit feedback is that we can improve the <br>retrieval accuracy</br> without requiring any user effort.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve <br>retrieval accuracy</br>."
            ],
            "translated_annotated_samples": [
                "En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la <br>precisiÃ³n de recuperaciÃ³n</br> en un entorno de recuperaciÃ³n de informaciÃ³n interactiva.",
                "Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la <br>precisiÃ³n de la recuperaciÃ³n</br>, siempre que estÃ© disponible.",
                "La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la <br>precisiÃ³n de recuperaciÃ³n</br>.",
                "Una ventaja importante del feedback implÃ­cito es que podemos mejorar la <br>precisiÃ³n de recuperaciÃ³n</br> sin necesidad de requerir esfuerzo por parte del usuario.",
                "EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la <br>precisiÃ³n de recuperaciÃ³n</br>."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la <br>precisiÃ³n de recuperaciÃ³n</br> en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la <br>precisiÃ³n de la recuperaciÃ³n</br>, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la <br>precisiÃ³n de recuperaciÃ³n</br>. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la <br>precisiÃ³n de recuperaciÃ³n</br> sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la <br>precisiÃ³n de recuperaciÃ³n</br>. ",
            "candidates": [],
            "error": [
                [
                    "precisiÃ³n de recuperaciÃ³n",
                    "precisiÃ³n de la recuperaciÃ³n",
                    "precisiÃ³n de recuperaciÃ³n",
                    "precisiÃ³n de recuperaciÃ³n",
                    "precisiÃ³n de recuperaciÃ³n"
                ]
            ]
        },
        "implicit feedback information": {
            "translated_key": "informaciÃ³n de retroalimentaciÃ³n implÃ­cita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit <br>implicit feedback information</br>, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such <br>implicit feedback information</br> can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using <br>implicit feedback information</br> such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with <br>implicit feedback information</br>, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using <br>implicit feedback information</br>, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are <br>implicit feedback information</br>, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using <br>implicit feedback information</br> may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit <br>implicit feedback information</br>, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating <br>implicit feedback information</br>.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "In this paper, we study how to exploit <br>implicit feedback information</br>, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such <br>implicit feedback information</br> can indeed improve the search accuracy for a group of people.",
                "Specifically, we develop models for using <br>implicit feedback information</br> such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We thus use the TREC AP data to create a test collection with <br>implicit feedback information</br>, which can be used to quantitatively evaluate implicit feedback models.",
                "The experimental results show that using <br>implicit feedback information</br>, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user."
            ],
            "translated_annotated_samples": [
                "En este artÃ­culo, estudiamos cÃ³mo aprovechar la <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva.",
                "En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br> puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas.",
                "EspecÃ­ficamente, desarrollamos modelos para utilizar <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n.",
                "Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita.",
                "Los resultados experimentales muestran que el uso de <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br> puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de <br>informaciÃ³n de retroalimentaciÃ³n implÃ­cita</br>, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "relevance feedback": {
            "translated_key": "retroalimentaciÃ³n de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "<br>relevance feedback</br> [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, <br>relevance feedback</br> requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of <br>relevance feedback</br> may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "<br>relevance feedback</br> and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "<br>relevance feedback</br> information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "<br>relevance feedback</br> [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, <br>relevance feedback</br> requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Thus the effectiveness of <br>relevance feedback</br> may be limited in real applications.",
                "<br>relevance feedback</br> and personalization: A language modeling perspective.",
                "<br>relevance feedback</br> information retrieval."
            ],
            "translated_annotated_samples": [
                "La <br>retroalimentaciÃ³n de relevancia</br> [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n.",
                "Sin embargo, la <br>retroalimentaciÃ³n de relevancia</br> requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes.",
                "Por lo tanto, la efectividad de la <br>retroalimentaciÃ³n de relevancia</br> puede estar limitada en aplicaciones reales.",
                "RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje.",
                "RecuperaciÃ³n de informaciÃ³n con <br>retroalimentaciÃ³n de relevancia</br>."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La <br>retroalimentaciÃ³n de relevancia</br> [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la <br>retroalimentaciÃ³n de relevancia</br> requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la <br>retroalimentaciÃ³n de relevancia</br> puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con <br>retroalimentaciÃ³n de relevancia</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interactive retrieval": {
            "translated_key": "recuperaciÃ³n interactiva",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an <br>interactive retrieval</br> scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an <br>interactive retrieval</br> system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an <br>interactive retrieval</br> process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "In such an <br>interactive retrieval</br> scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an <br>interactive retrieval</br> system, we present it in a more general way.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an <br>interactive retrieval</br> process using Bayesian estimation."
            ],
            "translated_annotated_samples": [
                "En un escenario de <br>recuperaciÃ³n interactiva</br>, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo).",
                "Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de <br>recuperaciÃ³n interactivo</br>, la presentamos de una manera mÃ¡s general.",
                "Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un <br>proceso interactivo</br> de recuperaciÃ³n utilizando estimaciÃ³n bayesiana."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de <br>recuperaciÃ³n interactiva</br>, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de <br>recuperaciÃ³n interactivo</br>, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un <br>proceso interactivo</br> de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    "recuperaciÃ³n interactiva",
                    "recuperaciÃ³n interactivo",
                    "proceso interactivo"
                ]
            ]
        },
        "kl-divergence retrieval model": {
            "translated_key": "modelo de recuperaciÃ³n de divergencia KL",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the <br>kl-divergence retrieval model</br> [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the <br>kl-divergence retrieval model</br> as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the <br>kl-divergence retrieval model</br>.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "We use the <br>kl-divergence retrieval model</br> [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "Using the <br>kl-divergence retrieval model</br> as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "Model-based feedback in the <br>kl-divergence retrieval model</br>."
            ],
            "translated_annotated_samples": [
                "Utilizamos el <br>modelo de recuperaciÃ³n de divergencia KL</br> [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda.",
                "Usando el <br>modelo de recuperaciÃ³n de divergencia KL</br> como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp.",
                "RetroalimentaciÃ³n basada en el modelo en el <br>modelo de recuperaciÃ³n de divergencia de Kullback-Leibler</br>."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el <br>modelo de recuperaciÃ³n de divergencia KL</br> [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el <br>modelo de recuperaciÃ³n de divergencia KL</br> como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el <br>modelo de recuperaciÃ³n de divergencia de Kullback-Leibler</br>. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    "modelo de recuperaciÃ³n de divergencia KL",
                    "modelo de recuperaciÃ³n de divergencia KL",
                    "modelo de recuperaciÃ³n de divergencia de Kullback-Leibler"
                ]
            ]
        },
        "context-sensitive language": {
            "translated_key": "lenguaje sensibles al contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new <br>context-sensitive language</br> models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific <br>context-sensitive language</br> models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new <br>context-sensitive language</br> models for retrieval.",
                "We propose to use statistical language models to model a users information need and develop four specific <br>context-sensitive language</br> models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method."
            ],
            "translated_annotated_samples": [
                "Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de <br>lenguaje sensibles al contexto</br> para la recuperaciÃ³n.",
                "Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de <br>lenguaje sensibles al contexto</br> para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "long-term context": {
            "translated_key": "contexto a largo plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is <br>long-term context</br>, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "<br>long-term context</br> can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some <br>long-term context</br>.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "The other kind of context is <br>long-term context</br>, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "<br>long-term context</br> can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some <br>long-term context</br>."
            ],
            "translated_annotated_samples": [
                "El otro tipo de contexto es el <br>contexto a largo plazo</br>, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo.",
                "El <br>contexto a largo plazo</br> puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular.",
                "En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto <br>contexto a largo plazo</br>."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el <br>contexto a largo plazo</br>, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El <br>contexto a largo plazo</br> puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto <br>contexto a largo plazo</br>. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "short-term context": {
            "translated_key": "contexto a corto plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is <br>short-term context</br>, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of <br>short-term context</br>.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, <br>short-term context</br> is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the <br>short-term context</br> in improving search accuracy for a particular session.",
                "In this paper, we focus on the <br>short-term context</br>, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other <br>short-term context</br> information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "One is <br>short-term context</br>, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of <br>short-term context</br>.",
                "In general, <br>short-term context</br> is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the <br>short-term context</br> in improving search accuracy for a particular session.",
                "In this paper, we focus on the <br>short-term context</br>, though some of our methods can also be used to naturally incorporate some long-term context."
            ],
            "translated_annotated_samples": [
                "Uno es el <br>contexto a corto plazo</br>, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n.",
                "La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de <br>contexto a corto plazo</br>.",
                "En general, el <br>contexto a corto plazo</br> es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente.",
                "El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el <br>contexto a corto plazo</br> para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular.",
                "En este artÃ­culo, nos enfocamos en el <br>contexto a corto plazo</br>, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el <br>contexto a corto plazo</br>, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de <br>contexto a corto plazo</br>. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el <br>contexto a corto plazo</br> es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el <br>contexto a corto plazo</br> para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el <br>contexto a corto plazo</br>, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "fixed coefficient interpolation": {
            "translated_key": "InterpolaciÃ³n de Coeficiente Fijo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 <br>fixed coefficient interpolation</br> (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a <br>fixed coefficient interpolation</br> of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 <br>fixed coefficient interpolation</br> (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a <br>fixed coefficient interpolation</br> of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries."
            ],
            "translated_annotated_samples": [
                "Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 <br>InterpolaciÃ³n de Coeficiente Fijo</br> (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC).",
                "Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una <br>interpolaciÃ³n de coeficiente fijo</br> de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 <br>InterpolaciÃ³n de Coeficiente Fijo</br> (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una <br>interpolaciÃ³n de coeficiente fijo</br> de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "bayesian estimation": {
            "translated_key": "estimaciÃ³n bayesiana",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use <br>bayesian estimation</br>, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply <br>bayesian estimation</br> to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using <br>bayesian estimation</br>.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "A principled way of updating the query model is to use <br>bayesian estimation</br>, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply <br>bayesian estimation</br> to update a query model in general.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using <br>bayesian estimation</br>."
            ],
            "translated_annotated_samples": [
                "Una forma fundamentada de actualizar el modelo de consulta es utilizar la <br>estimaciÃ³n bayesiana</br>, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la <br>estimaciÃ³n bayesiana</br> para actualizar un modelo de consulta en general.",
                "Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando <br>estimaciÃ³n bayesiana</br>."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la <br>estimaciÃ³n bayesiana</br>, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la <br>estimaciÃ³n bayesiana</br> para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando <br>estimaciÃ³n bayesiana</br>. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "trec datum set": {
            "translated_key": "conjunto de datos trec",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "mean average precision": {
            "translated_key": "PrecisiÃ³n Promedio Media",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) <br>mean average precision</br> (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the <br>mean average precision</br> over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "We use two performance measures: (1) <br>mean average precision</br> (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "For example, BatchUp achieves 92.4% improvement in the <br>mean average precision</br> over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3."
            ],
            "translated_annotated_samples": [
                "Utilizamos dos medidas de rendimiento: (1) <br>PrecisiÃ³n Promedio Media</br> (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos.",
                "Por ejemplo, BatchUp logra una mejora del 92.4% en la <br>precisiÃ³n media promedio</br> sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) <br>PrecisiÃ³n Promedio Media</br> (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la <br>precisiÃ³n media promedio</br> sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    "PrecisiÃ³n Promedio Media",
                    "precisiÃ³n media promedio"
                ]
            ]
        },
        "query history information": {
            "translated_key": "informaciÃ³n del historial de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more <br>query history information</br>, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the <br>query history information</br> and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more <br>query history information</br>, we can put more and more weight on the history information.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the <br>query history information</br> and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information."
            ],
            "translated_annotated_samples": [
                "Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s <br>informaciÃ³n del historial de consultas</br>, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica.",
                "El beneficio de la <br>informaciÃ³n del historial de consultas</br> y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s <br>informaciÃ³n del historial de consultas</br>, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la <br>informaciÃ³n del historial de consultas</br> y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "current query": {
            "translated_key": "consulta actual",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the <br>current query</br> for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the <br>current query</br> is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the <br>current query</br> and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the <br>current query</br> Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the <br>current query</br> Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the <br>current query</br> Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the <br>current query</br> Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the <br>current query</br> be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the <br>current query</br> Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the <br>current query</br> model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the <br>current query</br> and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our <br>current query</br> Qk is very long, we should trust the <br>current query</br> more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our <br>current query</br> model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a <br>current query</br> model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the <br>current query</br> Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the <br>current query</br>.",
                "So most of our experiments involve comparing the retrieval performance using the <br>current query</br> only (thus ignoring any context) with that using the <br>current query</br> as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the <br>current query</br> only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the <br>current query</br>.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the <br>current query</br>.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the <br>current query</br>.",
                "Except for q2, there is clearly some tradeoff between the <br>current query</br> and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the <br>current query</br>.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the <br>current query</br> is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the <br>current query</br> is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the <br>current query</br> for better ranking of documents.",
                "For example, if the <br>current query</br> is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the <br>current query</br> and any search context information.",
                "Therefore for the <br>current query</br> Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Traditionally, the retrieval system only uses the <br>current query</br> Qk to do retrieval."
            ],
            "translated_annotated_samples": [
                "Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la <br>consulta actual</br> para una mejor clasificaciÃ³n de documentos.",
                "Por ejemplo, si la <br>consulta actual</br> es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia.",
                "Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la <br>consulta actual</br> y cualquier informaciÃ³n de contexto de bÃºsqueda.",
                "Por lo tanto, para la <br>consulta actual</br> Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual.",
                "Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la <br>consulta actual</br> Qk para realizar la recuperaciÃ³n."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la <br>consulta actual</br> para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la <br>consulta actual</br> es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la <br>consulta actual</br> y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la <br>consulta actual</br> Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la <br>consulta actual</br> Qk para realizar la recuperaciÃ³n. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "clickthrough information": {
            "translated_key": "informaciÃ³n de clics",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and <br>clickthrough information</br>, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the <br>clickthrough information</br> and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting <br>clickthrough information</br> were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using <br>clickthrough information</br>, in this paper, we use both <br>clickthrough information</br> and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user <br>clickthrough information</br>; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar <br>clickthrough information</br> [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the <br>clickthrough information</br>, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and <br>clickthrough information</br>) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using <br>clickthrough information</br> is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of <br>clickthrough information</br> are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the <br>clickthrough information</br>.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the <br>clickthrough information</br> is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "In this paper, we study how to exploit implicit feedback information, including previous queries and <br>clickthrough information</br>, to improve retrieval accuracy in an interactive information retrieval setting.",
                "In [11], Joachims explored how to capture and exploit the <br>clickthrough information</br> and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting <br>clickthrough information</br> were proposed and evaluated.",
                "While the previous work has mostly focused on using <br>clickthrough information</br>, in this paper, we use both <br>clickthrough information</br> and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user <br>clickthrough information</br>; such information is generally stable for a long time and is often accumulated over time."
            ],
            "translated_annotated_samples": [
                "En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la <br>informaciÃ³n de clics</br>, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva.",
                "En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la <br>informaciÃ³n de clics</br> y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas.",
                "En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la <br>informaciÃ³n de clics</br>.",
                "Si bien el trabajo anterior se ha centrado principalmente en el uso de la <br>informaciÃ³n de clics</br>, en este artÃ­culo utilizamos tanto la <br>informaciÃ³n de clics</br> como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n.",
                "El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la <br>informaciÃ³n de clics anteriores</br> de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la <br>informaciÃ³n de clics</br>, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la <br>informaciÃ³n de clics</br> y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la <br>informaciÃ³n de clics</br>. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la <br>informaciÃ³n de clics</br>, en este artÃ­culo utilizamos tanto la <br>informaciÃ³n de clics</br> como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la <br>informaciÃ³n de clics anteriores</br> de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. ",
            "candidates": [],
            "error": [
                [
                    "informaciÃ³n de clics",
                    "informaciÃ³n de clics",
                    "informaciÃ³n de clics",
                    "informaciÃ³n de clics",
                    "informaciÃ³n de clics",
                    "informaciÃ³n de clics anteriores"
                ]
            ]
        },
        "query history": {
            "translated_key": "historial de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user <br>query history</br> and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a <br>query history</br>, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term <br>query history</br> clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term <br>query history</br> is useful for improving retrieval accuracy.",
                "In addition to the <br>query history</br>, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both <br>query history</br> and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the <br>query history</br> and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the <br>query history</br> HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the <br>query history</br> and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the <br>query history</br> HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the <br>query history</br> HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse <br>query history</br>, thus we could use a smaller Âµi, but later as the <br>query history</br> is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the <br>query history</br> as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the <br>query history</br> Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also <br>query history</br> and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated <br>query history</br> and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no <br>query history</br> and clickthrough history data.",
                "We decide to augment a TREC data set by collecting <br>query history</br> and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer <br>query history</br> and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect <br>query history</br> and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using <br>query history</br> and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., <br>query history</br> and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - <br>query history</br> and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using <br>query history</br> only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using <br>query history</br> alone.",
                "We use the same parameter setting for <br>query history</br> as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using <br>query history</br> is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using <br>query history</br> is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using <br>query history</br> only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using <br>query history</br> only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using <br>query history</br> only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the <br>query history</br> is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more <br>query history</br> information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed <br>query history</br> results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the <br>query history</br> and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using <br>query history</br>.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the <br>query history</br> information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the <br>query history</br> is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the <br>query history</br> and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of <br>query history</br> and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including <br>query history</br> and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit <br>query history</br> and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting <br>query history</br> for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user <br>query history</br> and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a <br>query history</br>, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "But the short-term <br>query history</br> clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term <br>query history</br> is useful for improving retrieval accuracy.",
                "In addition to the <br>query history</br>, there may be other short-term context information available."
            ],
            "translated_annotated_samples": [
                "El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el <br>historial de consultas</br> de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo.",
                "Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un <br>historial de consultas</br>, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual.",
                "Pero el <br>historial de consultas</br> a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior.",
                "De hecho, nuestro trabajo previo [15] ha demostrado que el <br>historial de consultas</br> a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n.",
                "AdemÃ¡s del <br>historial de consultas</br>, puede haber otra informaciÃ³n de contexto a corto plazo disponible."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el <br>historial de consultas</br> de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un <br>historial de consultas</br>, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el <br>historial de consultas</br> a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el <br>historial de consultas</br> a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del <br>historial de consultas</br>, puede haber otra informaciÃ³n de contexto a corto plazo disponible. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query expansion": {
            "translated_key": "ampliaciÃ³n de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic <br>query expansion</br> using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "Probabilistic <br>query expansion</br> using query logs."
            ],
            "translated_annotated_samples": [
                "ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al contexto utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el contexto de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de contexto de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de informaciÃ³n de contexto adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al contexto ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentaciÃ³n de relevancia [14] puede considerarse como una forma para que un usuario proporcione mÃ¡s contexto de bÃºsqueda y se sabe que es efectiva para mejorar la precisiÃ³n de recuperaciÃ³n. Sin embargo, la retroalimentaciÃ³n de relevancia requiere que un usuario proporcione explÃ­citamente informaciÃ³n de retroalimentaciÃ³n, como especificar la categorÃ­a de la necesidad de informaciÃ³n o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa informaciÃ³n de retroalimentaciÃ³n. Por lo tanto, la efectividad de la retroalimentaciÃ³n de relevancia puede estar limitada en aplicaciones reales. Por esta razÃ³n, el feedback implÃ­cito ha atraÃ­do mucha atenciÃ³n recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperaciÃ³n utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendrÃ­a que revisar la consulta para mejorar la precisiÃ³n de la recuperaciÃ³n/clasificaciÃ³n [8]. Para una necesidad de informaciÃ³n compleja o difÃ­cil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de informaciÃ³n estÃ© completamente satisfecha. En un escenario de recuperaciÃ³n interactiva, la informaciÃ³n naturalmente disponible para el sistema de recuperaciÃ³n es mÃ¡s que solo la consulta actual del usuario y la colecciÃ³n de documentos; en general, todo el historial de interacciÃ³n puede estar disponible para el sistema de recuperaciÃ³n, incluidas las consultas anteriores, informaciÃ³n sobre quÃ© documentos ha elegido ver el usuario e incluso cÃ³mo ha leÃ­do un documento el usuario (por ejemplo, en quÃ© parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentaciÃ³n implÃ­cita de manera amplia como la explotaciÃ³n de todo el historial de interacciÃ³n naturalmente disponible para mejorar los resultados de recuperaciÃ³n. Una ventaja importante del feedback implÃ­cito es que podemos mejorar la precisiÃ³n de recuperaciÃ³n sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna informaciÃ³n adicional, serÃ­a imposible saber si se refiere al lenguaje de programaciÃ³n Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programaciÃ³n y otros sobre la isla. Sin embargo, es poco probable que algÃºn usuario en particular estÃ© buscando ambos tipos de documentos. Una ambigÃ¼edad como esta puede resolverse explotando la informaciÃ³n histÃ³rica. Por ejemplo, si sabemos que la consulta anterior del usuario es programaciÃ³n cgi, sugerirÃ­a fuertemente que es el lenguaje de programaciÃ³n que el usuario estÃ¡ buscando. El feedback implÃ­cito fue estudiado en varios trabajos previos. En [11], Joachims explorÃ³ cÃ³mo capturar y explotar la informaciÃ³n de clics y demostrÃ³ que dicha informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede mejorar la precisiÃ³n de la bÃºsqueda para un grupo de personas. En [18], se realizÃ³ un estudio de simulaciÃ³n sobre la efectividad de diferentes algoritmos de retroalimentaciÃ³n implÃ­cita, y se propusieron y evaluaron varios modelos de recuperaciÃ³n diseÃ±ados para aprovechar la informaciÃ³n de clics. En [17], algunos algoritmos de recuperaciÃ³n existentes se adaptan para mejorar los resultados de bÃºsqueda basados en el historial de navegaciÃ³n de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la bÃºsqueda personalizada [1, 3, 4, 7, 10], anÃ¡lisis de registros de consultas [5], factores de contexto [12] y consultas implÃ­citas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la informaciÃ³n de clics, en este artÃ­culo utilizamos tanto la informaciÃ³n de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperaciÃ³n. EspecÃ­ficamente, desarrollamos modelos para utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita, como la consulta y el historial de clics de la sesiÃ³n de bÃºsqueda actual, con el fin de mejorar la precisiÃ³n de recuperaciÃ³n. Utilizamos el modelo de recuperaciÃ³n de divergencia KL [19] como base y proponemos tratar la recuperaciÃ³n sensible al contexto como la estimaciÃ³n de un modelo de lenguaje de consulta basado en la consulta actual y cualquier informaciÃ³n de contexto de bÃºsqueda. Proponemos varios modelos estadÃ­sticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafÃ­o al estudiar modelos de retroalimentaciÃ³n implÃ­cita es que no existe ninguna colecciÃ³n de pruebas adecuada para la evaluaciÃ³n. Por lo tanto, utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de retroalimentaciÃ³n implÃ­cita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentaciÃ³n implÃ­cita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentaciÃ³n implÃ­cita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de informaciÃ³n de retroalimentaciÃ³n implÃ­cita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes estÃ¡n organizadas de la siguiente manera. En la SecciÃ³n 2, intentamos definir el problema de la retroalimentaciÃ³n implÃ­cita e introducir algunos tÃ©rminos que utilizaremos mÃ¡s adelante. En la SecciÃ³n 3, proponemos varios modelos de retroalimentaciÃ³n implÃ­cita basados en modelos estadÃ­sticos de lenguaje. En la SecciÃ³n 4, describimos cÃ³mo creamos el conjunto de datos para experimentos de retroalimentaciÃ³n implÃ­cita. En la SecciÃ³n 5, evaluamos diferentes modelos de retroalimentaciÃ³n implÃ­cita en el conjunto de datos creado. La secciÃ³n 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÃ“N DEL PROBLEMA Hay dos tipos de informaciÃ³n de contexto que podemos utilizar para obtener retroalimentaciÃ³n implÃ­cita. Uno es el contexto a corto plazo, que es la informaciÃ³n inmediata que rodea y arroja luz sobre la necesidad de informaciÃ³n actual de un usuario en una sola sesiÃ³n. Una sesiÃ³n puede ser considerada como un perÃ­odo que consiste en todas las interacciones para la misma necesidad de informaciÃ³n. La categorÃ­a de la necesidad de informaciÃ³n de un usuario (por ejemplo, niÃ±os o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha informaciÃ³n estÃ¡ directamente relacionada con la necesidad actual de informaciÃ³n del usuario y, por lo tanto, se espera que sea la mÃ¡s Ãºtil para mejorar la bÃºsqueda actual. En general, el contexto a corto plazo es mÃ¡s Ãºtil para mejorar la bÃºsqueda en la sesiÃ³n actual, pero puede que no sea tan Ãºtil para las actividades de bÃºsqueda en una sesiÃ³n diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a informaciÃ³n como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la informaciÃ³n de clics anteriores de los usuarios; esta informaciÃ³n suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisiÃ³n de bÃºsqueda en una sesiÃ³n particular. En este artÃ­culo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros mÃ©todos tambiÃ©n pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesiÃ³n de bÃºsqueda, un usuario puede interactuar con el sistema de bÃºsqueda varias veces. Durante las interacciones, el usuario modificarÃ­a continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesiÃ³n de bÃºsqueda), existe un historial de consultas, HQ = (Q1, ..., Qkâˆ’1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesiÃ³n actual. Se debe tener en cuenta que en este artÃ­culo asumimos que los lÃ­mites de las sesiones son conocidos. En la prÃ¡ctica, necesitamos tÃ©cnicas para descubrir automÃ¡ticamente los lÃ­mites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperaciÃ³n solo utiliza la consulta actual Qk para realizar la recuperaciÃ³n. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas Ãºtiles sobre la necesidad de informaciÃ³n actual del usuario, como se ve en el ejemplo de Java dado en la secciÃ³n anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es Ãºtil para mejorar la precisiÃ³n de recuperaciÃ³n. AdemÃ¡s del historial de consultas, puede haber otra informaciÃ³n de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente harÃ­a clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el tÃ­tulo, resumen y posiblemente tambiÃ©n el contenido y la ubicaciÃ³n (por ejemplo, la URL) del documento al que se hizo clic. Aunque no estÃ¡ claro si un documento visualizado es realmente relevante para la necesidad de informaciÃ³n del usuario, podemos asumir con seguridad que el resumen/tÃ­tulo mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite informaciÃ³n sobre la necesidad de informaciÃ³n del usuario. Supongamos que concatenamos toda la informaciÃ³n de texto mostrada sobre un documento (generalmente tÃ­tulo y resumen) juntas, tambiÃ©n tendremos un resumen clicado Ci en cada ronda de recuperaciÃ³n. En general, podemos tener un historial de resÃºmenes clicados C1, ..., Ckâˆ’1. TambiÃ©n aprovecharemos el historial de clics HC = (C1, ..., Ckâˆ’1) para mejorar la precisiÃ³n de nuestra bÃºsqueda para la consulta actual Qk. Trabajos anteriores tambiÃ©n han mostrado resultados positivos utilizando informaciÃ³n de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son informaciÃ³n de retroalimentaciÃ³n implÃ­cita, que existe naturalmente en la recuperaciÃ³n de informaciÃ³n interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artÃ­culo, estudiamos cÃ³mo explotar dicha informaciÃ³n (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una funciÃ³n de clasificaciÃ³n de recuperaciÃ³n, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperaciÃ³n de informaciÃ³n contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son Ãºtiles para mejorar la precisiÃ³n de la bÃºsqueda para la consulta actual Qk. Una pregunta de investigaciÃ³n importante es cÃ³mo podemos explotar esa informaciÃ³n de manera efectiva. Proponemos utilizar modelos de lenguaje estadÃ­stico para modelar la necesidad de informaciÃ³n de los usuarios y desarrollar cuatro modelos de lenguaje especÃ­ficos sensibles al contexto para incorporar informaciÃ³n de contexto en un modelo bÃ¡sico de recuperaciÃ³n. 3.1 Modelo bÃ¡sico de recuperaciÃ³n Utilizamos el mÃ©todo de divergencia de Kullback-Leibler (KL) [19] como nuestro mÃ©todo bÃ¡sico de recuperaciÃ³n. SegÃºn este modelo, la tarea de recuperaciÃ³n implica calcular un modelo de lenguaje de consulta Î¸Q para una consulta dada y un modelo de lenguaje de documento Î¸D para un documento y luego calcular su divergencia KL D(Î¸Q||Î¸D), que sirve como la puntuaciÃ³n del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de bÃºsqueda como evidencia adicional para mejorar nuestra estimaciÃ³n del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qkâˆ’1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ckâˆ’1) el historial de clics. Ten en cuenta que Ci es la concatenaciÃ³n de todos los resÃºmenes de documentos clicados en la i-Ã©sima ronda de recuperaciÃ³n, ya que podemos tratar razonablemente todos estos resÃºmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|Î¸k), basado en la consulta actual Qk, asÃ­ como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|Î¸k). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podrÃ­a ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el nÃºmero total de palabras en X. 3.2 InterpolaciÃ³n de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos histÃ³ricos para obtener el modelo histÃ³rico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) donde Î² âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en cada modelo de historial, y donde Î± âˆˆ [0, 1] es un parÃ¡metro para controlar el peso en la consulta actual y la informaciÃ³n histÃ³rica. Si combinamos estas ecuaciones, vemos que p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolaciÃ³n de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 InterpolaciÃ³n Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente Î±, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberÃ­amos confiar mÃ¡s en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner mÃ¡s peso en el historial. Para capturar esta intuiciÃ³n, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado estÃ¡ dado por p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] donde Âµ es el tamaÃ±o de la muestra previa para p(w|HQ) y Î½ es el tamaÃ±o de la muestra previa para p(w|HC ). Vemos que la Ãºnica diferencia entre BayesInt y FixInt es que los coeficientes de interpolaciÃ³n ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , por lo tanto, con Âµ y Î½ fijos, tendremos un Î± dependiente de la consulta. MÃ¡s adelante demostraremos que un Î± adaptativo empÃ­ricamente funciona mejor que un Î± fijo. 3.4 ActualizaciÃ³n Bayesiana en LÃ­nea (OnlineUp) Tanto FixInt como BayesInt resumen la informaciÃ³n histÃ³rica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resÃºmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resÃºmenes clicados. Sin embargo, a medida que el usuario interactÃºa con el sistema y adquiere mÃ¡s conocimiento sobre la informaciÃ³n en la colecciÃ³n, presumiblemente, las consultas reformuladas mejorarÃ¡n cada vez mÃ¡s. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar mÃ¡s en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de informaciÃ³n de los usuarios despuÃ©s de ver cada consulta, podrÃ­amos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualizaciÃ³n en lÃ­nea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperaciÃ³n interactivo, la presentamos de una manera mÃ¡s general. En un sistema de recuperaciÃ³n tÃ­pico, el sistema de recuperaciÃ³n responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de informaciÃ³n de los usuarios. En el modelo de recuperaciÃ³n de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimaciÃ³n bayesiana, la cual discutimos a continuaciÃ³n. 3.4.1 ActualizaciÃ³n bayesiana Primero discutimos cÃ³mo aplicamos la estimaciÃ³n bayesiana para actualizar un modelo de consulta en general. Sea p(w|Ï†) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos Ï† para definir un parÃ¡metro previo de Dirichlet parametrizado como Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) donde ÂµT es el tamaÃ±o de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribuciÃ³n predictiva de Ï† (o equivalentemente, la media de la distribuciÃ³n posterior de Ï†) estÃ¡ dada por p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parÃ¡metro ÂµT indica nuestra confianza en el prior expresada en tÃ©rminos de una muestra de texto equivalente a T. Por ejemplo, ÂµT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. ActualizaciÃ³n del modelo de consulta secuencial Ahora discutimos cÃ³mo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperaciÃ³n utilizando estimaciÃ³n bayesiana. En general, asumimos que el sistema de recuperaciÃ³n mantiene un modelo de consulta actual Ï†i en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentaciÃ³n implÃ­cita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta informaciÃ³n sobre el usuario. Por ejemplo, podemos tener informaciÃ³n sobre quÃ© documentos ha visto el usuario en el pasado. Utilizamos dicha informaciÃ³n para definir una distribuciÃ³n a priori en el modelo de consulta, que se denota como Ï†0. DespuÃ©s de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado Ï†1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resÃºmenes seleccionados) puede servir como nuevos datos para que actualicemos aÃºn mÃ¡s el modelo de consulta y obtener Ï†1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar Ï†1 para obtener un nuevo modelo Ï†2. En general, podemos repetir este proceso de actualizaciÃ³n para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualizaciÃ³n: (1) actualizaciÃ³n basada en una nueva consulta Qi; (2) actualizaciÃ³n basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. AsÃ­ tenemos las siguientes ecuaciones de actualizaciÃ³n: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i donde Âµi es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que Î½i es el tamaÃ±o de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos Âµi = 0 (o Î½i = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzarÃ­amos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos Âµi = +âˆž (o Î½i = +âˆž) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observÃ¡ramos ninguna nueva evidencia textual. En general, los parÃ¡metros Âµi y Î½i pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podrÃ­amos usar un Âµi mÃ¡s pequeÃ±o, pero mÃ¡s tarde, a medida que el historial de consultas sea mÃ¡s amplio, podrÃ­amos considerar usar un Âµi mÃ¡s grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, âˆ€i, j, Âµi = Âµj, Î½i = Î½j. Ten en cuenta que podemos tomar tanto p(w|Ï†i) como p(w|Ï†i) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperaciÃ³n; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|Ï†i) para volver a clasificar inmediatamente cualquier documento que un usuario aÃºn no haya visto. Para puntuar documentos despuÃ©s de ver la consulta Qk, usamos p(w|Ï†k), es decir, p(w|Î¸k) = p(w|Ï†k) 3.5 ActualizaciÃ³n bayesiana por lotes (BatchUp). Si fijamos los parÃ¡metros de tamaÃ±o de muestra equivalente a una constante fija, el algoritmo OnlineUp introducirÃ­a un factor de decaimiento: la interpolaciÃ³n repetida harÃ­a que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez mÃ¡s en la formulaciÃ³n de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la informaciÃ³n de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolaciÃ³n en descomposiciÃ³n a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qiâˆ’1), pero no para los datos de clics C. Primero almacenamos en bÃºfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecuciÃ³n de OnlineUp en consultas anteriores. Las ecuaciones de actualizaciÃ³n son las siguientes. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i donde Âµi tiene la misma interpretaciÃ³n que en OnlineUp, pero Î½i ahora indica en quÃ© medida queremos confiar en los resÃºmenes clicados. En OnlineUp, establecemos todos los Âµis y los Î½is en el mismo valor. Y para clasificar los documentos despuÃ©s de ver la consulta actual Qk, usamos p(w|Î¸k) = p(w|Ïˆk) 4. RECOLECCIÃ“N DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino tambiÃ©n el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, asÃ­ como el historial de clics para cada tema del registro de un sistema de recuperaciÃ³n (por ejemplo, un motor de bÃºsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opciÃ³n es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripciÃ³n del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artÃ­culos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayorÃ­a de los artÃ­culos tienen tÃ­tulos. Si no, seleccionamos la primera oraciÃ³n del texto como tÃ­tulo. Para el preprocesamiento, solo realizamos la conversiÃ³n a minÃºsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difÃ­ciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisiÃ³n entre los temas 1-150 de TREC segÃºn algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorizaciÃ³n bayesiana [20]. La razÃ³n por la que seleccionamos temas difÃ­ciles es que el usuario tendrÃ­a que tener varias interacciones con el sistema de recuperaciÃ³n para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente mÃ¡s ricos del usuario. En aplicaciones reales, tambiÃ©n podemos esperar que nuestros modelos sean mÃ¡s Ãºtiles para temas difÃ­ciles, por lo que nuestra estrategia de recolecciÃ³n de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de bÃºsqueda e interfaz web para los artÃ­culos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el tÃ­tulo del tema dado en la descripciÃ³n original del tema de TREC. DespuÃ©s de que el sujeto envÃ­e la consulta, el motor de bÃºsqueda realizarÃ¡ la recuperaciÃ³n y devolverÃ¡ una lista clasificada de resultados de bÃºsqueda al sujeto. El sujeto revisarÃ¡ los resultados y tal vez haga clic en uno o mÃ¡s resultados para leer el texto completo del artÃ­culo o artÃ­culos. El sujeto tambiÃ©n puede modificar la consulta para realizar otra bÃºsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el nÃºmero del tema de un menÃº de selecciÃ³n antes de enviar la consulta al motor de bÃºsqueda para que podamos detectar fÃ¡cilmente el lÃ­mite de la sesiÃ³n, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los tÃ©rminos de la consulta y las pÃ¡ginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la pÃ¡gina de resultados de bÃºsqueda. El resumen del artÃ­culo es dependiente de la consulta y se calcula en lÃ­nea utilizando la recuperaciÃ³n de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificaciÃ³n de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes segÃºn el archivo de juicio de TREC. Este conjunto de datos estÃ¡ disponible pÃºblicamente. EXPERIMENTOS 5.1 DiseÃ±o del experimento Nuestra hipÃ³tesis principal es que el uso del contexto de bÃºsqueda (es decir, el historial de consultas y la informaciÃ³n de clics) puede ayudar a mejorar la precisiÃ³n de la bÃºsqueda. En particular, el contexto de bÃºsqueda puede proporcionar informaciÃ³n adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayorÃ­a de nuestros experimentos implican comparar el rendimiento de recuperaciÃ³n utilizando solo la consulta actual (ignorando asÃ­ cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de bÃºsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versiÃ³n de consultas. Utilizamos dos medidas de rendimiento: (1) PrecisiÃ³n Promedio Media (MAP): Esta es la precisiÃ³n promedio estÃ¡ndar no interpolada y sirve como una buena medida de la precisiÃ³n general de clasificaciÃ³n. (2) PrecisiÃ³n en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es mÃ¡s significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de bÃºsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parÃ¡metros (Î± y Î² para FixInt; Âµ y Î½ para los demÃ¡s). Ten en cuenta que Âµ y Î½ pueden necesitar ser interpretados de manera diferente para diferentes mÃ©todos. Variamos estos parÃ¡metros e identificamos el rendimiento Ã³ptimo para cada mÃ©todo. TambiÃ©n variamos los parÃ¡metros para estudiar la sensibilidad de nuestros algoritmos a la configuraciÃ³n de los parÃ¡metros. 5.2 AnÃ¡lisis de resultados 5.2.1 Efecto general del contexto de bÃºsqueda Comparamos el rendimiento Ã³ptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de bÃºsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuaciÃ³n de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de bÃºsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente mÃ¡s sustancial en comparaciÃ³n con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de bÃºsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisiÃ³n media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas tambiÃ©n hacen que la mejora relativa sea engaÃ±osamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de bÃºsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolaciÃ³n, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolaciÃ³n de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaÃ­dos para combinar los mÃºltiples resÃºmenes clicados, mientras que BatchUp simplemente concatena todos los resÃºmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resÃºmenes clicados no deberÃ­an estar decayendo. Si bien OnlineUp es teÃ³ricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor mÃ©todo cuando variamos la configuraciÃ³n de los parÃ¡metros. Tenemos dos tipos diferentes de contexto de bÃºsqueda: historial de consultas y datos de clics. Ahora analizamos la contribuciÃ³n de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parÃ¡metros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuraciÃ³n de parÃ¡metros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. AquÃ­ vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultÃ³ ser consistentemente Ãºtil. Otra observaciÃ³n es que los contextos de ejecuciÃ³n funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el tÃ­tulo en la descripciÃ³n original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generaciÃ³n es claramente inferior al de todas las demÃ¡s consultas formuladas por los usuarios en las generaciones posteriores. Otra observaciÃ³n es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificaciÃ³n de documentos. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisiÃ³n promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variaciÃ³n causada por el parÃ¡metro Âµ. Un ajuste mÃ¡s bajo de 2.0 se ve mejor que un valor mÃ¡s alto de 5.0. Una imagen mÃ¡s completa de la influencia del ajuste de Âµ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango mÃ¡s amplio de valores de Âµ. El valor de Âµ se puede interpretar como cuÃ¡ntas palabras consideramos que vale la historia de la consulta. Un valor mÃ¡s grande pone mÃ¡s peso en la historia y se considera que afecta mÃ¡s el rendimiento cuando la informaciÃ³n histÃ³rica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para Âµ âˆˆ [2, 5], solo cuando Âµ = 0.5 vemos algÃºn pequeÃ±o beneficio para q2. Como esperarÃ­amos, un Âµ excesivamente grande perjudicarÃ­a el rendimiento en general, pero q2 es el mÃ¡s perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos mÃ¡s informaciÃ³n del historial de consultas, podemos poner mÃ¡s peso en esa informaciÃ³n histÃ³rica. Esto tambiÃ©n sugiere que una estrategia mejor probablemente deberÃ­a ajustar dinÃ¡micamente los parÃ¡metros segÃºn la cantidad de informaciÃ³n histÃ³rica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar informaciÃ³n de retroalimentaciÃ³n implÃ­cita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsecciÃ³n. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resÃºmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la informaciÃ³n de clics es mucho mÃ¡s significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. TambiÃ©n es claro que cuanto mÃ¡s ricos sean los datos del contexto, mayor serÃ¡ la mejora que se puede lograr al utilizar resÃºmenes seleccionados. Aparte de alguna degradaciÃ³n ocasional de la precisiÃ³n en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante Ãºtil para inferir la necesidad de informaciÃ³n de un usuario. Intuitivamente, tiene mÃ¡s sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrÃ¡s de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resÃºmenes elevarÃ­a la clasificaciÃ³n de estos documentos relevantes, lo que provocarÃ­a una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuÃ¡nta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la lÃ­nea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del mÃ©todo base es menor debido a la eliminaciÃ³n de los 29 documentos relevantes, los cuales habrÃ­an sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resÃºmenes clicados tambiÃ©n ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo Ãºtiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraÃ­mos los 29 resÃºmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto mÃ¡s pequeÃ±o de resÃºmenes clicados HC, y reevaluamos el rendimiento del mÃ©todo BayesInt utilizando HC con la misma configuraciÃ³n de parÃ¡metros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisiÃ³n promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es mÃ¡s probable que un usuario haga clic en algunos resÃºmenes relevantes, lo que ayudarÃ­a a mostrar mÃ¡s documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificaciÃ³n de documentos. El beneficio de la informaciÃ³n del historial de consultas y la informaciÃ³n de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la informaciÃ³n de clics. En la Tabla 7, mostramos este efecto para el mÃ©todo BatchUp. Sensibilidad de parÃ¡metros. Los cuatro modelos tienen dos parÃ¡metros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrizaciÃ³n es diferente de un modelo a otro. En esta subsecciÃ³n, estudiamos la sensibilidad de los parÃ¡metros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parÃ¡metros Âµ y Î½. Primero miramos a Âµ. Cuando Âµ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos Âµ, incorporaremos gradualmente mÃ¡s informaciÃ³n de las consultas anteriores. En la Tabla 8, mostramos cÃ³mo la precisiÃ³n promedio de BatchUp cambia a medida que variamos Âµ con Î½ fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de Âµ para q3 y q4, pero disminuye a medida que Âµ aumenta para q2. El patrÃ³n tambiÃ©n es similar cuando establecemos Î½ en otros valores. AdemÃ¡s del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razÃ³n por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos mÃ¡s datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeÃ±as diferencias causadas por Âµ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando Âµ estÃ¡ alrededor de 2.0, lo que significa que la informaciÃ³n de consultas anteriores es tan Ãºtil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la informaciÃ³n de contexto y el uso de una combinaciÃ³n equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parÃ¡metro Î½. Cuando Î½ se establece en 0, solo usamos los datos de clics; cuando Î½ se establece en +âˆž, solo usamos el historial de consultas y la consulta actual. Con Âµ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos Î½ y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando Î½ â‰¤ 30, con frecuencia el mejor rendimiento se logra en Î½ = 15. Esto significa que la informaciÃ³n combinada del historial de consultas y la consulta actual es tan Ãºtil como aproximadamente 15 palabras en los datos de clics, lo que indica que la informaciÃ³n de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros mÃ©todos, sino que tambiÃ©n es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artÃ­culo, hemos explorado cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluida la historia de consultas y la historia de clics dentro de la misma sesiÃ³n de bÃºsqueda, para mejorar el rendimiento de la recuperaciÃ³n de informaciÃ³n. Usando el modelo de recuperaciÃ³n de divergencia KL como base, propusimos y estudiamos cuatro modelos estadÃ­sticos de lenguaje para la recuperaciÃ³n de informaciÃ³n sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de Âµ en BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de Î½ en BatchUp para evaluar modelos de retroalimentaciÃ³n implÃ­cita. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperaciÃ³n sin requerir ningÃºn esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar informaciÃ³n de retroalimentaciÃ³n implÃ­cita. SerÃ­a interesante desarrollar modelos mÃ¡s sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalizaciÃ³n o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema prÃ¡ctico. Actualmente estamos desarrollando un agente de bÃºsqueda personalizado del lado del cliente, que incorporarÃ¡ algunos de los algoritmos propuestos. TambiÃ©n realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la bÃºsqueda web real. Finalmente, deberÃ­amos estudiar mÃ¡s a fondo un marco general de recuperaciÃ³n para la toma de decisiones secuenciales en la recuperaciÃ³n de informaciÃ³n interactiva y estudiar cÃ³mo optimizar algunos de los parÃ¡metros en los modelos de recuperaciÃ³n sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la FundaciÃ³n Nacional de Ciencias bajo los nÃºmeros de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anÃ³nimos por sus comentarios Ãºtiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de informaciÃ³n por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. DesafÃ­os en la recuperaciÃ³n de informaciÃ³n y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explÃ­cita del contexto de bÃºsqueda para apoyar la bÃºsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. RetroalimentaciÃ³n de relevancia y personalizaciÃ³n: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: PersonalizaciÃ³n y Sistemas de RecomendaciÃ³n en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. ExpansiÃ³n de consulta probabilÃ­stica utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implÃ­citas (IQ) para bÃºsqueda contextualizada (descripciÃ³n de demostraciÃ³n). En Actas de SIGIR 2004, pÃ¡gina 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la bÃºsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de tÃ©rminos basada en la sesiÃ³n de bÃºsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. IdentificaciÃ³n dinÃ¡mica de sesiones de registro web con modelos de lenguaje estadÃ­stico. Revista de la Sociedad Americana de Ciencia de la InformaciÃ³n y TecnologÃ­a, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la bÃºsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. OptimizaciÃ³n de motores de bÃºsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentaciÃ³n implÃ­cita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. RetroalimentaciÃ³n implÃ­cita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. RecuperaciÃ³n de informaciÃ³n con retroalimentaciÃ³n de relevancia. En el Sistema de RecuperaciÃ³n Inteligente-Experimentos en Procesamiento AutomÃ¡tico de Documentos, pÃ¡ginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificaciÃ³n de documentos en la recuperaciÃ³n de informaciÃ³n interactiva (pÃ³ster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de bÃºsqueda basado en sesiones (pÃ³ster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. BÃºsqueda web adaptativa basada en el perfil del usuario construido sin ningÃºn esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentaciÃ³n implÃ­cita. En Actas de ECIR 2004, pÃ¡ginas 311-326, 2004. [19] C. Zhai y J. Lafferty. RetroalimentaciÃ³n basada en el modelo en el modelo de recuperaciÃ³n de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de mÃ©todos de suavizado para modelos de lenguaje aplicados a la recuperaciÃ³n de informaciÃ³n ad-hoc. En Actas de SIGIR 2001, 2001. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "context": {
            "translated_key": "contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>context</br>-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search <br>context</br> is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search <br>context</br> information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional <br>context</br> information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, <br>context</br>-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of <br>context</br> that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more <br>context</br> of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using <br>context</br> includes personalized search [1, 3, 4, 7, 10], query log analysis [5], <br>context</br> factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new <br>context</br>-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat <br>context</br>-sensitive retrieval as estimating a query language model based on the current query and any search <br>context</br> information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of <br>context</br> information we can use for implicit feedback.",
                "One is short-term <br>context</br>, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term <br>context</br>.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term <br>context</br> is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of <br>context</br> is long-term <br>context</br>, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term <br>context</br> can be applicable to all sessions, but may not be as effective as the short-term <br>context</br> in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term <br>context</br>, though some of our methods can also be used to naturally incorporate some long-term <br>context</br>.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qkâˆ’1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term <br>context</br> information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ckâˆ’1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ckâˆ’1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific <br>context</br>-sensitive language models to incorporate <br>context</br> information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model Î¸Q for a given query and a document language model Î¸D for a document and then computing their KL divergence D(Î¸Q||Î¸D), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search <br>context</br> as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qkâˆ’1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ckâˆ’1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a <br>context</br> query model, which we denote by p(w|Î¸k), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|Î¸k).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k âˆ’ 1 i=kâˆ’1 i=1 p(w|Ci) p(w|H) = Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ) p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)p(w|H) where Î² âˆˆ [0, 1] is a parameter to control the weight on each history model, and where Î± âˆˆ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|Î¸k) = Î±p(w|Qk) + (1 âˆ’ Î±)[Î²p(w|HC ) + (1 âˆ’ Î²)p(w|HQ)] That is, the estimated <br>context</br> query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially Î±, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a <br>context</br> query model using Bayesian estimator.",
                "The estimated model is given by p(w|Î¸k) = c(w, Qk) + Âµp(w|HQ) + Î½p(w|HC ) |Qk| + Âµ + Î½ = |Qk| |Qk| + Âµ + Î½ p(w|Qk)+ Âµ + Î½ |Qk| + Âµ + Î½ [ Âµ Âµ + Î½ p(w|HQ)+ Î½ Âµ + Î½ p(w|HC )] where Âµ is the prior sample size for p(w|HQ) and Î½ is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that Î± = |Qk| |Qk|+Âµ+Î½ , Î² = Î½ Î½+Âµ , thus with fixed Âµ and Î½, we will have a query-dependent Î±.",
                "Later we will show that such an adaptive Î± empirically performs better than a fixed Î±. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|Ï†) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use Ï† to define a Dirichlet prior parameterized as Dir(ÂµT p(w1|Ï†), ..., ÂµT p(wN |Ï†)) where ÂµT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of Ï† (or equivalently, the mean of the posterior distribution of Ï† is given by p(w|Ï†) = c(w, T) + ÂµT p(w|Ï†) |T| + ÂµT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter ÂµT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, ÂµT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model Ï†i at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by Ï†0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model Ï†1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain Ï†1.",
                "As we obtain the second query Q2 from the user, we can update Ï†1 to obtain a new model Ï†2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the <br>context</br> query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ï†i) = c(w, Ci) + Î½ip(w|Ï†i) |Ci| + Î½i where Âµi is the equivalent sample size for the prior when updating the model based on a query, while Î½i is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set Âµi = 0 (or Î½i = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set Âµi = +âˆž (or Î½i = +âˆž) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters Âµi and Î½i may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller Âµi, but later as the query history is richer, we can consider using a larger Âµi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., âˆ€i, j, Âµi = Âµj, Î½i = Î½j.",
                "Note that we can take either p(w|Ï†i) or p(w|Ï†i) as our <br>context</br> query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|Ï†i) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|Ï†k), i.e., p(w|Î¸k) = p(w|Ï†k) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qiâˆ’1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|Ï†i) = c(w, Qi) + Âµip(w|Ï†iâˆ’1) |Qi| + Âµi p(w|Ïˆi) = iâˆ’1 j=1 c(w, Cj) + Î½ip(w|Ï†i) iâˆ’1 j=1 |Cj| + Î½i where Âµi has the same interpretation as in OnlineUp, but Î½i now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all Âµis and Î½is to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|Î¸k) = p(w|Ïˆk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1.0) (Âµ = 0.2, Î½ = 5.0) (Âµ = 5.0, Î½ = 15.0) (Âµ = 2.0, Î½ = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search <br>context</br> (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search <br>context</br> can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any <br>context</br>) with that using the current query as well as the search <br>context</br>.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search <br>context</br> (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (Î± and Î² for FixInt; Âµ and Î½ for others).",
                "Note that Âµ and Î½ may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search <br>context</br> We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search <br>context</br>.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search <br>context</br> generally has positive effect, especially when the <br>context</br> is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the <br>context</br> may hurt the performance, probably because the history at that point is sparse.",
                "When the search <br>context</br> is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search <br>context</br>, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search <br>context</br> - query history and clickthrough data.",
                "We now look into the contribution of each kind of <br>context</br>. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the <br>context</br> runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 0) (Âµ = 0.2,Î½ = 0) (Âµ = 5.0,Î½ = +âˆž) (Âµ = 2.0, Î½ = +âˆž) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. Âµ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter Âµ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of Âµ can be seen from Table 3, where we show the performance figures for a wider range of values of Âµ.",
                "The value of Âµ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for Âµ âˆˆ [2, 5], only when Âµ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large Âµ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the <br>context</br> data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (Î± = 0.1, Î² = 1) (Âµ = 0, Î½ = 5.0) (Âµk = 5.0, Î½ = 15, âˆ€i < k, Âµi = +âˆž) (Âµ = 0, Î½ = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(Âµ = 0, Î½ = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of <br>context</br> information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters Âµ and Î½.",
                "We first look at Âµ.",
                "When Âµ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase Âµ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary Âµ with Î½ fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of Âµ for q3 and q4, but is decreasing as Âµ increases for q2.",
                "The pattern is also similar when we set Î½ to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by Âµ less visible for q3 and q4.",
                "The best performance is generally achieved when Âµ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of <br>context</br> information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter Î½.",
                "When Î½ is set to 0, we only use the clickthrough data; When Î½ is set to +âˆž, we only use the query history and the current query.",
                "With Âµ set to 2.0, where the best performance of BatchUp is achieved, we vary Î½ and show the results in Table 9.",
                "We see that the performance is also not very sensitive when Î½ â‰¤ 30, with the best performance often achieved at Î½ = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for <br>context</br>-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set Âµ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of Âµ in BatchUp Î½ 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of Î½ in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the <br>context</br>-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search <br>context</br> to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in <br>context</br>: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [
                "<br>context</br>-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search <br>context</br> is largely ignored.",
                "We use the TREC AP data to create a test collection with search <br>context</br> information, and quantitatively evaluate our models using this test set.",
                "An optimal retrieval system thus should try to exploit as much additional <br>context</br> information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, <br>context</br>-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of <br>context</br> that we can exploit."
            ],
            "translated_annotated_samples": [
                "RecuperaciÃ³n de informaciÃ³n sensible al <br>contexto</br> utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el <br>contexto</br> de bÃºsqueda se ignora en gran medida.",
                "Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de <br>contexto</br> de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas.",
                "Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de <br>informaciÃ³n de contexto</br> adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible.",
                "De hecho, la recuperaciÃ³n sensible al <br>contexto</br> ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2].",
                "Hay muchos tipos de <br>contexto</br> que podemos aprovechar."
            ],
            "translated_text": "RecuperaciÃ³n de informaciÃ³n sensible al <br>contexto</br> utilizando retroalimentaciÃ³n implÃ­cita Xuehua Shen Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la ComputaciÃ³n Universidad de Illinois en Urbana-Champaign RESUMEN Una limitaciÃ³n importante de la mayorÃ­a de los modelos y sistemas de recuperaciÃ³n existentes es que la decisiÃ³n de recuperaciÃ³n se toma Ãºnicamente en funciÃ³n de la consulta y la colecciÃ³n de documentos; la informaciÃ³n sobre el usuario real y el <br>contexto</br> de bÃºsqueda se ignora en gran medida. En este artÃ­culo, estudiamos cÃ³mo aprovechar la informaciÃ³n de retroalimentaciÃ³n implÃ­cita, incluidas las consultas anteriores y la informaciÃ³n de clics, para mejorar la precisiÃ³n de recuperaciÃ³n en un entorno de recuperaciÃ³n de informaciÃ³n interactiva. Proponemos varios algoritmos de recuperaciÃ³n sensibles al contexto basados en modelos de lenguaje estadÃ­stico para combinar las consultas anteriores y los resÃºmenes de documentos clicados con la consulta actual para una mejor clasificaciÃ³n de documentos. Utilizamos los datos de TREC AP para crear una colecciÃ³n de pruebas con informaciÃ³n de <br>contexto</br> de bÃºsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentaciÃ³n implÃ­cita, especialmente los resÃºmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperaciÃ³n. CategorÃ­as y Descriptores de Asignaturas H.3.3 [BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n]: Modelos de recuperaciÃ³n TÃ©rminos generales Algoritmos 1. En la mayorÃ­a de los modelos de recuperaciÃ³n de informaciÃ³n existentes, el problema de recuperaciÃ³n se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperaciÃ³n solo puede tener una pista muy limitada sobre la necesidad de informaciÃ³n del usuario. Un sistema de recuperaciÃ³n Ã³ptimo deberÃ­a intentar aprovechar la mayor cantidad de <br>informaciÃ³n de contexto</br> adicional posible para mejorar la precisiÃ³n de la recuperaciÃ³n, siempre que estÃ© disponible. De hecho, la recuperaciÃ³n sensible al <br>contexto</br> ha sido identificada como un desafÃ­o importante en la investigaciÃ³n de la recuperaciÃ³n de informaciÃ³n[2]. Hay muchos tipos de <br>contexto</br> que podemos aprovechar. ",
            "candidates": [],
            "error": [
                [
                    "contexto",
                    "contexto",
                    "contexto",
                    "informaciÃ³n de contexto",
                    "contexto",
                    "contexto"
                ]
            ]
        }
    }
}