{
    "id": "J-44",
    "original_text": "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept. of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering. In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.). These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community. Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1. INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history. Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings. Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8]. However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations. Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success. Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering. We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds. Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations. In turn, this capability helps support scenarios such as: 1. Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match. Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2. Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items. Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3. Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack. Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved. These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5]. As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community. The rest of the paper is organized as follows. Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2. Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles. In Section 5, we illustrate the use of these roles to address the goals outlined above. 2. BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction. An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12]. In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based). Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item. Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions. We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12]. The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v). Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2]. These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u. A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i. The item-based algorithm we use is the one defined by Sarwar et al. [12]. In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i. As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item. A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3]. Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set. A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3]. A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions. Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list. So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order. Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5]. Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3]. A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3]. For instance, an inversion toward the end of the list is given the same weight as one in the beginning. One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3. ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used. Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.) The recommender predicts whether Tom will like The Mask using the other already available ratings. How this is done depends on the algorithm: 1. An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask). Toms ratings of those movies are then used to make a prediction for The Mask. 2. A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix). The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim. Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings. So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings. In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3). Note that these paths are undirected, and are all of length 3. Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).) A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3). Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value. Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors. To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1. The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask. This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2. The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3. The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom. Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a. The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively). Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users. For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny. Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix. However, this does not make The Matrix the best movie to rate for everyone. For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him. His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars. This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system. While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items. In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom. The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it. On the other hand, Jerrys rating of Star Wars does not help promote it to any other user. We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users. Connectors serve a crucial role in a recommender system that is not as obvious. The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4. Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors. A connector improves the systems ability to make recommendations with no explicit gain for the user. Note that every rating can be of varied benefit in each of these roles. The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter. The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter. Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter. As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not. We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system. But we can also measure the contribution of each rating to the quality of recommendations or health of the system. Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error. We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played. The next section describes the approach to measuring the values of a rating in each role. 4. CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways. Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction. By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system. This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy. We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations. Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering. The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user. As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)). A set of the top K neighbors is maintained for all items for space and computational efficiency. A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4). The list of recommendations for a user is then the list of items sorted in descending order of their predicted values. We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j. This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed. Here we illustrate the approach using predictive accuracy as the evaluation metric. In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role. For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e). Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not. We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1]. For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences. This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter. In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount. Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles). Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy. For this computation, we must know the users preference order for a set of items for which predictions can be computed. We assume that we have a test set of the users ratings of the items presented in the recommendation list. For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference. We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ). Similarly, a pair (i, j) is discordant (with error ) if it is not concordant. Our experiments described below use an error tolerance of = 0.1. All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited. The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances. The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences. We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system. Here we consider how we can use the role values to characterize the health of a neighborhood. Consider the list of top recommendations presented to a user at a specific point in time. The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations. We call these ratings the recommender neighborhood of the user. The user implicitly chooses this neighborhood of ratings through the items he rates. Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system. We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list. Different sections of the users neighborhood wield varied influence on his recommendation list. For instance, ratings reachable through highly rated items have a bigger say in the recommended items. Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood. A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters. He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless. We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future. A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations. Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list. In our studies we use K(i) = p position(i). Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i). The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) and PFN(u) are defined similarly. This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5. EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system. In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset. In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies. The ratings are in the range 1 to 5, and are labeled with the time the rating was given. As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy. Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order. The timestamping provided by MovieLens is hence crucial for the analyses presented here. We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots). We incrementally update the role values as the time ordered ratings are merged into the model. To keep the experiment computationally manageable, we define a test dataset for each user. As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data. At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions. One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles. We overcome this concern by repeating the experiment, using different random seeds. The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds. The results here are based on n = 4 repetitions. The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy. Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model. The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations. We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values. Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference. Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists. The distribution of the scout values for most users ratings are Gaussian with mean zero. Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot. We observe that a large number of ratings never serve as scouts for their users. A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts. With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions. Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal. An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9]. They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE. Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts. We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system. Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved. Both popularity and popularity*variance performed similarly. A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals). Table 1: Movies forming the best scouts. Best Scouts Conf. Pop. Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts. Worst scouts Conf. Pop. Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time. We claim these movies will make viable scouts for other users. We found the aggregated scout values for all movies in intervals of 10,000 ratings each. A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list. Movies appearing consistently high over time are expected to remain up there in the future. The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered. Using this measure, the top few movies expected to induce the best scouts are shown in Table 1. Movies that would be bad scout choices are shown in Table 2 with their associated confidences. The popularities of the movies are also displayed. Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous. Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive. Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences. On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie. Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw. Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users. So, inducing good promoters is important for cold-start recommendation. We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6). This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users. We find a strong correlation between a users number of ratings and his aggregated promoter value. Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value. We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily. Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values. A new movie is thus cast into the neighborhood of many other movies improving its visibility. Note, though, that a user may have long stopped using the system. Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors. Similarly, the users that constitute the best promoters are also part of the best connectors. Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value. In our experiments, we find that a ratings longest standing role is often as a connector. A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout. Such ratings can be removed from the prediction process to bring marginal improvements to recommendations. In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%. The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time. The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used. Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system. We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system. We classify different rating states as good, bad, or negligible. Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set. If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good. Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value. For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad. The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system. Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size. For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system! Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit. Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later). Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit. The bad connectors (0.8% of the system) hold 36% of all discredit. Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit. This reiterates that a few ratings influence most of the systems performance. Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change. A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on. It is not practical to expect a recommender system to have no ratings in bad roles. However, it suffices to see ratings in bad roles either convert to good or vestigial roles. Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system. We employ the principle of non-overlapping episodes [6] to count such transitions. A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. See [6] for further details about this counting procedure. Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions. We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role. Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states. In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted. Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset. Only transitions with frequency greater than or equal to 3% are shown. The percentages for each state indicates the number of ratings that were found to be in those states. We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying. From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role. The majority of the transitions involve both good and bad ratings becoming negligible. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts. Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts. Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors. As indicated earlier, there are hardly any transitions from promoters/connectors to scouts. This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally. Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time. However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system. For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny. Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user. In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics. To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1. Inactive user: (SFN(u) = 0) 2. Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive. Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood. Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system. As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations. The system can be expected to deliver more if they engineer some good scouts. Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage. Handling users with bad scouts and bad neighborhoods is a more difficult challenge. Such a classification allows the use of different strategies to better a users experience with the system depending on his context. In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6. CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings. A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor. Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics. In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions. We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values. Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7. REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J. Is Seeing Believing?: How Recommender System Interfaces Affect Users Opinions. In Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., and Riedl, J. An Algorithmic Framework for Performing Collaborative Filtering. In Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. A. Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit. In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., and Riedl, J. Getting to Know You: Learning New User Preferences in Recommender Systems. In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J. Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach. In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms. In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation. In Proc. SIGIR (2002), pp. 253-260. 259",
    "original_translation": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259",
    "original_sentences": [
        "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
        "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
        "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
        "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
        "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
        "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
        "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
        "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
        "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
        "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
        "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
        "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
        "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
        "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
        "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
        "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
        "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
        "In turn, this capability helps support scenarios such as: 1.",
        "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
        "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
        "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
        "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
        "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
        "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
        "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
        "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
        "The rest of the paper is organized as follows.",
        "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
        "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
        "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
        "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
        "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
        "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
        "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
        "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
        "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
        "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
        "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
        "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
        "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
        "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
        "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
        "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
        "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
        "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
        "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
        "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
        "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
        "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
        "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
        "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
        "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
        "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
        "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
        "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
        "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
        "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
        "How this is done depends on the algorithm: 1.",
        "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
        "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
        "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
        "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
        "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
        "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
        "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
        "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
        "Note that these paths are undirected, and are all of length 3.",
        "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
        "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
        "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
        "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
        "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
        "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
        "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
        "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
        "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
        "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
        "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
        "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
        "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
        "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
        "However, this does not make The Matrix the best movie to rate for everyone.",
        "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
        "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
        "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
        "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
        "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
        "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
        "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
        "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
        "Connectors serve a crucial role in a recommender system that is not as obvious.",
        "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
        "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
        "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
        "Note that every rating can be of varied benefit in each of these roles.",
        "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
        "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
        "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
        "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
        "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
        "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
        "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
        "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
        "The next section describes the approach to measuring the values of a rating in each role. 4.",
        "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
        "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
        "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
        "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
        "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
        "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
        "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
        "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
        "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
        "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
        "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
        "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
        "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
        "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
        "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
        "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
        "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
        "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
        "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
        "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
        "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
        "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
        "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
        "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
        "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
        "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
        "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
        "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
        "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
        "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
        "Our experiments described below use an error tolerance of = 0.1.",
        "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
        "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
        "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
        "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
        "Here we consider how we can use the role values to characterize the health of a neighborhood.",
        "Consider the list of top recommendations presented to a user at a specific point in time.",
        "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
        "We call these ratings the recommender neighborhood of the user.",
        "The user implicitly chooses this neighborhood of ratings through the items he rates.",
        "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
        "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
        "Different sections of the users neighborhood wield varied influence on his recommendation list.",
        "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
        "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
        "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
        "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
        "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
        "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
        "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
        "In our studies we use K(i) = p position(i).",
        "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
        "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
        "CFN(u) and PFN(u) are defined similarly.",
        "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
        "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
        "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
        "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
        "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
        "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
        "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
        "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
        "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
        "We incrementally update the role values as the time ordered ratings are merged into the model.",
        "To keep the experiment computationally manageable, we define a test dataset for each user.",
        "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
        "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
        "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
        "We overcome this concern by repeating the experiment, using different random seeds.",
        "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
        "The results here are based on n = 4 repetitions.",
        "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
        "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
        "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
        "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
        "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
        "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
        "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
        "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
        "We observe that a large number of ratings never serve as scouts for their users.",
        "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
        "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
        "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
        "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
        "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
        "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
        "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
        "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
        "Both popularity and popularity*variance performed similarly.",
        "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
        "Table 1: Movies forming the best scouts.",
        "Best Scouts Conf.",
        "Pop.",
        "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
        "Worst scouts Conf.",
        "Pop.",
        "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
        "We claim these movies will make viable scouts for other users.",
        "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
        "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
        "Movies appearing consistently high over time are expected to remain up there in the future.",
        "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
        "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
        "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
        "The popularities of the movies are also displayed.",
        "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
        "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
        "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
        "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
        "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
        "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
        "So, inducing good promoters is important for cold-start recommendation.",
        "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
        "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
        "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
        "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
        "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
        "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
        "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
        "Note, though, that a user may have long stopped using the system.",
        "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
        "Similarly, the users that constitute the best promoters are also part of the best connectors.",
        "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
        "In our experiments, we find that a ratings longest standing role is often as a connector.",
        "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
        "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
        "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
        "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
        "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
        "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
        "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
        "We classify different rating states as good, bad, or negligible.",
        "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
        "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
        "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
        "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
        "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
        "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
        "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
        "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
        "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
        "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
        "The bad connectors (0.8% of the system) hold 36% of all discredit.",
        "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
        "This reiterates that a few ratings influence most of the systems performance.",
        "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
        "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
        "It is not practical to expect a recommender system to have no ratings in bad roles.",
        "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
        "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
        "We employ the principle of non-overlapping episodes [6] to count such transitions.",
        "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
        "See [6] for further details about this counting procedure.",
        "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
        "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
        "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
        "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
        "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
        "Only transitions with frequency greater than or equal to 3% are shown.",
        "The percentages for each state indicates the number of ratings that were found to be in those states.",
        "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
        "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
        "The majority of the transitions involve both good and bad ratings becoming negligible.",
        "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
        "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
        "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
        "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
        "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
        "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
        "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
        "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
        "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
        "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
        "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
        "Inactive user: (SFN(u) = 0) 2.",
        "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
        "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
        "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
        "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
        "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
        "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
        "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
        "The system can be expected to deliver more if they engineer some good scouts.",
        "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
        "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
        "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
        "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
        "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
        "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
        "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
        "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
        "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
        "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
        "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
        "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
        "In Proc.",
        "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
        "A., Borchers, A., and Riedl, J.",
        "An Algorithmic Framework for Performing Collaborative Filtering.",
        "In Proc.",
        "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
        "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
        "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
        "A.",
        "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
        "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
        "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
        "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
        "A., and Riedl, J.",
        "Getting to Know You: Learning New User Preferences in Recommender Systems.",
        "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
        "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
        "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
        "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
        "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
        "In Proc.",
        "SIGIR (2002), pp. 253-260. 259"
    ],
    "translated_text_sentences": [
        "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan.",
        "Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes.",
        "La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico.",
        "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos.",
        "En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí.",
        "Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad.",
        "Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling.",
        "Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad.",
        "Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1.",
        "INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones.",
        "El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios.",
        "Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8].",
        "Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas).",
        "Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo.",
        "Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos.",
        "Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos.",
        "Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones.",
        "A su vez, esta capacidad ayuda a respaldar escenarios como: 1.",
        "Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario.",
        "Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor.",
        "Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados.",
        "Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3.",
        "Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque.",
        "Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado.",
        "Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5].",
        "Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad.",
        "El resto del documento está organizado de la siguiente manera.",
        "El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2.",
        "La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles.",
        "En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2.",
        "FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción.",
        "Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12].",
        "En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos).",
        "Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo.",
        "Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones.",
        "Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12].",
        "El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v).",
        "La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2].",
        "Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u.",
        "Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i.",
        "El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12].",
        "En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i.",
        "En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento.",
        "Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3].",
        "Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento.",
        "Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3].",
        "Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales.",
        "Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista.",
        "Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto.",
        "Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5].",
        "El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3].",
        "Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3].",
        "Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio.",
        "Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista.",
        "ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza.",
        "Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados).",
        "El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles.",
        "Cómo se hace esto depende del algoritmo: 1.",
        "Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask).",
        "Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2.",
        "Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix).",
        "La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim.",
        "Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones.",
        "Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones.",
        "En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom.",
        "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3).",
        "Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3.",
        "Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)).",
        "Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3).",
        "Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho.",
        "Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores.",
        "Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1.",
        "La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask.",
        "Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2.",
        "La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3.",
        "La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom.",
        "Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a.",
        "Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente).",
        "Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios.",
        "Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny.",
        "Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix.",
        "Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos.",
        "Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars.",
        "Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\".",
        "Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema.",
        "Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos.",
        "En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom.",
        "La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan.",
        "Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario.",
        "Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios.",
        "Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio.",
        "Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4.",
        "Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores.",
        "Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario.",
        "Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles.",
        "La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor.",
        "La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno.",
        "Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor.",
        "Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no.",
        "Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema.",
        "Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema.",
        "Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema.",
        "Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado.",
        "La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4.",
        "CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras.",
        "Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción.",
        "Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema.",
        "Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango.",
        "También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario.",
        "Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos.",
        "La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario.",
        "Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)).",
        "Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional.",
        "Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4).",
        "La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos.",
        "Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j.",
        "Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
        "Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
        "La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada.",
        "Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación.",
        "En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel.",
        "Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e).",
        "La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es.",
        "Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1].",
        "Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias.",
        "Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor.",
        "En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo.",
        "Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles).",
        "Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado.",
        "Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones.",
        "Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones.",
        "Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia.",
        "Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε).",
        "De manera similar, un par (i, j) es discordante (con error ) si no es concordante.",
        "Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1.",
        "Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados.",
        "El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias.",
        "El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias.",
        "Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema.",
        "Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario.",
        "Considera la lista de recomendaciones principales presentada a un usuario en un momento específico.",
        "El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones.",
        "Llamamos a estas calificaciones el vecindario recomendador del usuario.",
        "El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica.",
        "Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema.",
        "Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada.",
        "Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones.",
        "Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados.",
        "Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable.",
        "Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores.",
        "Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil.",
        "Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro.",
        "El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones.",
        "Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista.",
        "En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p.",
        "Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i).",
        "El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
        "CFN(u) y PFN(u) están definidos de manera similar.",
        "Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5.",
        "Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo.",
        "En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens.",
        "En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas.",
        "Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones.",
        "Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango.",
        "Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada.",
        "La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí.",
        "Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas).",
        "Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo.",
        "Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario.",
        "A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba.",
        "En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones.",
        "Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles.",
        "Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias.",
        "La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias.",
        "Los resultados aquí se basan en n = 4 repeticiones.",
        "El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación.",
        "La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo.",
        "El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones.",
        "Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos.",
        "Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios.",
        "Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas.",
        "La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero.",
        "La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado.",
        "Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios.",
        "Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos.",
        "Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras.",
        "Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal.",
        "Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9].",
        "Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE.",
        "Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores.",
        "Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema.",
        "Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba.",
        "Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar.",
        "Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos).",
        "Tabla 1: Películas que forman a los mejores exploradores.",
        "Mejor Conferencia de Exploradores.",
        "Plop.",
        "Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores.",
        "Peores exploradores Conf.",
        "Plop.",
        "Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo.",
        "Afirmamos que estas películas serán exploraciones viables para otros usuarios.",
        "Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una.",
        "Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista.",
        "Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro.",
        "La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados.",
        "Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1.",
        "Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas.",
        "Las popularidades de las películas también se muestran.",
        "Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa.",
        "Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva.",
        "Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares.",
        "Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película.",
        "Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron.",
        "Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios.",
        "Por lo tanto, inducir buenos promotores es importante para la recomendación en frío.",
        "Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6).",
        "Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios.",
        "Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado.",
        "La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado.",
        "Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables.",
        "Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados.",
        "Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad.",
        "Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo.",
        "El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores.",
        "De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores.",
        "Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador.",
        "En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector.",
        "Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador.",
        "Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones.",
        "En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%.",
        "El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo.",
        "El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado.",
        "Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación.",
        "Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones.",
        "Clasificamos diferentes estados de calificación como buenos, malos o insignificantes.",
        "Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas.",
        "Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno.",
        "Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor.",
        "Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo.",
        "El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema.",
        "Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño.",
        "Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema!",
        "De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito.",
        "Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante).",
        "Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo.",
        "Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito.",
        "Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito.",
        "Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas.",
        "Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar.",
        "Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente.",
        "No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos.",
        "Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales.",
        "De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema.",
        "Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones.",
        "Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
        "Consulte [6] para obtener más detalles sobre este procedimiento de conteo.",
        "Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles.",
        "Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante.",
        "Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos.",
        "En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás.",
        "La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens.",
        "Solo se muestran las transiciones con una frecuencia mayor o igual al 3%.",
        "Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados.",
        "Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia.",
        "A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol.",
        "La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes.",
        "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts.",
        "Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores.",
        "Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores.",
        "Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores.",
        "Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente.",
        "Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija.",
        "Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación.",
        "Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido.",
        "Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario.",
        "En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario.",
        "Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1.",
        "Usuario inactivo: (SFN(u) = 0) 2.",
        "Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
        "Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
        "Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
        "Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos.",
        "De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario.",
        "Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema.",
        "Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema.",
        "Se espera que el sistema entregue más si diseñan algunos buenos exploradores.",
        "Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño.",
        "Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil.",
        "Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto.",
        "En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6.",
        "CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones.",
        "Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo.",
        "Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas.",
        "En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores.",
        "También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol.",
        "Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7.",
        "REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J.",
        "¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios.",
        "En Proc.",
        "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
        "A., Borchers, A., y Riedl, J.",
        "Un marco algorítmico para realizar filtrado colaborativo.",
        "En Proc.",
        "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
        "A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo.",
        "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
        "I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish?",
        "Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio.",
        "En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal.",
        "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario.",
        "En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez.",
        "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
        "A., y Riedl, J.",
        "Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación.",
        "En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J.",
        "Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo.",
        "En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews.",
        "En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos.",
        "En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío.",
        "En Proc.",
        "SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259"
    ],
    "error_count": 1,
    "keys": {
        "nearest neighbor": {
            "translated_key": "vecinos más cercanos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in <br>nearest neighbor</br> Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in <br>nearest neighbor</br> Collaborative Filtering Bharath Kumar Mohan Dept."
            ],
            "translated_annotated_samples": [
                "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de <br>vecinos más cercanos</br>. Departamento de Bharath Kumar Mohan."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de <br>vecinos más cercanos</br>. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "collaborative filtering algorithm": {
            "translated_key": "algoritmo de filtrado colaborativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based <br>collaborative filtering algorithm</br> constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based <br>collaborative filtering algorithm</br> would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the <br>collaborative filtering algorithm</br> used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The <br>collaborative filtering algorithm</br> traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the <br>collaborative filtering algorithm</br>, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based <br>collaborative filtering algorithm</br>. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the <br>collaborative filtering algorithm</br> used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A <br>collaborative filtering algorithm</br> and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "An item-based <br>collaborative filtering algorithm</br> constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "A user-based <br>collaborative filtering algorithm</br> would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "So, irrespective of the <br>collaborative filtering algorithm</br> used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "The <br>collaborative filtering algorithm</br> traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "Apart from the <br>collaborative filtering algorithm</br>, the health of this neighborhood completely influences a users satisfaction with the system."
            ],
            "translated_annotated_samples": [
                "Un <br>algoritmo de filtrado colaborativo</br> basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask).",
                "Un <br>algoritmo de filtrado colaborativo</br> basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix).",
                "Por lo tanto, independientemente del <br>algoritmo de filtrado colaborativo</br> utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones.",
                "El <br>algoritmo de filtrado colaborativo</br> recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones.",
                "Además del <br>algoritmo de filtrado colaborativo</br>, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un <br>algoritmo de filtrado colaborativo</br> basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un <br>algoritmo de filtrado colaborativo</br> basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del <br>algoritmo de filtrado colaborativo</br> utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El <br>algoritmo de filtrado colaborativo</br> recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del <br>algoritmo de filtrado colaborativo</br>, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "aggregation process": {
            "translated_key": "proceso de agregación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this <br>aggregation process</br> crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "The quality of this <br>aggregation process</br> crucially affects the user experience and hence the effectiveness of recommenders in e-commerce."
            ],
            "translated_annotated_samples": [
                "La calidad de este <br>proceso de agregación</br> afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este <br>proceso de agregación</br> afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "recommender": {
            "translated_key": "sistemas de recomendación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT <br>recommender</br> systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global <br>recommender</br> performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender</br> system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION <br>recommender</br> systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a <br>recommender</br> in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a <br>recommender</br> system, to engineer mechanisms to sustain the <br>recommender</br>, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global <br>recommender</br> performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: <br>recommender</br> systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the <br>recommender</br> system. 3.",
                "Monitoring the evolution of the <br>recommender</br> system and its stakeholders: A <br>recommender</br> system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a <br>recommender</br> system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation <br>recommender</br> algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on <br>recommender</br> algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie <br>recommender</br>. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie <br>recommender</br> system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The <br>recommender</br> predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the <br>recommender</br> to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the <br>recommender</br> system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a <br>recommender</br> system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the <br>recommender</br> neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users <br>recommender</br> neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The <br>recommender</br> neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a <br>recommender</br> system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the <br>recommender</br> system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons <br>recommender</br> makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of <br>recommender</br> systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the <br>recommender</br> system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a <br>recommender</br> system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a <br>recommender</br> system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a <br>recommender</br> system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the <br>recommender</br> system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further <br>recommender</br> system acceptance and deployment, we require new tools and methodologies to manage an installed <br>recommender</br> and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of <br>recommender</br> system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How <br>recommender</br> System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering <br>recommender</br> Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling <br>recommender</br> Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in <br>recommender</br> Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based <br>recommender</br> Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT <br>recommender</br> systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "We present a novel study that disaggregates global <br>recommender</br> performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender</br> system and its community.",
                "INTRODUCTION <br>recommender</br> systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a <br>recommender</br> in terms of the individual ratings and the roles they play in making (good) recommendations."
            ],
            "translated_annotated_samples": [
                "Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los <br>sistemas de recomendación</br> agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes.",
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de <br>recomendaciones</br> globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos.",
                "Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad.",
                "INTRODUCCIÓN Los <br>sistemas de recomendación</br> se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones.",
                "Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un <br>sistema de recomendación</br> en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas)."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los <br>sistemas de recomendación</br> agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de <br>recomendaciones</br> globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los <br>sistemas de recomendación</br> se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un <br>sistema de recomendación</br> en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). ",
            "candidates": [],
            "error": [
                [
                    "sistemas de recomendación",
                    "recomendaciones",
                    "sistema de recomendación",
                    "sistemas de recomendación",
                    "sistema de recomendación"
                ]
            ]
        },
        "rating": {
            "translated_key": "calificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual <br>rating</br>, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify <br>rating</br> subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three <br>rating</br> roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or <br>rating</br> history.",
                "Collaborative filtering, a common form of recommendation, predicts a users <br>rating</br> for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual <br>rating</br>, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a <br>rating</br> in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a <br>rating</br> and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify <br>rating</br> subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of <br>rating</br> roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a <br>rating</br>, and Section 4 defines measures of the contribution of a <br>rating</br> in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us <br>rating</br> for item i, and ¯ru is the average <br>rating</br> of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A <br>rating</br> Our basic observation is that each <br>rating</br> plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the <br>rating</br> values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose <br>rating</br> behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms <br>rating</br> for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms <br>rating</br> of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a <br>rating</br> is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A <br>rating</br> can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The <br>rating</br> p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms <br>rating</br> for The Mask.",
                "This <br>rating</br> serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The <br>rating</br> p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The <br>rating</br> p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a <br>rating</br> ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a <br>rating</br> rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms <br>rating</br> of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the <br>rating</br> behavior of other users.",
                "For example, in Fig. 4 the <br>rating</br> Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by <br>rating</br> The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His <br>rating</br> of The Mask is the best scout for Jeff, and Jerrys only scout is his <br>rating</br> of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims <br>rating</br>, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one <br>rating</br>, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys <br>rating</br> of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every <br>rating</br> can be of varied benefit in each of these roles.",
                "The <br>rating</br> Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The <br>rating</br> Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the <br>rating</br> Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a <br>rating</br> can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a <br>rating</br> is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each <br>rating</br> to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each <br>rating</br> (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a <br>rating</br> according to the role played.",
                "The next section describes the approach to measuring the values of a <br>rating</br> in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a <br>rating</br> may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each <br>rating</br> proportional to its influence in the prediction.",
                "By tracking the role of each <br>rating</br> in a prediction, we can accumulate the credit for a <br>rating</br> in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a <br>rating</br>, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for <br>rating</br> influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean <br>rating</br> as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as <br>rating</br> is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a <br>rating</br> in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a <br>rating</br> scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every <br>rating</br> is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each <br>rating</br>. 4.4 Aggregating <br>rating</br> roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each <br>rating</br> when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million <br>rating</br> dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of <br>rating</br> roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million <br>rating</br> dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the <br>rating</br> was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the <br>rating</br> role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of <br>rating</br> roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every <br>rating</br> is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely <br>rating</br> popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and <br>rating</br> entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A <br>rating</br> with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of <br>rating</br> roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a <br>rating</br> can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of <br>rating</br> roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different <br>rating</br> states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a <br>rating</br> as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a <br>rating</br> r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a <br>rating</br> is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a <br>rating</br> can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a <br>rating</br> can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the <br>rating</br> is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among <br>rating</br> roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users <br>rating</br> is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider <br>rating</br> of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of <br>rating</br> roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering <br>rating</br> roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of <br>rating</br> role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual <br>rating</br>, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify <br>rating</br> subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three <br>rating</br> roles presented here provide broad primitives to manage a recommender system and its community.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or <br>rating</br> history.",
                "Collaborative filtering, a common form of recommendation, predicts a users <br>rating</br> for an item by combining (other) ratings of that user with other users ratings."
            ],
            "translated_annotated_samples": [
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada <br>calificación</br> individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos.",
                "Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de <br>calificación</br> que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling.",
                "Sostenemos que los tres <br>roles de calificación</br> presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad.",
                "INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de <br>calificaciones</br>.",
                "El filtrado colaborativo, una forma común de recomendación, predice la <br>calificación</br> de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada <br>calificación</br> individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de <br>calificación</br> que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres <br>roles de calificación</br> presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de <br>calificaciones</br>. El filtrado colaborativo, una forma común de recomendación, predice la <br>calificación</br> de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. ",
            "candidates": [],
            "error": [
                [
                    "calificación",
                    "calificación",
                    "roles de calificación",
                    "calificaciones",
                    "calificación"
                ]
            ]
        },
        "purchase": {
            "translated_key": "compras",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous <br>purchase</br>s or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were <br>purchase</br>d as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous <br>purchase</br>s or rating history.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were <br>purchase</br>d as gifts."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en <br>compras</br> anteriores o historial de calificaciones.",
                "Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron <br>comprados</br> como regalos."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en <br>compras</br> anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron <br>comprados</br> como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259 ",
            "candidates": [],
            "error": [
                [
                    "compras",
                    "comprados"
                ]
            ]
        },
        "opinion": {
            "translated_key": "opinión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same <br>opinion</br> about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "Two users may have the same <br>opinion</br> about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw."
            ],
            "translated_annotated_samples": [
                "Dos usuarios pueden tener la misma <br>opinión</br> sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma <br>opinión</br> sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "scout": {
            "translated_key": "explorador",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a <br>scout</br> in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the <br>scout</br> to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a <br>scout</br> for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best <br>scout</br> for Jeff, and Jerrys only <br>scout</br> is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor <br>scout</br> and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good <br>scout</br>, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good <br>scout</br>, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a <br>scout</br>, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the <br>scout</br> value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative <br>scout</br> value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable <br>scout</br> values will be happier with the system than those with ratings with low <br>scout</br> values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the <br>scout</br> values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of <br>scout</br> values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated <br>scout</br> values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the <br>scout</br> values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 <br>scout</br> Value Frequency Figure 6: Distribution of <br>scout</br> values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of <br>scout</br> values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated <br>scout</br> values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good <br>scout</br> if the movie was in the top 100 of the sorted list, and to induce a bad <br>scout</br> if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad <br>scout</br> choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of <br>scout</br> based on popularity alone can be potentially dangerous.",
                "Interestingly, the best <br>scout</br>-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad <br>scout</br>, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high <br>scout</br> value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad <br>scout</br>.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a <br>scout</br> has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its <br>scout</br>, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [<br>scout</br> +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good <br>scout</br> can become a bad <br>scout</br>, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a <br>scout</br>, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [<br>scout</br>+] ; [scout0] : 1 [<br>scout</br>+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "<br>scout</br> + (2%) <br>scout</br>(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "This rating serves as a <br>scout</br> in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the <br>scout</br> to the promoter. 3.",
                "Formally, given a prediction pu,a of a target item a for user u, a <br>scout</br> for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "His rating of The Mask is the best <br>scout</br> for Jeff, and Jerrys only <br>scout</br> is his rating of Star Wars.",
                "The rating Jim → My Cousin Vinny is a poor <br>scout</br> and connector, but is a very good promoter."
            ],
            "translated_annotated_samples": [
                "Esta calificación sirve como un <br>explorador</br> en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2.",
                "La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al <br>explorador</br> con el promotor. 3.",
                "Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un <br>explorador</br> para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a.",
                "Su calificación de \"The Mask\" es la mejor <br>referencia</br> para Jeff, y la única <br>referencia</br> de Jerry es su calificación de \"Star Wars\".",
                "La calificación de Jim → Mi primo Vinny es un mal <br>explorador</br> y conector, pero es un muy buen promotor."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un <br>explorador</br> en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al <br>explorador</br> con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un <br>explorador</br> para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor <br>referencia</br> para Jeff, y la única <br>referencia</br> de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal <br>explorador</br> y conector, pero es un muy buen promotor. ",
            "candidates": [],
            "error": [
                [
                    "explorador",
                    "explorador",
                    "explorador",
                    "referencia",
                    "referencia",
                    "explorador"
                ]
            ]
        },
        "promoter": {
            "translated_key": "promotor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the <br>promoter</br>. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a <br>promoter</br> for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good <br>promoter</br> connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good <br>promoter</br>.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good <br>promoter</br>.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or <br>promoter</br>.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a <br>promoter</br>.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the <br>promoter</br> value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated <br>promoter</br> value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of <br>promoter</br> values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated <br>promoter</br> value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated <br>promoter</br> value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate <br>promoter</br> values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated <br>promoter</br> values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking <br>promoter</br> values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high <br>promoter</br> value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad <br>promoter</br>, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and <br>promoter</br> value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, <br>promoter</br> −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good <br>promoter</br> can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, <br>promoter</br>, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [<br>promoter</br>+] ; [scout0] : 1 [<br>promoter</br>+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) <br>promoter</br> + (3%) <br>promoter</br>(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-<br>promoter</br> pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the <br>promoter</br>. 3.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a <br>promoter</br> for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "We conclude that a good <br>promoter</br> connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good <br>promoter</br>.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good <br>promoter</br>."
            ],
            "translated_annotated_samples": [
                "La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el <br>promotor</br>. 3.",
                "Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un <br>promotor</br> para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a.",
                "Concluimos que un buen <br>promotor</br> conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios.",
                "La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen <br>promotor</br>.",
                "La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un <br>promotor</br> bueno."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el <br>promotor</br>. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un <br>promotor</br> para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen <br>promotor</br> conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen <br>promotor</br>. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un <br>promotor</br> bueno. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "connector": {
            "translated_key": "conector",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a <br>connector</br> for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A <br>connector</br> improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and <br>connector</br>, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good <br>connector</br>, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a <br>connector</br> or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a <br>connector</br>, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the <br>connector</br> value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a <br>connector</br>.",
                "A rating with a poor <br>connector</br> value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, <br>connector</br> and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, <br>connector</br> 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good <br>connector</br>, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or <br>connector</br>, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [<br>connector</br>+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [<br>connector</br>+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) <br>connector</br> + (1.2%) <br>connector</br>(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some <br>connector</br>-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a <br>connector</br> for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "A <br>connector</br> improves the systems ability to make recommendations with no explicit gain for the user.",
                "The rating Jim → My Cousin Vinny is a poor scout and <br>connector</br>, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good <br>connector</br>, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a <br>connector</br> or promoter."
            ],
            "translated_annotated_samples": [
                "Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un <br>conector</br> para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a.",
                "Un <br>conector</br> mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario.",
                "La calificación de Jim → Mi primo Vinny es un mal <br>explorador y conector</br>, pero es un muy buen promotor.",
                "La calificación de Jim → The Mask es un explorador razonablemente bueno, un <br>conector</br> muy bueno y un promotor bueno.",
                "Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como <br>conector</br> o promotor."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un <br>conector</br> para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un <br>conector</br> mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal <br>explorador y conector</br>, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un <br>conector</br> muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como <br>conector</br> o promotor. ",
            "candidates": [],
            "error": [
                [
                    "conector",
                    "conector",
                    "explorador y conector",
                    "conector",
                    "conector"
                ]
            ]
        },
        "list rank accuracy": {
            "translated_key": "precisión de rango de lista",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with <br>list rank accuracy</br> as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "Although we have presented results on only the item-based algorithm with <br>list rank accuracy</br> as the metric, the same approach outlined here applies to user-based algorithms and other metrics."
            ],
            "translated_annotated_samples": [
                "Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con <br>precisión de rango de lista</br> como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para \"The Mask\" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de \"The Mask\" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de \"The Mask\" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de \"Star Wars\". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con <br>precisión de rango de lista</br> como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. \n\nSIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. \n\nACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. \n\nACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259\n\nSIGIR (2002), pp. 253-260. 259 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "neighborhood": {
            "translated_key": "vecindario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a <br>neighborhood</br> for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better <br>neighborhood</br>. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static <br>neighborhood</br> Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the <br>neighborhood</br> Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a <br>neighborhood</br> of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a <br>neighborhood</br> around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users <br>neighborhood</br>, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader <br>neighborhood</br> of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the <br>neighborhood</br> of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a <br>neighborhood</br>.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his <br>neighborhood</br> through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender <br>neighborhood</br> of the user.",
                "The user implicitly chooses this <br>neighborhood</br> of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this <br>neighborhood</br> completely influences a users satisfaction with the system.",
                "We can characterize a users recommender <br>neighborhood</br> by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users <br>neighborhood</br> wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy <br>neighborhood</br>.",
                "A user can have a good set of scouts, but may be exposed to a <br>neighborhood</br> with bad connectors and promoters.",
                "He can have a good <br>neighborhood</br>, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users <br>neighborhood</br> is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender <br>neighborhood</br> of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable <br>neighborhood</br>, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the <br>neighborhood</br> of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the <br>neighborhood</br> of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the <br>neighborhood</br> of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their <br>neighborhood</br> characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good <br>neighborhood</br>: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad <br>neighborhood</br>: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good <br>neighborhood</br>: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad <br>neighborhood</br>: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good <br>neighborhood</br>, 6 had bad scouts and a good <br>neighborhood</br>, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good <br>neighborhood</br> can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad <br>neighborhood</br> are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a <br>neighborhood</br> for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better <br>neighborhood</br>. 2.",
                "These new similarities are then used to define a static <br>neighborhood</br> Nu for each user u consisting of the top K users most similar to user u.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the <br>neighborhood</br> Ni of an item i is defined as the top K most similar items for that item.",
                "An item-based collaborative filtering algorithm constructs a <br>neighborhood</br> of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask)."
            ],
            "translated_annotated_samples": [
                "Ubicar a los usuarios en <br>vecindario</br>s mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un <br>vecindario</br> cuyos gustos no sean una combinación perfecta para el usuario.",
                "Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un <br>vecindario</br> mejor.",
                "Estas nuevas similitudes se utilizan luego para definir un <br>vecindario</br> estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u.",
                "En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el <br>vecindario</br> Ni de un elemento i se define como los K elementos más similares para ese elemento.",
                "Un algoritmo de filtrado colaborativo basado en elementos construye un <br>vecindario</br> de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask)."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en <br>vecindario</br>s mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un <br>vecindario</br> cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un <br>vecindario</br> mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un <br>vecindario</br> estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el <br>vecindario</br> Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un <br>vecindario</br> de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "recommender system": {
            "translated_key": "sistema de recomendación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender system</br> and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a <br>recommender system</br>, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the <br>recommender system</br>. 3.",
                "Monitoring the evolution of the <br>recommender system</br> and its stakeholders: A <br>recommender system</br> is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a <br>recommender system</br> and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie <br>recommender system</br> with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the <br>recommender system</br> suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a <br>recommender system</br> that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a <br>recommender system</br> through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the <br>recommender system</br> is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the <br>recommender system</br>.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a <br>recommender system</br>.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a <br>recommender system</br> to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a <br>recommender system</br>.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the <br>recommender system</br> or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further <br>recommender system</br> acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of <br>recommender system</br> health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How <br>recommender system</br> Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "We argue that the three rating roles presented here provide broad primitives to manage a <br>recommender system</br> and its community.",
                "Such an understanding will give an important handle to monitoring and managing a <br>recommender system</br>, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the <br>recommender system</br>. 3.",
                "Monitoring the evolution of the <br>recommender system</br> and its stakeholders: A <br>recommender system</br> is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a <br>recommender system</br> and its community."
            ],
            "translated_annotated_samples": [
                "Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad.",
                "Tal comprensión proporcionará un importante recurso para monitorear y gestionar un <br>sistema de recomendación</br>, para diseñar mecanismos que mantengan el <br>sistema de recomendación</br> y, de esta manera, garantizar su éxito continuo.",
                "Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del <br>sistema de recomendaciones</br>. 3.",
                "Monitoreando la evolución del <br>sistema de recomendación</br> y sus partes interesadas: Un <br>sistema de recomendación</br> está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque.",
                "Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un <br>sistema de recomendación</br>, para diseñar mecanismos que mantengan el <br>sistema de recomendación</br> y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del <br>sistema de recomendaciones</br>. 3. Monitoreando la evolución del <br>sistema de recomendación</br> y sus partes interesadas: Un <br>sistema de recomendación</br> está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un <br>sistema de recomendación</br> y su comunidad. ",
            "candidates": [],
            "error": [
                [
                    "sistema de recomendación",
                    "sistema de recomendación",
                    "sistema de recomendación",
                    "sistema de recomendaciones",
                    "sistema de recomendación",
                    "sistema de recomendación",
                    "sistema de recomendación"
                ]
            ]
        },
        "collaborative filter": {
            "translated_key": "filtrado colaborativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor <br>collaborative filter</br>ing.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate <br>collaborative filter</br>ing algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor <br>collaborative filter</br>ing.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor <br>collaborative filter</br>ing and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor <br>collaborative filter</br>ing algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based <br>collaborative filter</br>ing algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based <br>collaborative filter</br>ing algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the <br>collaborative filter</br>ing algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a <br>collaborative filter</br>ing algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based <br>collaborative filter</br>ing.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to <br>collaborative filter</br>ing relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The <br>collaborative filter</br>ing algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the <br>collaborative filter</br>ing algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a <br>collaborative filter</br>ing system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based <br>collaborative filter</br>ing algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based <br>collaborative filter</br>ing algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a <br>collaborative filter</br>ing system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the <br>collaborative filter</br>ing algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other <br>collaborative filter</br>ing algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor <br>collaborative filter</br>ing.",
                "Significant research has been conducted in implementing fast and accurate <br>collaborative filter</br>ing algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor <br>collaborative filter</br>ing.",
                "Background on nearest-neighbor <br>collaborative filter</br>ing and algorithm evaluation is discussed in Section 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor <br>collaborative filter</br>ing algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction."
            ],
            "translated_annotated_samples": [
                "Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el <br>filtrado colaborativo</br> de vecinos más cercanos.",
                "Se ha realizado una investigación significativa en la implementación de algoritmos de <br>filtrado colaborativo</br> rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8].",
                "Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el <br>filtrado colaborativo</br> de vecinos más cercanos.",
                "El fondo sobre el <br>filtrado colaborativo</br> de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2.",
                "FONDO 2.1 Algoritmos Los algoritmos de <br>filtrado colaborativo</br> de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción."
            ],
            "translated_text": "Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el <br>filtrado colaborativo</br> de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de <br>filtrado colaborativo</br> rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el <br>filtrado colaborativo</br> de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el <br>filtrado colaborativo</br> de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de <br>filtrado colaborativo</br> de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "user-base and item-base algorithm": {
            "translated_key": "algoritmo basado en usuarios y algoritmo basado en elementos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering Bharath Kumar Mohan Dept.",
                "of CSA Indian Institute of Science Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. of Computer Science Eastern Michigan University Ypsilanti, MI 48917, USA bkeller@emich.edu Naren Ramakrishnan Dept. of Computer Science Virginia Tech, Blacksburg VA 24061, USA naren@cs.vt.edu ABSTRACT Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors.",
                "The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce.",
                "We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering.",
                "In particular, we formulate three roles-scouts, promoters, and connectors-that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.).",
                "These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole.",
                "For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling.",
                "We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.",
                "Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems-Decision support; J.4 [Computer Applications]: Social and Behavioral Sciences General Terms Algorithms, Human Factors 1.",
                "INTRODUCTION Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.",
                "Collaborative filtering, a common form of recommendation, predicts a users rating for an item by combining (other) ratings of that user with other users ratings.",
                "Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8].",
                "However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations.",
                "Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.",
                "Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.",
                "We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds.",
                "Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations.",
                "In turn, this capability helps support scenarios such as: 1.",
                "Situating users in better neighborhoods: A users ratings may inadvertently connect the user to a neighborhood for which the users tastes may not be a perfect match.",
                "Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood. 2.",
                "Targeting items: Recommender systems suffer from lack of user participation, especially in cold-start scenarios [13] involving newly arrived items.",
                "Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system. 3.",
                "Monitoring the evolution of the recommender system and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack.",
                "Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.",
                "These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].",
                "As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community.",
                "The rest of the paper is organized as follows.",
                "Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2.",
                "Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles.",
                "In Section 5, we illustrate the use of these roles to address the goals outlined above. 2.",
                "BACKGROUND 2.1 Algorithms Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction.",
                "An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12].",
                "In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).",
                "Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the users neighbors and, in an item-based algorithm, involves aggregating the users ratings of items that are neighbors of the target item.",
                "Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.",
                "We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].",
                "The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user us rating for item i, and ¯ru is the average rating of user u (similarly for v).",
                "Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].",
                "These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u.",
                "A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.",
                "The item-based algorithm we use is the one defined by Sarwar et al. [12].",
                "In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i.",
                "As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item.",
                "A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i. 2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].",
                "Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set.",
                "A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].",
                "A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions.",
                "Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list.",
                "So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order.",
                "Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendalls Tau to measures that consider the fact that users tend to only look at a prefix of the list [5].",
                "Kendalls Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3].",
                "A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3].",
                "For instance, an inversion toward the end of the list is given the same weight as one in the beginning.",
                "One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. 3.",
                "ROLES OF A RATING Our basic observation is that each rating plays a different role in each prediction in which it is used.",
                "Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.)",
                "The recommender predicts whether Tom will like The Mask using the other already available ratings.",
                "How this is done depends on the algorithm: 1.",
                "An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jims ratings of The Matrix and The Mask; and Jeffs ratings of Star Wars and The Mask).",
                "Toms ratings of those movies are then used to make a prediction for The Mask. 2.",
                "A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Toms (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix).",
                "The prediction of Toms rating for The Mask is then based on the ratings of Jeff and Tim.",
                "Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings.",
                "So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Toms rating of The Mask as walking through a sequence of ratings.",
                "In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.",
                "Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3).",
                "Note that these paths are undirected, and are all of length 3.",
                "Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).)",
                "A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).",
                "Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value.",
                "Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors.",
                "To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom: 1.",
                "The rating p1 (Tom → Star Wars) makes a connection from Tom to other ratings that can be used to predict Toms rating for The Mask.",
                "This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask. 2.",
                "The rating p2 (Jeff → Star Wars) helps the system recommend The Mask to Tom by connecting the scout to the promoter. 3.",
                "The rating p3 (Jeff → The Mask) allows connections to The Mask, and, therefore, promotes this movie to Tom.",
                "Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a.",
                "The scouts, connectors, and promoters for the prediction of Toms rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively).",
                "Each of these roles has a value in the recommender to the user, the users neighborhood, and the system in terms of allowing recommendations to be made. 3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users.",
                "For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny.",
                "Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix.",
                "However, this does not make The Matrix the best movie to rate for everyone.",
                "For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him.",
                "His rating of The Mask is the best scout for Jeff, and Jerrys only scout is his rating of Star Wars.",
                "This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.",
                "While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items.",
                "In Fig. 4, My Cousin Vinny benefits from Jims rating, since it allows recommendations to Jeff and Tom.",
                "The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it.",
                "On the other hand, Jerrys rating of Star Wars does not help promote it to any other user.",
                "We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.",
                "Connectors serve a crucial role in a recommender system that is not as obvious.",
                "The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4.",
                "Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors.",
                "A connector improves the systems ability to make recommendations with no explicit gain for the user.",
                "Note that every rating can be of varied benefit in each of these roles.",
                "The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter.",
                "The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter.",
                "Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter.",
                "As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not.",
                "We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system.",
                "But we can also measure the contribution of each rating to the quality of recommendations or health of the system.",
                "Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the systems overall error.",
                "We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played.",
                "The next section describes the approach to measuring the values of a rating in each role. 4.",
                "CONTRIBUTIONS OF RATINGS As weve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways.",
                "Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction.",
                "By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.",
                "This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy.",
                "We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a users recommendations.",
                "Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering.",
                "The definition for user-based algorithms is similar and will be presented in an expanded version of this paper. 4.1 Influence of Ratings Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user.",
                "As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)).",
                "A set of the top K neighbors is maintained for all items for space and computational efficiency.",
                "A prediction of item i for a user u is computed as the weighted deviation from the items mean rating as shown in Equation (4).",
                "The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.",
                "We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j.",
                "This is the change in the similarity between i and j when as rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\\{a}(ru,i − ¯ru)2 qP u∈U\\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).",
                "If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.",
                "The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1. 4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed.",
                "Here we illustrate the approach using predictive accuracy as the evaluation metric.",
                "In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role.",
                "For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e).",
                "Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not.",
                "We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].",
                "For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences.",
                "This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter.",
                "In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount.",
                "Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles).",
                "Every rating is thus summarized by its triple [SF, CF, PF]. 4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy.",
                "For this computation, we must know the users preference order for a set of items for which predictions can be computed.",
                "We assume that we have a test set of the users ratings of the items presented in the recommendation list.",
                "For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference.",
                "We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).",
                "Similarly, a pair (i, j) is discordant (with error ) if it is not concordant.",
                "Our experiments described below use an error tolerance of = 0.1.",
                "All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited.",
                "The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the users test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances.",
                "The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences.",
                "We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating. 4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system.",
                "Here we consider how we can use the role values to characterize the health of a neighborhood.",
                "Consider the list of top recommendations presented to a user at a specific point in time.",
                "The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations.",
                "We call these ratings the recommender neighborhood of the user.",
                "The user implicitly chooses this neighborhood of ratings through the items he rates.",
                "Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a users satisfaction with the system.",
                "We can characterize a users recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list.",
                "Different sections of the users neighborhood wield varied influence on his recommendation list.",
                "For instance, ratings reachable through highly rated items have a bigger say in the recommended items.",
                "Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood.",
                "A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters.",
                "He can have a good neighborhood, but his bad scouts may ensure the neighborhoods potential is rendered useless.",
                "We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.",
                "A users neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations.",
                "Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list.",
                "In our studies we use K(i) = p position(i).",
                "Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).",
                "The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) !",
                "CFN(u) and PFN(u) are defined similarly.",
                "This triple estimates the quality of us recommendations based on the past track record of the ratings involved in their respective roles. 5.",
                "EXPERIMENTATION As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system.",
                "In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset.",
                "In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods. 5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies.",
                "The ratings are in the range 1 to 5, and are labeled with the time the rating was given.",
                "As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.",
                "Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order.",
                "The timestamping provided by MovieLens is hence crucial for the analyses presented here.",
                "We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots).",
                "We incrementally update the role values as the time ordered ratings are merged into the model.",
                "To keep the experiment computationally manageable, we define a test dataset for each user.",
                "As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data.",
                "At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions.",
                "One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles.",
                "We overcome this concern by repeating the experiment, using different random seeds.",
                "The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds.",
                "The results here are based on n = 4 repetitions.",
                "The item-based collaborative filtering algorithms performance was ordinary with respect to rank accuracy.",
                "Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model.",
                "The recall was always high, but the average precision was just about 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm. 5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations.",
                "We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values.",
                "Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the users preference.",
                "Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.",
                "The distribution of the scout values for most users ratings are Gaussian with mean zero.",
                "Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot.",
                "We observe that a large number of ratings never serve as scouts for their users.",
                "A relatable scenario is when Amazons recommender makes suggestions of books or items based on other items that were purchased as gifts.",
                "With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions.",
                "Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.",
                "An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9].",
                "They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE.",
                "Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts.",
                "We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system.",
                "Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved.",
                "Both popularity and popularity*variance performed similarly.",
                "A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).",
                "Table 1: Movies forming the best scouts.",
                "Best Scouts Conf.",
                "Pop.",
                "Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindlers List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.",
                "Worst scouts Conf.",
                "Pop.",
                "Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 Its a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.",
                "We claim these movies will make viable scouts for other users.",
                "We found the aggregated scout values for all movies in intervals of 10,000 ratings each.",
                "A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list.",
                "Movies appearing consistently high over time are expected to remain up there in the future.",
                "The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered.",
                "Using this measure, the top few movies expected to induce the best scouts are shown in Table 1.",
                "Movies that would be bad scout choices are shown in Table 2 with their associated confidences.",
                "The popularities of the movies are also displayed.",
                "Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.",
                "Interestingly, the best scout-Being John Malkovich-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as makes you feel giddy, seriously weird, comedy with depth, silly, strange, and inventive.",
                "Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.",
                "On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie.",
                "Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw.",
                "Bad scouts ensue when there is deviation in behavior around a common synchronization point. 5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users.",
                "So, inducing good promoters is important for cold-start recommendation.",
                "We note that the frequency distribution of promoter values for a sample movies ratings is also Gaussian (similar to Fig. 6).",
                "This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.",
                "We find a strong correlation between a users number of ratings and his aggregated promoter value.",
                "Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value.",
                "We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.",
                "Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values.",
                "A new movie is thus cast into the neighborhood of many other movies improving its visibility.",
                "Note, though, that a user may have long stopped using the system.",
                "Tracking promoter values consistently allows only the most active recent users to be considered. 5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors.",
                "Similarly, the users that constitute the best promoters are also part of the best connectors.",
                "Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value.",
                "In our experiments, we find that a ratings longest standing role is often as a connector.",
                "A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout.",
                "Such ratings can be removed from the prediction process to bring marginal improvements to recommendations.",
                "In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the systems overall performance by 1.5%.",
                "The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall. 5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time.",
                "The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used.",
                "Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system.",
                "We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.",
                "We classify different rating states as good, bad, or negligible.",
                "Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set.",
                "If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good.",
                "Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value.",
                "For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad.",
                "The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system.",
                "Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size.",
                "For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system!",
                "Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit.",
                "Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later).",
                "Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit.",
                "The bad connectors (0.8% of the system) hold 36% of all discredit.",
                "Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit.",
                "This reiterates that a few ratings influence most of the systems performance.",
                "Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change.",
                "A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on.",
                "It is not practical to expect a recommender system to have no ratings in bad roles.",
                "However, it suffices to see ratings in bad roles either convert to good or vestigial roles.",
                "Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.",
                "We employ the principle of non-overlapping episodes [6] to count such transitions.",
                "A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.",
                "See [6] for further details about this counting procedure.",
                "Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions.",
                "We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role.",
                "Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states.",
                "In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.",
                "Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.",
                "Only transitions with frequency greater than or equal to 3% are shown.",
                "The percentages for each state indicates the number of ratings that were found to be in those states.",
                "We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.",
                "From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role.",
                "The majority of the transitions involve both good and bad ratings becoming negligible.",
                "Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts.",
                "Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts.",
                "Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors.",
                "As indicated earlier, there are hardly any transitions from promoters/connectors to scouts.",
                "This indicates that, over the long run, a users rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.",
                "Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time.",
                "However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system.",
                "For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny.",
                "Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research. 5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user.",
                "In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics.",
                "To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: 1.",
                "Inactive user: (SFN(u) = 0) 2.",
                "Good scouts, Good neighborhood: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3.",
                "Good scouts, Bad neighborhood: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4.",
                "Bad scouts, Good neighborhood: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5.",
                "Bad scouts, Bad neighborhood: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive.",
                "Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood.",
                "Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.",
                "As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the systems recommendations.",
                "The system can be expected to deliver more if they engineer some good scouts.",
                "Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage.",
                "Handling users with bad scouts and bad neighborhoods is a more difficult challenge.",
                "Such a classification allows the use of different strategies to better a users experience with the system depending on his context.",
                "In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. 6.",
                "CONCLUSIONS To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings.",
                "A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor.",
                "Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.",
                "In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions.",
                "We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values.",
                "Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health. 7.",
                "REFERENCES [1] Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J.",
                "Is Seeing Believing? : How Recommender System Interfaces Affect Users Opinions.",
                "In Proc.",
                "CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J.",
                "A., Borchers, A., and Riedl, J.",
                "An Algorithmic Framework for Performing Collaborative Filtering.",
                "In Proc.",
                "SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J.",
                "A., Terveen, L. G., and Riedl, J. T. Evaluating Collaborative Filtering Recommender Systems.",
                "ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J.",
                "A.",
                "Personal communication. 2003. [5] Lam, S. K., and Riedl, J. Shilling Recommender Systems for Fun and Profit.",
                "In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S., and Unnikrishnan, K. P. Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection.",
                "IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., and Herlocker, J. L. A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., and Silvestre, G. Collaborative Recommendation: A Robustness Analysis.",
                "ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J.",
                "A., and Riedl, J.",
                "Getting to Know You: Learning New User Preferences in Recommender Systems.",
                "In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., and Riedl, J.",
                "Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.",
                "In Proc. of the SIAM International Conference on Data Mining (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J. GroupLens: An Open Architecture for Collaborative Filtering of Netnews.",
                "In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-Based Collaborative Filtering Recommendation Algorithms.",
                "In Proceedings of the Tenth International World Wide Web Conference (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., and Pennock, D. Methods and Metrics for Cold-Start Recommendation.",
                "In Proc.",
                "SIGIR (2002), pp. 253-260. 259"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}