{
    "id": "I-37",
    "original_text": "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents. We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes. This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions. We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms. We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework. General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1. INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance. Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]). All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem). Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems. In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.). Examples for applications of this kind abound. Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners. In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]). Furthermore, agents might have a vested interest in negatively affecting other agents learning performance. An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud. Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle. Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance. Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results). Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents. To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3). We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4). Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2. ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process. To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space. This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise. For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}. For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k). For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined). Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework. For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk. In other words, one learning step always consists of applying the update function to all samples in D exactly once. We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt. Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht). We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively. The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H). A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step. We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g). The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses. Such specifications allow agents to provide a selfdescription of their learning process. However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others. For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process. This allows us to describe properties of a learning process without specifying its details exhaustively. As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary). Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment). Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below. In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents. For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents. This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment. Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action. In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched). Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment. The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed. More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data). The goal of the agent is to output an optimal learning step in each iteration given the information that it has. One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing. This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case). In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information. For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings). Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i). Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1. In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }. To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i. For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether. More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables. Note that the list does not include any modifications to gj. This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance). Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj. Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components. In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . . Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3. APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data. In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts. This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information. Figure 2 shows a screenshot of our simulation system. It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour. Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account. While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts. To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user. However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results. Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1. The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions. Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions. Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i). In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles). This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space. For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance. For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi. Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|. These two sets assess the agents ability to detect suspicious vessels. For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid. Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi. With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi. As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality. In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model. We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP. Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model. If the 682 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues. To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process. Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively). If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else. If two agents make a deal, they exchange their learning hypotheses (models). In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided). As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj. Whenever m is large enough to encompass all clusters, we simply write join or filter for them. In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture. While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4. EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively. We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent. The Single Agent is learning from the whole dataset. The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9]. For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size. For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time. During each experiment the learning agents receive ship descriptions in batches of 10 samples. Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary. Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties. The validation set contains 100 real and 100 randomly generated fake ships. To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships). In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners. Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms. As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases. This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents). We can see that the quality of these operations is very close to the Single Agent that has access to all training data. For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies. This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents. This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance. Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process. The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5. RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems. Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori. A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm. A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour. The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents. Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method). An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm. The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners. A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering. Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks. The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses. Agents were able to critique each others hypotheses until agreement was reached. However, all agents in this system were identical and the system was strictly cooperative. The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system. As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6. CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining. This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity. To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour. Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation. Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free. This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results. Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical). In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes. Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be. These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject. Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7. REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky. Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters. In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi. An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants. Machine Learning, 36, 1999. [5] P. Berkhin. Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar. Agents that Learn from Distributed Dynamic Data sources. In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O. Hall. Creating ensembles of classifiers. In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper. Model Averaging for Prediction with Discrete Bayesian Networks. Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin. A Cluster Separation Measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies. A Heterogeneous Multi-Agent Learning System. In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu. A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing. In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker. Coactive Learning for Distributed Data Mining. In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro. Agent-based distributed data mining: The KDEC scheme. In AgentLink, number 2586 in LNCS. Springer, 2003. [14] T. M. Mitchell. Machine Learning, pages 29-36. McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza. Recycling Data for Multi-Agent Learning. In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B. Park and H. Kargupta. Distributed Data Mining: Algorithms, Systems, and Applications. In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy. Scaling up: Distributed machine learning with cooperation. In Proc. of AAAI-96, pages 74-79. AAAI Press, 1996. [19] S. Sian. Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml). In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456. Springer-Verlag, 1991. [20] R. Smith. The contract-net protocol: High-level communication and control in a distributed problem solver. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan. Jam: Java Agents for Meta-Learning over Distributed Databases. In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek. Market-Inspired Approach to Collaborative Learning. In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227. Springer, 2006. [23] Y. Z. Wei, L. Moreau, and N. R. Jennings. Recommender systems: a market-based design. In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß. A Multiagent Perspective of Parallel and Distributed Machine Learning. In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg. What is multi in multi-agent learning? Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685",
    "original_translation": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685",
    "original_sentences": [
        "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
        "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
        "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
        "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
        "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
        "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
        "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
        "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
        "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
        "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
        "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
        "Examples for applications of this kind abound.",
        "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
        "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
        "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
        "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
        "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
        "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
        "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
        "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
        "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
        "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
        "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
        "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
        "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
        "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
        "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
        "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
        "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
        "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
        "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
        "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
        "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
        "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
        "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
        "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
        "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
        "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
        "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
        "Such specifications allow agents to provide a selfdescription of their learning process.",
        "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
        "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
        "This allows us to describe properties of a learning process without specifying its details exhaustively.",
        "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
        "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
        "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
        "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
        "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
        "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
        "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
        "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
        "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
        "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
        "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
        "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
        "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
        "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
        "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
        "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
        "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
        "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
        "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
        "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
        "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
        "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
        "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
        "Note that the list does not include any modifications to gj.",
        "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
        "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
        "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
        "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
        "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
        "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
        "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
        "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
        "Figure 2 shows a screenshot of our simulation system.",
        "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
        "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
        "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
        "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
        "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
        "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
        "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
        "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
        "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
        "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
        "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
        "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
        "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
        "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
        "These two sets assess the agents ability to detect suspicious vessels.",
        "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
        "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
        "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
        "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
        "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
        "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
        "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
        "If the 682 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
        "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
        "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
        "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
        "If two agents make a deal, they exchange their learning hypotheses (models).",
        "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
        "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
        "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
        "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
        "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
        "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
        "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
        "The Single Agent is learning from the whole dataset.",
        "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
        "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
        "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
        "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
        "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
        "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
        "The validation set contains 100 real and 100 randomly generated fake ships.",
        "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
        "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
        "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
        "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
        "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
        "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
        "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
        "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
        "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
        "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
        "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
        "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
        "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
        "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
        "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
        "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
        "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
        "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
        "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
        "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
        "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
        "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
        "Agents were able to critique each others hypotheses until agreement was reached.",
        "However, all agents in this system were identical and the system was strictly cooperative.",
        "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
        "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
        "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
        "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
        "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
        "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
        "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
        "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
        "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
        "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
        "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
        "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
        "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
        "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
        "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
        "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
        "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
        "Machine Learning, 36, 1999. [5] P. Berkhin.",
        "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
        "Agents that Learn from Distributed Dynamic Data sources.",
        "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
        "Hall.",
        "Creating ensembles of classifiers.",
        "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
        "Model Averaging for Prediction with Discrete Bayesian Networks.",
        "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
        "A Cluster Separation Measure.",
        "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
        "A Heterogeneous Multi-Agent Learning System.",
        "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
        "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
        "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
        "Coactive Learning for Distributed Data Mining.",
        "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
        "Agent-based distributed data mining: The KDEC scheme.",
        "In AgentLink, number 2586 in LNCS.",
        "Springer, 2003. [14] T. M. Mitchell.",
        "Machine Learning, pages 29-36.",
        "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
        "Recycling Data for Multi-Agent Learning.",
        "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
        "Cooperative multi-agent learning: The state of the art.",
        "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
        "Park and H. Kargupta.",
        "Distributed Data Mining: Algorithms, Systems, and Applications.",
        "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
        "Scaling up: Distributed machine learning with cooperation.",
        "In Proc. of AAAI-96, pages 74-79.",
        "AAAI Press, 1996. [19] S. Sian.",
        "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
        "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
        "Springer-Verlag, 1991. [20] R. Smith.",
        "The contract-net protocol: High-level communication and control in a distributed problem solver.",
        "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
        "Jam: Java Agents for Meta-Learning over Distributed Databases.",
        "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
        "Market-Inspired Approach to Collaborative Learning.",
        "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
        "Springer, 2006. [23] Y.",
        "Z. Wei, L. Moreau, and N. R. Jennings.",
        "Recommender systems: a market-based design.",
        "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
        "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
        "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
        "What is multi in multi-agent learning?",
        "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
    ],
    "translated_text_sentences": [
        "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje.",
        "Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos.",
        "Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje.",
        "Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje.",
        "Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo.",
        "Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1.",
        "En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje.",
        "Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]).",
        "Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje).",
        "Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos.",
        "En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.).",
        "Ejemplos de aplicaciones de este tipo abundan.",
        "Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices.",
        "En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales).",
        "Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes.",
        "Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes.",
        "Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio.",
        "A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje.",
        "Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje).",
        "Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos.",
        "Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3).",
        "Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4).",
        "Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2.",
        "Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje.",
        "Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado.",
        "Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos.",
        "Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}.",
        "Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k).",
        "Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje).",
        "A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo.",
        "Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk.",
        "En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez.",
        "Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt.",
        "Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht).",
        "Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente.",
        "La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H).",
        "Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual.",
        "Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g).",
        "La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis.",
        "Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje.",
        "Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros.",
        "Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto.",
        "Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles.",
        "Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios).",
        "Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno).",
        "Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación.",
        "En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje.",
        "Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje.",
        "Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno.",
        "Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje.",
        "En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás).",
        "Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno.",
        "La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente.",
        "De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados).",
        "El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone.",
        "Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
        "Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente.",
        "Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial).",
        "En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información.",
        "Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos).",
        "Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i).",
        "Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1.",
        "En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }.",
        "Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i.",
        "Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci.",
        "Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite.",
        "Ten en cuenta que la lista no incluye ninguna modificación a gj.",
        "Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje).",
        "También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj.",
        "Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes.",
        "En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . .",
        "Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3.",
        "EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]).",
        "En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves.",
        "Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad.",
        "La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación.",
        "Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos.",
        "Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada.",
        "Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos.",
        "Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano.",
        "Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general.",
        "Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1.",
        "Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje.",
        "En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones.",
        "Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i).",
        "En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje).",
        "Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|.",
        "Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana.",
        "Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi.",
        "Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|.",
        "Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas.",
        "Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano.",
        "Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi.",
        "Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi.",
        "En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico.",
        "En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual.",
        "Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP.",
        "Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo.",
        "Si el 682 El Sexto Internacional.",
        "Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje.",
        "Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje.",
        "En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente).",
        "Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso.",
        "Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos).",
        "En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g).",
        "En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj.",
        "Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos.",
        "En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta.",
        "Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4.",
        "RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente.",
        "Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje.",
        "El Agente Único está aprendiendo de todo el conjunto de datos.",
        "El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9].",
        "Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo).",
        "Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo.",
        "Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras.",
        "Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario.",
        "Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades.",
        "El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente.",
        "Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes).",
        "En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados.",
        "Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes.",
        "Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos.",
        "Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes).",
        "Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento.",
        "Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas.",
        "Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos.",
        "Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual.",
        "Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje.",
        "El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5.",
        "TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido.",
        "Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano.",
        "Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo.",
        "Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global.",
        "La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes.",
        "Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico).",
        "Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular.",
        "Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices.",
        "Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido.",
        "Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas.",
        "El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis.",
        "Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo.",
        "Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo.",
        "El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo.",
        "Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo.",
        "CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos.",
        "Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar.",
        "Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos.",
        "Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple.",
        "Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita.",
        "Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje.",
        "En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos).",
        "En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes.",
        "Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma.",
        "Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto.",
        "Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232.",
        "REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky.",
        "Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres.",
        "En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi.",
        "Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes.",
        "Aprendizaje automático, 36, 1999. [5] P. Berkhin.",
        "Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar.",
        "Agentes que aprenden de fuentes de datos dinámicas distribuidas.",
        "En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O.",
        "Sala.",
        "Creando conjuntos de clasificadores.",
        "En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper.",
        "Promediando modelos para predicción con redes bayesianas discretas.",
        "Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin.",
        "Una medida de separación de clústeres.",
        "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies.",
        "Un sistema de aprendizaje multiagente heterogéneo.",
        "En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu.",
        "Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento.",
        "En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker.",
        "Aprendizaje coactivo para la minería de datos distribuida.",
        "En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro.",
        "Minería de datos distribuida basada en agentes: El esquema KDEC.",
        "En AgentLink, número 2586 en LNCS.",
        "Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell.",
        "Aprendizaje automático, páginas 29-36.",
        "McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza.",
        "Reciclaje de datos para el aprendizaje multiagente.",
        "En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke.",
        "Aprendizaje cooperativo de múltiples agentes: El estado del arte.",
        "Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B.",
        "Parque y H. Kargupta.",
        "Minería de datos distribuida: algoritmos, sistemas y aplicaciones.",
        "En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy.",
        "Escalando: Aprendizaje automático distribuido con cooperación.",
        "En Proc. de AAAI-96, páginas 74-79.",
        "AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian.",
        "Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml).",
        "En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456.",
        "Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith.",
        "El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido.",
        "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan.",
        "Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas.",
        "En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek.",
        "Enfoque inspirado en el mercado para el aprendizaje colaborativo.",
        "En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227.",
        "Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y.",
        "Z. Wei, L. Moreau y N. R. Jennings.",
        "Sistemas de recomendación: un diseño basado en el mercado.",
        "En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß.",
        "Una perspectiva multiagente del aprendizaje automático paralelo y distribuido.",
        "En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg.",
        "¿Qué significa \"multi\" en el aprendizaje multiagente?",
        "Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685"
    ],
    "error_count": 8,
    "keys": {
        "agent": {
            "translated_key": "agente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for <br>agent</br>-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for <br>agent</br>-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing <br>agent</br> interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in <br>agent</br>-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of <br>agent</br> design (all agents apply the same learning algorithm) and/or <br>agent</br> objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the <br>agent</br> metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-<br>agent</br>) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by <br>agent</br> technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning <br>agent</br>, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning <br>agent</br> is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the <br>agent</br> as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the <br>agent</br> is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the <br>agent</br> is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of <br>agent</br> j when receiving a learning process description li = Di, Hi, fi, gi, hi from <br>agent</br> i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how <br>agent</br> js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an <br>agent</br> j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 <br>agent</br>-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular <br>agent</br> i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional <br>agent</br> that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each <br>agent</br> uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this <br>agent</br>-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning <br>agent</br>.",
                "The Single <br>agent</br> is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve <br>agent</br> performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one <br>agent</br> uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single <br>agent</br> that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous <br>agent</br> groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly <br>agent</br>-based completely disregard <br>agent</br> autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-<br>agent</br> Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "<br>agent</br>-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-<br>agent</br> Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-<br>agent</br> learning: The state of the art.",
                "Autonomous Agents and Multi-<br>agent</br> Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-<br>agent</br> machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-<br>agent</br> learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "A Framework for <br>agent</br>-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for <br>agent</br>-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "This allows us to apply existing <br>agent</br> interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in <br>agent</br>-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "All of these approaches assume homogeneity of <br>agent</br> design (all agents apply the same learning algorithm) and/or <br>agent</br> objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the <br>agent</br> metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-<br>agent</br>) distribution and parallelisation in principle.",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by <br>agent</br> technology so as to build effective autonomous learning agents."
            ],
            "translated_annotated_samples": [
                "Un marco para el aprendizaje automático distribuido basado en <br>agente</br>s y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en <br>agente</br>s y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje.",
                "Esto nos permite aplicar mecanismos de interacción de <br>agentes</br> existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en <br>agentes</br>, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje.",
                "Todas estas aproximaciones asumen la homogeneidad del diseño de los <br>agentes</br> (todos los <br>agentes</br> aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje).",
                "Ver a los estudiantes como <br>agente</br>s autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del <br>agente</br> se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio.",
                "Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de <br>agente</br>s para construir <br>agente</br>s de aprendizaje autónomos efectivos."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en <br>agente</br>s y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en <br>agente</br>s y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de <br>agentes</br> existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en <br>agentes</br>, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los <br>agentes</br> (todos los <br>agentes</br> aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como <br>agente</br>s autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del <br>agente</br> se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de <br>agente</br>s para construir <br>agente</br>s de aprendizaje autónomos efectivos. ",
            "candidates": [],
            "error": [
                [
                    "agente",
                    "agente",
                    "agentes",
                    "agentes",
                    "agentes",
                    "agentes",
                    "agente",
                    "agente",
                    "agente",
                    "agente"
                ]
            ]
        },
        "machine learning": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed <br>machine learning</br> and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed <br>machine learning</br> and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed <br>machine learning</br> tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of <br>machine learning</br> and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed <br>machine learning</br> and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) <br>machine learning</br> and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed <br>machine learning</br> and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "<br>machine learning</br>, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of <br>machine learning</br> Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "<br>machine learning</br>, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed <br>machine learning</br> with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent <br>machine learning</br> (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed <br>machine learning</br>.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "A Framework for Agent-Based Distributed <br>machine learning</br> and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed <br>machine learning</br> and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "This allows us to apply existing agent interaction mechanisms to distributed <br>machine learning</br> tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "INTRODUCTION In the areas of <br>machine learning</br> and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed <br>machine learning</br> and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) <br>machine learning</br> and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems."
            ],
            "translated_annotated_samples": [
                "Un marco para el <br>aprendizaje automático</br> distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el <br>aprendizaje automático</br> distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje.",
                "Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de <br>aprendizaje automático</br>, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje.",
                "En las áreas de <br>aprendizaje automático</br> y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje.",
                "Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl.",
                "TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre <br>aprendizaje automático</br> y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido."
            ],
            "translated_text": "Un marco para el <br>aprendizaje automático</br> distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el <br>aprendizaje automático</br> distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de <br>aprendizaje automático</br>, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de <br>aprendizaje automático</br> y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre <br>aprendizaje automático</br> y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "datum mining": {
            "translated_key": "minería de datos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "individual learning process": {
            "translated_key": "procesos de aprendizaje individuales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of <br>individual learning process</br>es among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of <br>individual learning process</br>es among agents and (ii) online reasoning about learning success and learning progress by learning agents."
            ],
            "translated_annotated_samples": [
                "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de <br>procesos de aprendizaje individuales</br> entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de <br>procesos de aprendizaje individuales</br> entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "meta-reasoning": {
            "translated_key": "meta-razonamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in <br>meta-reasoning</br> about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in <br>meta-reasoning</br> about their own learning decisions."
            ],
            "translated_annotated_samples": [
                "Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la <br>meta-razonamiento</br> sobre sus propias decisiones de aprendizaje."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la <br>meta-razonamiento</br> sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distributed clustering application": {
            "translated_key": "agrupamiento distribuido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world <br>distributed clustering application</br> to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "We apply this architecture to a real-world <br>distributed clustering application</br> to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms."
            ],
            "translated_annotated_samples": [
                "Aplicamos esta arquitectura a una aplicación de <br>agrupamiento distribuido</br> del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de <br>agrupamiento distribuido</br> del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "frameworks and architecture": {
            "translated_key": "marcos y arquitectura",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "unsupervised clustering": {
            "translated_key": "Agrupamiento no supervisado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "bayesian classifier": {
            "translated_key": "clasificador bayesiano",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "consensusbased method": {
            "translated_key": "métodos basados en consenso",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or <br>consensusbased method</br>s for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or <br>consensusbased method</br>s for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21])."
            ],
            "translated_annotated_samples": [
                "Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o <br>métodos basados en consenso</br> para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21])."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o <br>métodos basados en consenso</br> para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "communication and coordination": {
            "translated_key": "comunicación y coordinación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) <br>communication and coordination</br> offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) <br>communication and coordination</br> offered by agent technology so as to build effective autonomous learning agents."
            ],
            "translated_annotated_samples": [
                "Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) <br>comunicación y coordinación</br> ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) <br>comunicación y coordinación</br> ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "autonomous learning agent": {
            "translated_key": "agentes de aprendizaje autónomos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective <br>autonomous learning agent</br>s.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective <br>autonomous learning agent</br>s."
            ],
            "translated_annotated_samples": [
                "Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir <br>agentes de aprendizaje autónomos</br> efectivos."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir <br>agentes de aprendizaje autónomos</br> efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "historical information": {
            "translated_key": "información histórica",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiagent learn": {
            "translated_key": "aprendizaje multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from <br>multiagent learn</br>ing systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early <br>multiagent learn</br>ing system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from <br>multiagent learn</br>ing systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "The MALE system [19] was a very early <br>multiagent learn</br>ing system in which agents used a blackboard approach to communicate their hypotheses."
            ],
            "translated_annotated_samples": [
                "Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de <br>aprendizaje multiagente</br> en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl.",
                "El sistema MALE [19] fue un sistema de <br>aprendizaje multiagente</br> muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis."
            ],
            "translated_text": "Un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos Jan Tozicka Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa tozicka@labe.felk.cvut.cz Michael Rovatsos Escuela de Informática Universidad de Edimburgo Edimburgo EH8 9LE Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Laboratorio Gerstner Universidad Técnica Checa Technick«a 2, Praga, 166 27 República Checa pechouc@labe.felk.cvut.cz RESUMEN Este artículo propone un marco para el aprendizaje automático distribuido basado en agentes y la minería de datos, que se fundamenta en (i) el intercambio de descripciones a nivel meta de procesos de aprendizaje individuales entre agentes y (ii) el razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por parte de los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce una serie de métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas distribuidas de aprendizaje automático, aprovechando así los potentes métodos de coordinación disponibles en la computación basada en agentes, y permite a los agentes participar en la meta-razonamiento sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupamiento distribuido del mundo real para ilustrar cómo el marco conceptual puede ser utilizado en sistemas prácticos en los que diferentes aprendices pueden estar utilizando diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Informamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos trabajos relacionados sobre el tema y discutimos posibles extensiones futuras al marco de trabajo. Categorías y Descriptores de Asignaturas de la Teoría de Términos Generales I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente 1. En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para revisiones), se ha reconocido desde hace tiempo que la paralelización y la distribución pueden utilizarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas al respecto, que van desde la integración a nivel bajo de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promediando modelos de clasificadores bayesianos [8], o métodos basados en consenso para integrar diferentes agrupamientos [11]), hasta la combinación a nivel alto de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos utilizando meta-aprendizaje (por ejemplo, [3, 10, 21]). Todas estas aproximaciones asumen la homogeneidad del diseño de los agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o los objetivos de los agentes (todos los agentes intentan resolver cooperativamente un único problema global de aprendizaje). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de aprendices autónomos interactuando en sistemas abiertos. En tales sistemas, los aprendices (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos y representaciones de datos, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúen de manera estrictamente cooperativa (el conocimiento descubierto y los datos recopilados podrían ser activos económicos que solo deben compartirse cuando se considere rentable; los agentes maliciosos podrían intentar influir negativamente en los resultados de aprendizaje de otros, etc.). Ejemplos de aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos sensibles y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos de pacientes en el diagnóstico distribuido de tumores cerebrales) - sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje locales entre diferentes aprendices. En otras áreas, los datos de entrenamiento podrían tener un valor comercial, por lo que los agentes solo los pondrían a disposición de otros si esos agentes pudieran ofrecer algo a cambio (por ejemplo, en la vigilancia y seguimiento de barcos remotos, donde las diferentes agencias involucradas son proveedores de servicios comerciales). Además, los agentes podrían tener un interés propio en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo de esto es el de agentes fraudulentos en eBay que pueden intentar evitar que los agentes de aprendizaje de reputación construyan modelos útiles para detectar fraudes. Ver a los estudiantes como agentes autónomos y autodirigidos es la única perspectiva adecuada que se puede adoptar al modelar estos entornos de aprendizaje distribuido: la metáfora del agente se convierte en una necesidad en contraposición a las preferencias por la escalabilidad, la selección dinámica de datos, la interactividad [13], que también se pueden lograr a través de la distribución y paralelización (no de agentes) en principio. A pesar de la autonomía y auto-dirección de los agentes de aprendizaje, muchos de estos sistemas muestran una superposición suficiente en términos de objetivos de aprendizaje individuales para que la cooperación beneficiosa sea posible si existiera un modelo de interacción flexible entre los aprendices autónomos que permitiera a los agentes 1. intercambiar información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin ser obligados a revelar información privada que no debería ser divulgada, 2. decidir en qué medida desean compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros aprendices, y 3. razonar sobre cómo esta información puede ser mejor utilizada para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la idea simple de que los aprendices autónomos deben mantener meta-descripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo general de mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: Si podemos idear una vista lo suficientemente general y abstracta para describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecidos por la tecnología de agentes para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos una arquitectura abstracta (sección 2) e implementamos una instancia simple y concreta de la misma en un dominio del mundo real (sección 3). Informamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestran la viabilidad de nuestro enfoque (sección 4). Finalmente, revisamos el trabajo relacionado (sección 5) y concluimos con un resumen, discusión de nuestro enfoque y perspectivas para trabajos futuros sobre el tema (sección 6). 2. Nuestra arquitectura abstracta se basa en proporcionar descripciones formales (a nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para asegurar que este marco sea lo suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: Dados los datos D ⊆ D tomados de un espacio de instancias D, un espacio de hipótesis H y una función objetivo c ∈ H1 (desconocida), derivar una función h ∈ H que aproxime c lo mejor posible según alguna medida de rendimiento g : H → Q donde Q es un conjunto de posibles niveles de rendimiento de aprendizaje. Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye varios componentes de un problema de aprendizaje para los cuales se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupamiento, por ejemplo, podemos especificar lo anterior de la siguiente manera: Los datos de aprendizaje pueden describirse en ambos casos como D = ×n i=1[Ai] donde [Ai] es el dominio del i-ésimo atributo y el conjunto de atributos es A = {1, . . . , n}. Para el espacio de hipótesis obtenemos H ⊆ {h|h : D → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizaje utilizado) y H ⊆ {h|h : D → N, h es total con rango {1, . . . , k}} en el caso de agrupamiento (es decir, un subconjunto de todos los conjuntos de asignaciones de clúster posibles que asignan puntos de datos a un número finito de clústeres numerados del 1 al k). Para la clasificación, g podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y el agrupamiento podría utilizar diversas medidas de validez de clúster para evaluar la calidad de una hipótesis actual, de modo que Q = R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, introducimos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone intercambiarán información utilizando nuestro marco de trabajo. Para esto, asumimos que a cada aprendiz se le presenta un conjunto finito de datos D = d1, . . . dk en cada paso (este es un conjunto ordenado para expresar que el orden en el que se utilizan las muestras para el entrenamiento es importante) y emplea una función de entrenamiento/actualización f : H × D∗ → H que actualiza h dado una serie de muestras d1, . . . , dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como una tupla l = D, H, f, g, h donde requerimos que H ⊆ H y h ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la Figura 1: en el paso t, el aprendiz actualiza la hipótesis actual ht−1 con los datos Dt, y evalúa la nueva hipótesis resultante ht de acuerdo con la medida de rendimiento actual gt. Un paso de aprendizaje como este es equivalente a los siguientes pasos de computación: 1. entrenar el algoritmo en todas las muestras en D (una vez), es decir, calcular ft(ht−1, Dt) = ht, 2. calcular la calidad gt de la hipótesis resultante gt(ht). Denotamos el conjunto de todos los posibles pasos de aprendizaje como L. Para mayor facilidad en la notación, denotamos los componentes de cualquier l ∈ L como D(l), H(l), f(l) y g(l), respectivamente. La razón por la que las especificaciones de pasos de aprendizaje utilizan un subconjunto H de H en lugar de H mismo es que los aprendices a menudo tienen conocimiento explícito sobre qué hipótesis son efectivamente descartadas por f dado h en el futuro (si este no es el caso, aún podemos establecer H = H). Un proceso de aprendizaje es una secuencia finita y no vacía l = l1 → l2 → . . . → ln de pasos de aprendizaje tal que ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) El Sexto Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 679 función de entrenamiento ht medida de rendimiento calidad de solución qtgtft conjunto de entrenamiento Dt hipótesis hipótesis ht−1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito que hace la relación de transición →⊆ L × L es que la nueva hipótesis sea el resultado de entrenar la antigua hipótesis con todos los datos de muestra disponibles que pertenecen al paso actual. Denotamos el conjunto de todos los posibles procesos de aprendizaje como L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de entrenamiento y evaluación f y g). La traza de rendimiento asociada con un proceso de aprendizaje l es la secuencia q1, . . . , qn ∈ Qn donde qi = g(li)(h(li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos individuales de aprendizaje en las respectivas hipótesis. Tales especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre agentes de aprendizaje, a menudo es útil proporcionar solo información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, por ejemplo, al publicitar esta información para entrar en negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los aprendices describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPDs) en lugar de dar descripciones precisas sobre un único proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar exhaustivamente sus detalles. Como ejemplo, el conjunto {l ∈ L|∀l = l[i].D(l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de como máximo 100 muestras (donde todos los demás elementos son arbitrarios). Asimismo, {l ∈ L|∀l = l[i].D(l) = {d}} es equivalente a simplemente proporcionar información sobre una única muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil para modelar, por ejemplo, datos recibidos del entorno). Por lo tanto, utilizamos ℘(L), que es el conjunto de todos los LPDs, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido estará, por supuesto, más restringido y permitirá solo que se especifiquen de manera compacta un tipo especial de subconjuntos de L, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los posibles elementos de los respectivos conjuntos y espacios de funciones (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convertiría en ({d}, ∗, ∗, ∗, ∗)). 2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metarazonamiento que opera sobre información acerca de procesos de aprendizaje y está situado en un entorno cohabitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar a nivel meta cómo aprender, sino que al hacerlo puede tener en cuenta la información proporcionada por otros agentes o el entorno. Aunque son posibles casos puramente cooperativos o híbridos, para los propósitos de este documento asumiremos que los agentes son puramente egoístas, y que aunque pueda haber un potencial para la cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de cada uno, no hay un mecanismo global que pueda hacer cumplir dicho comportamiento cooperativo. Formalmente hablando, la función de aprendizaje de un agente es una función que, dada un conjunto de historias de procesos de aprendizaje previos (propios y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce un paso de aprendizaje que es su próxima acción de aprendizaje. En el sentido más general, el proceso de actualización interna de aprendizaje de nuestros agentes de aprendizaje puede verse como una función λ: ℘(L) → L × ℘(L) que toma un conjunto de historias de aprendizaje propias y de otros como entradas y calcula un nuevo paso de aprendizaje a ejecutar mientras actualiza el conjunto de historias de procesos de aprendizaje conocidos (por ejemplo, agregando la nueva acción de aprendizaje a nuestro propio proceso de aprendizaje y dejando intacta toda la información sobre los procesos de aprendizaje de los demás). Ten en cuenta que en λ({l1, . . . ln}) = (l, {l1, . . . ln }) algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida libremente por el agente siempre y cuando se cumpla un requisito, a saber, que los datos de aprendizaje que se estén utilizando siempre provengan de lo que se ha observado previamente. De manera más formal, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) es decir, lo que sea que λ genere como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos; tiene que trabajar con las muestras que han estado disponibles anteriormente en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar con datos previamente utilizados). El objetivo del agente es producir un paso de aprendizaje óptimo en cada iteración dada la información de la que dispone. Una posibilidad de especificar esto es requerir que ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) pero dado que generalmente será poco realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2. Nota que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje distribuido de máquinas y minería de datos, sino que también delinea nuestro enfoque de los sistemas de <br>aprendizaje multiagente</br> en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el entorno. 680 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . . Hola. .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Tabla 1: Matriz de funciones de integración para mensajes enviados del aprendiz i al j para simplemente usar g(l )(h(l )) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto y poco específico para nuestros propósitos: aunque describe lo que los agentes deberían hacer (transformar los ajustes para el siguiente paso de aprendizaje de manera óptima), no especifica cómo esto puede lograrse en la práctica. 2.2 Integración de información del proceso de aprendizaje Para especificar cómo el proceso de aprendizaje de un agente puede verse afectado al integrar información recibida de otros, necesitamos detallar cómo los pasos de aprendizaje que realizará pueden ser modificados utilizando la información entrante sobre procesos de aprendizaje descritos por otros agentes (esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones al información existente sobre historias de aprendizaje que pueden realizarse utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente y solo intercambiar información sobre este paso individual de aprendizaje (nuestro modelo puede ser fácilmente extendido para adaptarse a entornos más complejos). Deje que lj = Dj, Hj, fj, gj, hj sea el estado actual del agente j al recibir una descripción del proceso de aprendizaje li = Di, Hi, fi, gi, hi del agente i (por el momento, asumimos que este es un paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de i). Considerando todas las posibles interacciones a un nivel abstracto, básicamente obtenemos una matriz de posibilidades para modificaciones de la especificación del paso de aprendizaje de js, como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración pc→c 1 , . . . , pc→c kc→c donde c, c ∈ {D, H, f, g, h} y que definen cómo se modificará el componente cj del agente j utilizando la información ci proporcionada sobre (el mismo o un componente diferente) en el paso de aprendizaje aplicando pc→c r (ci, cj) para algún r ∈ {1, . . . , kc→c }. Para simplificar, las colecciones de funciones p que un agente j utiliza especifican cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el propio modelo de aprendizaje, las formas obvias de modificar el propio proceso de aprendizaje incluyen reemplazar cj por ci o ignorar por completo ci. Formas más complejas/sutiles de integración del proceso de aprendizaje incluyen: • Modificación de Dj: añadir Di a Dj; filtrar todos los elementos de Dj que también aparecen en Di; añadir Di a Dj descartando todos los elementos con atributos fuera de los rangos que afectan a gj, o aquellos elementos ya clasificados correctamente por hj; • Modificación de Hi: usar la unión/intersección de Hi y Hj; alternativamente, descartar elementos de Hj que son inconsistentes con Dj en el proceso de intersección o unión, o filtrar elementos que no pueden ser obtenidos usando fj (a menos que fj sea modificado al mismo tiempo) • Modificación de fj: modificar parámetros o conocimiento previo de fj usando información sobre fi; evaluar su relevancia simulando pasos de aprendizaje anteriores en Dj usando gj y descartar aquellos que no ayuden a mejorar el rendimiento propio • Modificación de hj: combinar hj con hi usando operadores lógicos o matemáticos (por ejemplo); hacer que el uso de hi dependa de una evaluación previa de su calidad usando los datos propios Dj y gj. Aunque esta lista no incluye operaciones de integración completamente desarrolladas y concretas para procesos de aprendizaje, es indicativa de la amplia gama de interacciones entre agentes individuales en procesos de aprendizaje que nuestro marco permite. Ten en cuenta que la lista no incluye ninguna modificación a gj. Esto se debe a que no permitimos modificaciones en la medida de calidad propia de los agentes, ya que esto haría que el modelo de acción racional (de aprendizaje) fuera inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento de aprendizaje). También hay que tener en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de lj que no aparecen como argumentos de las p-operaciones; los omitimos por facilidad de notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de lj. Además de las operaciones a lo largo de la diagonal de la matriz, son concebibles operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría podríamos llenar la mayor parte de la matriz con entradas para ellos, pero por falta de espacio solo enumeramos algunos ejemplos: • Modificación de Dj usando fi: preprocesar muestras en fi, por ejemplo, para lograr representaciones intermedias a las que fj se pueda aplicar • Modificación de Dj usando hi: filtrar muestras de Dj que estén cubiertas por hi y construir hj usando fj solo en las muestras restantes • Modificación de Hj usando fi: filtrar hipótesis de Hj que no sean realizables usando fi • Modificación de hj usando gi: si hj está compuesto por varios subcomponentes, filtrar aquellos subcomponentes que no funcionen bien según gi • . . . Finalmente, muchos mensajes recibidos de otros describiendo propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, dando lugar a operaciones aún más complejas que dependen de qué tipos de información estén disponibles. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, mostrando datos de seguimiento en línea de embarcaciones para la región del Mar del Norte 3. EJEMPLO DE APLICACIÓN 3.1 Descripción del dominio Como ilustración de nuestro marco de trabajo, presentamos un sistema de minería de datos basado en agentes para vigilancia basada en agrupamiento utilizando datos del AIS (Sistema de Identificación Automática [1]). En nuestro dominio de aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos a lo largo del tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por los barcos equipados con estaciones móviles AIS a estaciones en tierra, otros barcos y aeronaves. Estos datos contienen la identidad de los barcos, tipo, posición, rumbo, velocidad, estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es tarea de las agencias de AIS detectar comportamientos anómalos para alertar a las unidades de policía/costera a fin de investigar más a fondo comportamientos inusuales, potencialmente sospechosos. Dicho comportamiento podría incluir cosas como desviaciones de las rutas estándar entre el origen declarado y el destino del viaje, encuentros cercanos inesperados entre diferentes embarcaciones en el mar, o patrones inusuales en la elección del destino en múltiples viajes, teniendo en cuenta el tipo de embarcación y la carga reportada. Si bien las razones de dicho comportamiento inusual pueden ir desde una mera coincidencia o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), resulta obviamente útil preprocesar la gran cantidad de datos de embarcaciones (de seguimiento) disponibles antes de proceder con un análisis más detallado por parte de expertos humanos. Para apoyar esta tarea automatizada de preprocesamiento, el software utilizado por estas agencias aplica métodos de agrupamiento para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este ámbito son empresas competidoras y utilizan sus conjuntos de datos y hipótesis de aprendizaje (modelos) (parcialmente superpuestos, pero distintos) como activos, por lo que no se puede esperar que colaboren de manera totalmente cooperativa para mejorar los resultados de aprendizaje en general. Considerando que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido arriba podría ser útil para explotar el potencial de cooperación que no es aprovechado por los sistemas actuales. 3.2 Diseño de un sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio de AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y algoritmos de agrupamiento disponibles para agentes individuales, 2. el mecanismo de interacción utilizado para intercambiar descripciones de procesos de aprendizaje, y 3. el mecanismo de decisión que los agentes aplican para tomar decisiones de aprendizaje. En cuanto a 1., nuestros agentes cuentan con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre embarcaciones individuales en términos de atributos A = {1, . . . , n}, que incluyen cosas como ancho, longitud, etc., con dominios de valores reales ([Ai] = R para todo i). En términos de algoritmo de aprendizaje, consideramos el agrupamiento con un número fijo de k clústeres utilizando los algoritmos de agrupamiento k-means y k-medoids [5] (fijo significa que el algoritmo de aprendizaje siempre producirá k clústeres; sin embargo, permitimos a los agentes cambiar el valor de k en diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede ser definido como H = { c1, . . . , ck |ci ∈ R|A| } es decir, el conjunto de todos los posibles conjuntos de k centroides de clúster en el espacio euclidiano de dimensión |A|. Para cada hipótesis h = c1, . . . , ck y cualquier punto de datos d ∈ ×n i=1[Ai] dado el dominio [Ai] para el atributo i-ésimo de cada muestra, la asignación a los clústeres se da por C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| es decir, d se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionado con un agente particular i se divide inicialmente en un conjunto de entrenamiento Di y un conjunto de validación Vi. Luego, generamos un conjunto de embarcaciones falsas Fi tal que |Fi| = |Vi|. Estos dos conjuntos evalúan la capacidad de los agentes para detectar embarcaciones sospechosas. Para esto, asignamos un valor de confianza r(h, d) a cada barco d: r(h, d) = 1 |d − cC(h,d)| donde C(h, d) es el índice del centroide más cercano. Basándonos en esta medida, clasificamos cualquier embarcación en Fi ∪ Vi como falsa si su valor r está por debajo de la mediana de todas las confianzas r(h, d) para d ∈ Fi ∪ Vi. Con esto, podemos calcular la calidad gi(h) ∈ R como la proporción entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ Vi. En lo que respecta al punto 2., utilizamos un mecanismo de intercambio de hipótesis basado en un Protocolo de Red de Contratos (CNP) [20] simple: Antes de cada iteración de aprendizaje, los agentes emiten (difunden públicamente) Llamadas a Propuestas (CfPs), anunciando la calidad de su propio modelo numérico. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi(h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros al publicitar la calidad de su modelo, pero observamos que esta calidad podría tener una relevancia limitada para otros agentes, ya que podrían especializarse en regiones específicas del espacio de datos no relacionadas con el conjunto de pruebas del remitente de la CfP. Posteriormente, algunos agentes pueden realizar ofertas en las que publicitan, a su vez, la calidad de su propio modelo. Si el 682 El Sexto Internacional. Las ofertas (si las hay) para la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió la convocatoria, los agentes intercambian sus hipótesis y comienza la siguiente iteración de aprendizaje. Para describir lo que es necesario para el punto 3., tenemos que especificar (i) bajo qué condiciones los agentes presentan ofertas en respuesta a una CfP, (ii) cuándo aceptan ofertas en el proceso de negociación CNP, y (iii) cómo integran la información recibida en su propio proceso de aprendizaje. En cuanto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: sea g la calidad del propio modelo y g la anunciada por el CfP (o la oferta más alta, respectivamente). Si g > g, respondemos a la CfP (aceptamos la oferta), de lo contrario respondemos a la CfP (aceptamos la oferta) con probabilidad P(g / g) y la ignoramos (rechazamos) en otro caso. Si dos agentes hacen un trato, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, g y g se calculan mediante un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, se tendría que proporcionar un mecanismo de comparación para diferentes funciones g). En cuanto a (iii), cada agente utiliza un único operador de fusión de modelos tomado de las siguientes dos clases de operadores (hj es el modelo propio del receptor y hi es el modelo del proveedor): • ph→h (hi, hj): - m-join: Los m mejores grupos (en términos de cobertura de Dj) de la hipótesis hi se añaden a hj. - m-select: El conjunto de los m mejores grupos (en términos de cobertura de Dj) de la unión hi ∪ hj se elige como un nuevo modelo. (A diferencia de m-join, este método no prefiere los grupos propios sobre los de otros). • ph→D (hi, Dj): - m-filter: Los m mejores grupos (como se mencionó anteriormente) de hi se identifican y se añaden a un nuevo modelo formado utilizando esas muestras no cubiertas por estos grupos aplicando el algoritmo de aprendizaje propio fj. Cuando m es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes elecciones de m. Es destacable que este sistema de minería de datos distribuida basado en agentes es uno de los ejemplos más simples concebibles de nuestra arquitectura abstracta. Aunque anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando aprendices de Programación Lógica Inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones de diseño clave involucradas en el uso de nuestro marco y proporciona soluciones de ejemplo simples para estos problemas de diseño. 4. RESULTADOS EXPERIMENTALES La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupamiento k-means y k-medoids respectivamente. Dividimos el conjunto de datos total de 300 barcos en tres conjuntos disjuntos de 100 muestras cada uno y asignamos cada uno de estos a un agente de aprendizaje. El Agente Único está aprendiendo de todo el conjunto de datos. El parámetro k se establece en 10 ya que este es el valor óptimo para el conjunto de datos total según el índice de Davies-Bouldin [9]. Para m-select asumimos que m = k, lo cual logra una Figura 3 constante: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de aprendices homogéneas utilizando el tamaño del modelo de los métodos k-means (arriba) y k-medoids (abajo). Para m-join y m-filter asumimos que m = 3 para limitar en qué medida los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recalcular los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, calado y velocidad con el objetivo de aprender a detectar qué embarcaciones han proporcionado descripciones falsas de sus propiedades. El conjunto de validación contiene 100 barcos reales y 100 barcos falsos generados aleatoriamente. Para generar propiedades lo suficientemente realistas para barcos falsos, los valores de sus atributos individuales se toman de barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributos de varios barcos existentes). En estos experimentos, estamos principalmente interesados en investigar si una forma simple de compartir conocimiento entre agentes de aprendizaje interesados en sí mismos podría mejorar el rendimiento del agente en comparación con un entorno de aprendices aislados. Por lo tanto, distinguimos entre sociedades de aprendices homogéneas donde todos los agentes utilizan el mismo algoritmo de agrupamiento y heterogéneas donde diferentes agentes utilizan algoritmos diferentes. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro), esto es claramente el caso para las operaciones de integración de unión y filtro (m = k) en ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de lo beneficioso que parezca el intercambio para los agentes). Podemos ver que la calidad de estas operaciones es muy cercana a la del Agente Único que tiene acceso a todos los datos de entrenamiento. Para los métodos de unión m, filtrado m y selección m restringidos (m < k), también podemos observar una distinción interesante, La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de aprendices utilizando los métodos k-means (arriba) y k-medoids (abajo), es decir, que estos métodos tienen un rendimiento similar al caso de aprendices aislados en grupos de agentes homogéneos, pero mejor que los aprendices aislados en sociedades más heterogéneas. Esto sugiere que los aprendices heterogéneos pueden beneficiarse incluso de un intercambio de conocimientos bastante limitado (y esto es lo que equivale a usar un valor bastante pequeño de m = 3 dado que k = 10), mientras que esto no siempre es cierto para agentes homogéneos. Esto ilustra claramente cómo diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio del problema y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos beneficios obvios en el rendimiento, integrar resultados de aprendizaje parciales también puede tener otras ventajas: la operación de filtro m, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medido en nuestros experimentos se muestra en la siguiente tabla. k-means k-medoids filtrado 30-40 % 10-20 % m-filtrado 20-30 % 5-15 % La conclusión general que podemos extraer de estos experimentos iniciales con nuestra arquitectura es que dado que una aplicación muy simplista de sus principios ha demostrado ser capaz de mejorar el rendimiento de agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre procesos de aprendizaje entre aprendices autónomos. 5. TRABAJO RELACIONADO Ya hemos mencionado el trabajo sobre aprendizaje automático y minería de datos distribuidos (no basados en agentes) en el capítulo introductorio, por lo que en esta sección nos limitaremos a enfoques que están más estrechamente relacionados con nuestra perspectiva sobre sistemas de aprendizaje distribuido. Con mucha frecuencia, los enfoques que supuestamente son basados en agentes ignoran por completo la autonomía de los agentes y prescriben procedimientos de toma de decisiones locales de antemano. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al. [6], el cual se basa en un enfoque de máquina de vectores de soporte distribuida donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje colaborativo basados en agentes [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que es perseguido colaborativamente por todos los agentes. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente de casos basada en agentes en el razonamiento basado en casos, pero solo es aplicable a sociedades de aprendices homogéneos (y acuñado hacia un método de aprendizaje específico). Un método basado en agentes para integrar procesos de análisis de clúster distribuidos utilizando estimación de densidad es presentado por Klusch et al. [13], el cual también está específicamente diseñado para un algoritmo de aprendizaje particular. Lo mismo ocurre con [22, 23], que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, aunque estos enfoques consideran mecanismos de interacción más interesantes entre los aprendices. Se han propuesto varias estrategias para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre agentes, y Ghosh et al. [11] es un paso en la dirección correcta en cuanto a revelar solo información parcial sobre el proceso de aprendizaje, ya que aborda el intercambio limitado de información en el agrupamiento distribuido. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la meta-descripción de datos, hipótesis y resultados intermedios, y permite el intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para tareas de minería de datos masivamente distribuidas. El sistema MALE [19] fue un sistema de <br>aprendizaje multiagente</br> muy temprano en el que los agentes utilizaban un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar las hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema ANIMALS [10] se utilizó para simular el aprendizaje multiestratégico al combinar dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) con el fin de superar las debilidades en los algoritmos individuales, aunque también era un sistema estrictamente cooperativo. Como muestran estos ejemplos y según nuestro mejor conocimiento, no ha habido intentos previos de proporcionar un marco que pueda dar cabida tanto a agentes de aprendizaje independientes como heterogéneos, y esto puede considerarse como la principal contribución de nuestro trabajo. CONCLUSIÓN En este documento, delineamos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, según nuestro conocimiento, el primer intento 684 de la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para capturar formas complejas de interacción entre aprendices heterogéneos y/o egoístas en una arquitectura que puede ser utilizada como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir a los agentes informar y mejorar sus habilidades de aprendizaje con información proporcionada por otros aprendices en el sistema, siempre y cuando todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupamiento distribuido basado en el mercado que fue evaluado en el ámbito del seguimiento de embarcaciones con el fin de identificar comportamientos desviados o sospechosos. Aunque nuestros resultados experimentales solo insinúan el potencial de utilizar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su forma más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y hemos hecho la suposición implícita de que la comunicación requerida es gratuita. Esto es, por supuesto, insuficiente si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que utilicen algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr un intercambio exitoso de información pueden ser muy diferentes a las descritas aquí, y métodos de comunicación y razonamiento mucho más complejos pueden ser necesarios para lograr una integración útil de los procesos de aprendizaje de los diferentes agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido con el fin de arrojar luz sobre cuáles deberían ser las medidas correctas de optimalidad para agentes que razonan y se comunican de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaborativos y competitivos, serán el tema de nuestro futuro trabajo sobre el asunto. Agradecimiento: Agradecemos sinceramente el apoyo brindado a la investigación presentada por el proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y el proyecto de la Oficina de Investigación Naval N00014-06-1-0232. REFERENCIAS [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: Un sistema para la minería de datos en clústeres locales y de área amplia, así como super-clústeres. En Proc. de la Conferencia sobre Supercomputación. 1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de algoritmos de clasificación de votación: Bagging, Boosting y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupamiento, Informe técnico, Accrue Software, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de fuentes de datos dinámicas distribuidas. En Actas del Taller sobre Agentes de Aprendizaje, 2000. [7] N. Chawla y S. E. y L. O. Sala. Creando conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promediando modelos para predicción con redes bayesianas discretas. Revista de Investigación en Aprendizaje Automático, 5:1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clústeres. IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje multiagente heterogéneo. En Actas del Grupo de Interés Especial en Sistemas Basados en el Conocimiento Cooperativo, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar agrupamientos distribuidos bajo un intercambio limitado de conocimiento. En el Taller de la NSF sobre Minería de Datos de Próxima Generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para la minería de datos distribuida. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuida basada en agentes: El esquema KDEC. En AgentLink, número 2586 en LNCS. Springer, 2003. [14] T. M. Mitchell. \n\nSpringer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontañón y E. Plaza. Reciclaje de datos para el aprendizaje multiagente. En Proc. de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: El estado del arte. Agentes Autónomos y Sistemas Multiagente, 11(3):387-434, 2005. [17] B. Parque y H. Kargupta. Minería de datos distribuida: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de Minería de Datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escalando: Aprendizaje automático distribuido con cooperación. En Proc. de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. \n\nEditorial AAAI, 1996. [19] S. Sian. Ampliando el aprendizaje a múltiples agentes: Problemas y un modelo para el aprendizaje automático multiagente (ma-ml). En Y. Kodratoff, editor, Machine LearningEWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. \n\nSpringer-Verlag, 1991. [20] R. Smith. El protocolo de red de contratos: Comunicación y control de alto nivel en un solucionador de problemas distribuido. IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, y P. K. Chan. Jam: Agentes de Java para Meta-Aprendizaje sobre Bases de Datos Distribuidas. En Proc. de la KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En Agentes de Información Cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. \n\nSpringer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03, páginas 600-607, 2003. [24] G. Weiß. Una perspectiva multiagente del aprendizaje automático paralelo y distribuido. En Actas de Agents98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué significa \"multi\" en el aprendizaje multiagente? Aprendizaje colaborativo: Enfoques cognitivos y computacionales, 64-80, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 685 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distribute machine learn": {
            "translated_key": "distribuir aprendizaje automático",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "framework and architecture": {
            "translated_key": "marco y arquitectura",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "unsupervise cluster": {
            "translated_key": "grupo no supervisado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Framework for Agent-Based Distributed Machine Learning and Data Mining Jan Tozicka Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic tozicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics The University of Edinburgh Edinburgh EH8 9LE United Kingdom mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratory Czech Technical University Technick«a 2, Prague, 166 27 Czech Republic pechouc@labe.felk.cvut.cz ABSTRACT This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents.",
                "We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes.",
                "This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions.",
                "We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms.",
                "We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",
                "General Terms Theory Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent Systems 1.",
                "INTRODUCTION In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance.",
                "Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).",
                "All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem).",
                "Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems.",
                "In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others learning results, etc.).",
                "Examples for applications of this kind abound.",
                "Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners.",
                "In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]).",
                "Furthermore, agents might have a vested interest in negatively affecting other agents learning performance.",
                "An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.",
                "Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.",
                "Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to 1. exchange information about different aspects of their own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, 2. decide to what extent they want to share information about their own learning processes and utilise information provided by other learners, and 3. reason about how this information can best be used to improve their own learning performance.",
                "Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results).",
                "Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.",
                "To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3).",
                "We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).",
                "Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). 2.",
                "ABSTRACT ARCHITECTURE Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.",
                "To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.",
                "This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise.",
                "For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.",
                "For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k).",
                "For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).",
                "Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.",
                "For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk.",
                "In other words, one learning step always consists of applying the update function to all samples in D exactly once.",
                "We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H. The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt.",
                "Such a learning step is equivalent to the following steps of computation: 1. train the algorithm on all samples in D (once), i.e. calculate ft(ht−1, Dt) = ht, 2. calculate the quality gt of the resulting hypothesis gt(ht).",
                "We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively.",
                "The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).",
                "A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step.",
                "We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g).",
                "The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.",
                "Such specifications allow agents to provide a selfdescription of their learning process.",
                "However, in communication among learning agents, it is often useful to provide only partial information about ones internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others.",
                "For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.",
                "This allows us to describe properties of a learning process without specifying its details exhaustively.",
                "As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary).",
                "Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment).",
                "Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.",
                "In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents.",
                "For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)). 2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents.",
                "This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment.",
                "Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agents learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action.",
                "In the most general sense, our learning agents internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to ones own learning process and leaving all information about others learning processes untouched).",
                "Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.",
                "The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed.",
                "More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).",
                "The goal of the agent is to output an optimal learning step in each iteration given the information that it has.",
                "One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents behaviour in the environment. 680 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) . .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .",
                "Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h 1 (gi, hj) . .. pg→h kg→h (gi, hj) hi . .. n/a ... Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.",
                "This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesnt specify how this can be achieved in practice. 2.2 Integrating learning process information To specify how an agents learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case).",
                "In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information.",
                "For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).",
                "Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i).",
                "Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of js learning step specification as shown in Table 1.",
                "In this matrix, each entry specifies a family of integration functions pc→c 1 , . . . , pc→c kc→c where c, c ∈ {D, H, f, g, h} and which define how agent js component cj will be modified using the information ci provided about (the same or a different component of) is learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.",
                "To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.",
                "For the diagonal of this matrix, which contains the most common ways of integrating new information in ones own learning model, obvious ways of modifying ones own learning process include replacing cj by ci or ignoring ci altogether.",
                "More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents learning processes that our framework enables.",
                "Note that the list does not include any modifications to gj.",
                "This is because we do not allow modifications to the agents own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance).",
                "Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.",
                "Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components.",
                "In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .",
                "Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region 3.",
                "APPLICATION EXAMPLE 3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data.",
                "In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.",
                "This data contains the ships identity, type, position, course, speed, navigational status and other safety-related information.",
                "Figure 2 shows a screenshot of our simulation system.",
                "It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour.",
                "Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account.",
                "While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.",
                "To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user.",
                "However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results.",
                "Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems. 3.2 Agent-based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: 1.",
                "The datasets and clustering algorithms available to individual agents, 2. the interaction mechanism used for exchanging descriptions of learning processes, and 3. the decision mechanism agents apply to make learning decisions.",
                "Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions.",
                "Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).",
                "In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles).",
                "This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space.",
                "For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.",
                "For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi.",
                "Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|.",
                "These two sets assess the agents ability to detect suspicious vessels.",
                "For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid.",
                "Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi.",
                "With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.",
                "As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality.",
                "In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model.",
                "We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.",
                "Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model.",
                "If the 682 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.",
                "To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process.",
                "Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be ones own model quality and g that advertised by the CfP (or highest bid, respectively).",
                "If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else.",
                "If two agents make a deal, they exchange their learning hypotheses (models).",
                "In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).",
                "As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receivers own model and hi is the providers model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others.) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.",
                "Whenever m is large enough to encompass all clusters, we simply write join or filter for them.",
                "In section 4 we analyse the performance of each of these two classes for different choices of m. It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture.",
                "While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. 4.",
                "EXPERIMENTAL RESULTS Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively.",
                "We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent.",
                "The Single Agent is learning from the whole dataset.",
                "The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].",
                "For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size.",
                "For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.",
                "During each experiment the learning agents receive ship descriptions in batches of 10 samples.",
                "Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary.",
                "Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties.",
                "The validation set contains 100 real and 100 randomly generated fake ships.",
                "To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).",
                "In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners.",
                "Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.",
                "As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases.",
                "This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).",
                "We can see that the quality of these operations is very close to the Single Agent that has access to all training data.",
                "For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction, The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies.",
                "This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents.",
                "This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.",
                "Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process.",
                "The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. 5.",
                "RELATED WORK We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.",
                "Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori.",
                "A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm.",
                "A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.",
                "The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.",
                "Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method).",
                "An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm.",
                "The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.",
                "A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about ones learning process as it deals with limited information sharing in distributed clustering.",
                "Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.",
                "The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses.",
                "Agents were able to critique each others hypotheses until agreement was reached.",
                "However, all agents in this system were identical and the system was strictly cooperative.",
                "The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.",
                "As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. 6.",
                "CONCLUSION In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining.",
                "This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.",
                "To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.",
                "Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.",
                "Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free.",
                "This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results.",
                "Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical).",
                "In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents learning processes.",
                "Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.",
                "These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.",
                "Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232. 7.",
                "REFERENCES [1] http://www.aislive.com. [2] http://www.healthagents.com. [3] S. Bailey, R. Grossman, H. Sivakumar, and A. Turinsky.",
                "Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters.",
                "In Proc. of the Conference on Supercomputing. 1999. [4] E. Bauer and R. Kohavi.",
                "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.",
                "Machine Learning, 36, 1999. [5] P. Berkhin.",
                "Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002. [6] D. Caragea, A. Silvescu, and V. Honavar.",
                "Agents that Learn from Distributed Dynamic Data sources.",
                "In Proc. of the Workshop on Learning Agents, 2000. [7] N. Chawla and S. E. abd L. O.",
                "Hall.",
                "Creating ensembles of classifiers.",
                "In Proceedings of ICDM 2001, pages 580-581, San Jose, CA, USA, 2001. [8] D. Dash and G. F. Cooper.",
                "Model Averaging for Prediction with Discrete Bayesian Networks.",
                "Journal of Machine Learning Research, 5:1177-1203, 2004. [9] D. L. Davies and D. W. Bouldin.",
                "A Cluster Separation Measure.",
                "IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224-227, 1979. [10] P. Edwards and W. Davies.",
                "A Heterogeneous Multi-Agent Learning System.",
                "In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163-184, 1993. [11] J. Ghosh, A. Strehl, and S. Merugu.",
                "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing.",
                "In NSF Workshop on Next Generation Data Mining, 99-108, 2002. [12] D. L. Grecu and L. A. Becker.",
                "Coactive Learning for Distributed Data Mining.",
                "In Proceedings of KDD-98, pages 209-213, New York, NY, August 1998. [13] M. Klusch, S. Lodi, and G. Moro.",
                "Agent-based distributed data mining: The KDEC scheme.",
                "In AgentLink, number 2586 in LNCS.",
                "Springer, 2003. [14] T. M. Mitchell.",
                "Machine Learning, pages 29-36.",
                "McGraw-Hill, New York, 1997. [15] S. Ontanon and E. Plaza.",
                "Recycling Data for Multi-Agent Learning.",
                "In Proc. of ICML-05, 2005. [16] L. Panait and S. Luke.",
                "Cooperative multi-agent learning: The state of the art.",
                "Autonomous Agents and Multi-Agent Systems, 11(3):387-434, 2005. [17] B.",
                "Park and H. Kargupta.",
                "Distributed Data Mining: Algorithms, Systems, and Applications.",
                "In N. Ye, editor, Data Mining Handbook, pages 341-358, 2002. [18] F. J. Provost and D. N. Hennessy.",
                "Scaling up: Distributed machine learning with cooperation.",
                "In Proc. of AAAI-96, pages 74-79.",
                "AAAI Press, 1996. [19] S. Sian.",
                "Extending learning to multiple agents: Issues and a model for multi-agent machine learning (ma-ml).",
                "In Y. Kodratoff, editor, Machine LearningEWSL-91, pages 440-456.",
                "Springer-Verlag, 1991. [20] R. Smith.",
                "The contract-net protocol: High-level communication and control in a distributed problem solver.",
                "IEEE Transactions on Computers, C-29(12):1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan, and P. K. Chan.",
                "Jam: Java Agents for Meta-Learning over Distributed Databases.",
                "In Proc. of the KDD-97, pages 74-81, USA, 1997. [22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.",
                "Market-Inspired Approach to Collaborative Learning.",
                "In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213-227.",
                "Springer, 2006. [23] Y.",
                "Z. Wei, L. Moreau, and N. R. Jennings.",
                "Recommender systems: a market-based design.",
                "In Proceedings of AAMAS-03), pages 600-607, 2003. [24] G. Weiß.",
                "A Multiagent Perspective of Parallel and Distributed Machine Learning.",
                "In Proceedings of Agents98, pages 226-230, 1998. [25] G. Weiss and P. Dillenbourg.",
                "What is multi in multi-agent learning?",
                "Collaborative-learning: Cognitive and Computational Approaches, 64-80, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}