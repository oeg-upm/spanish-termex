{
    "id": "J-69",
    "original_text": "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C. Berkeley 2 HP Labs 3 Computer Science Division U.C. Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems. What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors. To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques. These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies. Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation. Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1. INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users. For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3]. In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26]. Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6]. In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance. As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest. A wants service from B, B wants service form C, and C wants service from A. utility of the system. Avoiding this tragedy of the commons [18] requires incentives for cooperation. We adopt a game-theoretic approach in addressing this problem. In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously. While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest. In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash). Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context. Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly. However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity. We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history. Consider the example in Figure 1. If everyone provides service, then the system operates optimally. However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc. We show that with shared history, B knows that A served C and consequently will serve A. This results in a higher level of cooperation than with private history. The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion. In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service. We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population. The basic idea is that B would only believe C if C had already provided service to B. The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system. To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation. We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped. We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system. The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers. The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host). Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions. We show that short-term history prevents traitors from disrupting cooperation. The rest of the paper is organized as follows. We describe the model in Section 2 and the reciprocative decision function in Section 3. We then proceed to the incentive techniques in Section 4. In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history. In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it. In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities. In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them. We discuss related work in Section 5 and conclude in Section 6. 2. MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit. However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4). For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate. A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 . Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients. We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4). We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors. In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover. Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants. Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her. This models the difficulty or expense of determining that a peer could have provided a service, but didnt. For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma. T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement. Each game consists of two players who can defect or cooperate. Depending how each acts, the players receive a payoff. The players use a strategy to decide how to act. Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4]. Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma. In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction. A player can be a client in one game and a server in another. The client and server receive the payoff from a generalized payoff matrix (Figure 2). Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff. A GPD payoff matrix must have the following properties to create a social dilemma: 1. Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2. Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3. Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate. These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma. Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable. If one player makes an untraceable action, the other player does not know the identity of the first player. For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3. This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting. In addition, for this particular payoff matrix, clients are unable to trace server defections. This is the payoff matrix that we use in our simulation results. Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously. Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics. An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times. For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively. It does not specify which specific players switched. Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system. As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers. In our model, entities take independent and continuous actions that change the composition of the population. Time consists of rounds. In each round, every player plays one game as a client and one game as a server. At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same. If a player mutates, she switches to a randomly picked strategy. If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below). If she maintains her identity after switching strategies, then she is referred to as a traitor. If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player. To learn, a player collects local information about the performance of different strategies. This information consists of both her personal observations of strategy performance and the observations of those players she interacts with. This models users communicating out-of-band about how strategies perform. Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy. A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) . We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating. At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3. RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques. A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player. A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy. Our approach to incentives is to design strategies which maximize both individual and social benefit. Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation. Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system. In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system. Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection. Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria. For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections . In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above. The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity. Generosity measures the benefit an entity has provided relative to the benefit it has consumed. This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse. For some entity i, let pi and ci be the services i has provided and consumed, respectively. Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity. Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3). This will cause Reciprocative players to defect on each other. To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity. Normalized generosity measures entity is generosity relative to entity js generosity. More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function. Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2. We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks. Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors. Table 1 presents the nominal parameter values used in our simulation. The Ratio using rows refer to the initial ratio of the total population using a particular strategy. In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack. We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip. Private Figure 4: The evolution of strategy populations over time. Time the number of elapsed rounds. Population is the number of players using a strategy. In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios. Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator. Each point in the graph represents the number of players using a particular strategy at one point in time. Figures 5(a) and (b) show the corresponding mean overall score per round. This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects). From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so. We use this metric in all later results to evaluate our incentive techniques. Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players. One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system. It does not because of asymmetry of interest. For example, suppose player B is using Reciprocative with private history. Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim. Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative. We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4. RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation. To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity. Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective. In addition, private history does not deal well with asymmetry of interest. For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him. We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection. However, the asymmetry of transactions challenges selection mechanisms. Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric. As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection. In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors. In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities. This way, users approach their past recipients and give them a chance to reciprocate. In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3). In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games. Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes. We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario). The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations. In Figure 7 we fix the population size and vary the turnover rate. It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale. This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history. Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not. It allows players to leverage off of the experiences of others in cases of few repeat transactions. It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest. Some examples of shared history schemes are [20] [23] [28]. Figure 7 shows the effectiveness of shared history under high turnover rates. In this figure, we fix the population size and vary the turnover rate. While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1. This means that 10% of the players leave the system at the end of each round. In Figure 6 we fix the turnover and vary the population size. It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population. These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions. Nevertheless, shared history has two disadvantages. First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs. Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover. The x-axis is the turnover rate. The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization. A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols. Second, and more fundamental, shared history is vulnerable to collusion. In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion. Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them). Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation). An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28]. The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements. Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B. Our subjective algorithm is based on maxflow [24] [32]. Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints. For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it. The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints. In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets. The maxflow between the source and the target in the graph is 2. C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values. We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other. This information can be stored using the same methods as the shared history. A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints. As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system. Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other. When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0. This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j. The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders. Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time. The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ). Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists. The essential idea is to bound the mean number of nodes examined during the maxflow computation. This bounds the overhead, but also bounds the effectiveness. Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history. Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders. In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter. Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9. As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails. The difference here is that objective shared history fails for all population sizes. This is because the Reciprocative players cooperate with the colluders because of their high reputations. However, subjective history can reach high levels of cooperation regardless of colluders. This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0. Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder. Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers). Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history. This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter). Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks. One is for an entity (the mole) to provide service and then lie positively about other colluders. The other colluders can then exploit their reputation to receive service. However, the effectiveness of this attack relies on the amount of service that the mole provides. Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy. This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity. There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service. Falsely claiming to receive service is the simple collusion attack described above. Falsely claiming not to have provided service provides no benefit to the attacker. Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity. An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service. These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not. To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph. If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored. This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation. The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her. In these cases, an attacker can only lower the reputation of the victim. The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities. However, in most P2P systems, identities are zero-cost. This is desirable for network growth as it encourages newcomers to join the system. However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing). Whitewashers can cause the system to collapse if they are not punished appropriately. Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer. Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior. Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection. This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers. Our solution is the Stranger Adaptive stranger policy. The idea is to be generous to strangers when they are being generous and stingy when they are stingy. Let ps and cs be the number of services that strangers have provided and consumed, respectively. The probability that a player using Stranger Adaptive helps a stranger is ps/cs. However, we do not wish to keep these counts permanently (for reasons described in Section 4.4). Also, players may not know when strangers defect because defections are untraceable (as described in Section 2). Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs. When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history. The x-axis is the turnover rate on a log scale. The y-axis is the mean overall per round score. Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers. Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history. The x-axis is the turnover rate on a log scale. The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history. In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers. The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation. This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing. In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history. This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them. This is consistent with previous work [13] showing that punishing strangers deals with whitewashers. However, Figure 12 shows that Stranger Defect is not effective with private history. This is because Reciprocative requires some initial cooperation to bootstrap. In the shared history case, a Reciprocative player can observe that another player has already cooperated with others. With private history, the Reciprocative player only knows about the other players actions toward her. Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other. In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players. Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction). In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation. Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover. We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective. By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system. They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors. A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors. The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history). In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy. We use the default values for the other parameters. Without traitors, the cooperative strategies thrive. With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds. As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning. Cooperation eventually collapses. On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip. Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5. RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular. A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3]. The problem has been extensively studied adopting a game theoretic approach. The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players. In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates. Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks. Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history. Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold. As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high). In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications. Mechanism design is the inverse of game theory. It asks how to design a game in which the behavior of strategic players results in the socially desired outcome. Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing. More recently, DAMD has been also studied in dynamic environments [38]. In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players. The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31]. Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable. Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system. Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code. These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions. Also, these mechanisms are still vulnerable to zero-cost identities and collusion. BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6. CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks. Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance. We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest. Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers. Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7. ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper. This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811. Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8. REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B. A. Free Riding on Gnutella. First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation. Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems. In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks. In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B. Incentives build robustness in bittorrent. In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks. In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack. In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing. In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions. In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions. In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms. Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games. The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks. In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A. Balances of Power on EBay: Peers or Unquals? In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation. In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons. Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND. Evolutionary Games and Population Dynamics. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks. In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed. OReilly & Associates, Inc., March 2001, ch. Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma. In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B. Cooperative Peer Groups in Nice. In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification. In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y. Preserving Peer Replicas by Rate-Limited Sampled Voting. In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks. In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring. Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups. Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A. Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties. In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I. To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments. In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems. In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness. In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing. In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B. To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering. In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation. In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111",
    "original_translation": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111",
    "original_sentences": [
        "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
        "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
        "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
        "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
        "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
        "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
        "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
        "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
        "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
        "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
        "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
        "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
        "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
        "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
        "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
        "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
        "We adopt a game-theoretic approach in addressing this problem.",
        "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
        "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
        "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
        "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
        "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
        "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
        "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
        "Consider the example in Figure 1.",
        "If everyone provides service, then the system operates optimally.",
        "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
        "We show that with shared history, B knows that A served C and consequently will serve A.",
        "This results in a higher level of cooperation than with private history.",
        "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
        "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
        "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
        "The basic idea is that B would only believe C if C had already provided service to B.",
        "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
        "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
        "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
        "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
        "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
        "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
        "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
        "We show that short-term history prevents traitors from disrupting cooperation.",
        "The rest of the paper is organized as follows.",
        "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
        "We then proceed to the incentive techniques in Section 4.",
        "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
        "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
        "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
        "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
        "We discuss related work in Section 5 and conclude in Section 6. 2.",
        "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
        "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
        "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
        "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
        "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
        "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
        "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
        "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
        "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
        "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
        "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
        "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
        "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
        "Each game consists of two players who can defect or cooperate.",
        "Depending how each acts, the players receive a payoff.",
        "The players use a strategy to decide how to act.",
        "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
        "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
        "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
        "A player can be a client in one game and a server in another.",
        "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
        "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
        "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
        "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
        "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
        "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
        "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
        "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
        "If one player makes an untraceable action, the other player does not know the identity of the first player.",
        "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
        "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
        "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
        "This is the payoff matrix that we use in our simulation results.",
        "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
        "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
        "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
        "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
        "It does not specify which specific players switched.",
        "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
        "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
        "In our model, entities take independent and continuous actions that change the composition of the population.",
        "Time consists of rounds.",
        "In each round, every player plays one game as a client and one game as a server.",
        "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
        "If a player mutates, she switches to a randomly picked strategy.",
        "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
        "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
        "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
        "To learn, a player collects local information about the performance of different strategies.",
        "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
        "This models users communicating out-of-band about how strategies perform.",
        "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
        "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
        "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
        "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
        "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
        "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
        "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
        "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
        "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
        "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
        "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
        "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
        "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
        "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
        "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
        "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
        "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
        "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
        "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
        "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
        "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
        "This will cause Reciprocative players to defect on each other.",
        "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
        "Normalized generosity measures entity is generosity relative to entity js generosity.",
        "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
        "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
        "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
        "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
        "Table 1 presents the nominal parameter values used in our simulation.",
        "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
        "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
        "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
        "Private Figure 4: The evolution of strategy populations over time.",
        "Time the number of elapsed rounds.",
        "Population is the number of players using a strategy.",
        "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
        "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
        "Each point in the graph represents the number of players using a particular strategy at one point in time.",
        "Figures 5(a) and (b) show the corresponding mean overall score per round.",
        "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
        "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
        "We use this metric in all later results to evaluate our incentive techniques.",
        "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
        "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
        "It does not because of asymmetry of interest.",
        "For example, suppose player B is using Reciprocative with private history.",
        "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
        "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
        "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
        "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
        "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
        "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
        "In addition, private history does not deal well with asymmetry of interest.",
        "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
        "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
        "However, the asymmetry of transactions challenges selection mechanisms.",
        "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
        "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
        "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
        "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
        "This way, users approach their past recipients and give them a chance to reciprocate.",
        "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
        "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
        "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
        "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
        "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
        "In Figure 7 we fix the population size and vary the turnover rate.",
        "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
        "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
        "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
        "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
        "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
        "Some examples of shared history schemes are [20] [23] [28].",
        "Figure 7 shows the effectiveness of shared history under high turnover rates.",
        "In this figure, we fix the population size and vary the turnover rate.",
        "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
        "This means that 10% of the players leave the system at the end of each round.",
        "In Figure 6 we fix the turnover and vary the population size.",
        "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
        "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
        "Nevertheless, shared history has two disadvantages.",
        "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
        "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
        "The x-axis is the turnover rate.",
        "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
        "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
        "Second, and more fundamental, shared history is vulnerable to collusion.",
        "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
        "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
        "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
        "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
        "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
        "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
        "Our subjective algorithm is based on maxflow [24] [32].",
        "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
        "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
        "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
        "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
        "The maxflow between the source and the target in the graph is 2.",
        "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
        "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
        "This information can be stored using the same methods as the shared history.",
        "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
        "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
        "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
        "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
        "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
        "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
        "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
        "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
        "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
        "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
        "This bounds the overhead, but also bounds the effectiveness.",
        "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
        "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
        "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
        "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
        "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
        "The difference here is that objective shared history fails for all population sizes.",
        "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
        "However, subjective history can reach high levels of cooperation regardless of colluders.",
        "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
        "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
        "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
        "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
        "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
        "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
        "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
        "The other colluders can then exploit their reputation to receive service.",
        "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
        "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
        "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
        "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
        "Falsely claiming to receive service is the simple collusion attack described above.",
        "Falsely claiming not to have provided service provides no benefit to the attacker.",
        "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
        "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
        "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
        "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
        "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
        "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
        "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
        "In these cases, an attacker can only lower the reputation of the victim.",
        "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
        "However, in most P2P systems, identities are zero-cost.",
        "This is desirable for network growth as it encourages newcomers to join the system.",
        "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
        "Whitewashers can cause the system to collapse if they are not punished appropriately.",
        "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
        "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
        "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
        "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
        "Our solution is the Stranger Adaptive stranger policy.",
        "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
        "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
        "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
        "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
        "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
        "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
        "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
        "The x-axis is the turnover rate on a log scale.",
        "The y-axis is the mean overall per round score.",
        "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
        "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
        "The x-axis is the turnover rate on a log scale.",
        "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
        "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
        "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
        "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
        "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
        "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
        "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
        "However, Figure 12 shows that Stranger Defect is not effective with private history.",
        "This is because Reciprocative requires some initial cooperation to bootstrap.",
        "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
        "With private history, the Reciprocative player only knows about the other players actions toward her.",
        "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
        "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
        "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
        "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
        "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
        "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
        "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
        "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
        "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
        "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
        "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
        "We use the default values for the other parameters.",
        "Without traitors, the cooperative strategies thrive.",
        "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
        "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
        "Cooperation eventually collapses.",
        "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
        "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
        "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
        "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
        "The problem has been extensively studied adopting a game theoretic approach.",
        "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
        "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
        "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
        "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
        "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
        "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
        "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
        "Mechanism design is the inverse of game theory.",
        "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
        "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
        "More recently, DAMD has been also studied in dynamic environments [38].",
        "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
        "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
        "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
        "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
        "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
        "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
        "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
        "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
        "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
        "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
        "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
        "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
        "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
        "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
        "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
        "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
        "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
        "A.",
        "Free Riding on Gnutella.",
        "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
        "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
        "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
        "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
        "Incentives build robustness in bittorrent.",
        "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
        "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
        "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
        "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
        "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
        "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
        "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
        "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
        "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
        "Balances of Power on EBay: Peers or Unquals?",
        "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
        "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
        "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
        "Evolutionary Games and Population Dynamics.",
        "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
        "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
        "OReilly & Associates, Inc., March 2001, ch.",
        "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
        "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
        "Cooperative Peer Groups in Nice.",
        "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
        "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
        "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
        "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
        "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
        "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
        "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
        "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
        "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
        "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
        "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
        "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
        "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
        "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
        "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
        "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
        "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
        "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
        "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
        "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
    ],
    "translated_text_sentences": [
        "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C.",
        "Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C.",
        "La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día.",
        "Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores.",
        "Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos.",
        "Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas.",
        "A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos.",
        "Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1.",
        "INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios.",
        "Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3].",
        "En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26].",
        "Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6].",
        "En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento.",
        "Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés.",
        "A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema.",
        "Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación.",
        "Adoptamos un enfoque de teoría de juegos para abordar este problema.",
        "En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente.",
        "Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés.",
        "En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear).",
        "Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P.",
        "Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente.",
        "Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida.",
        "Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido.",
        "Considera el ejemplo en la Figura 1.",
        "Si todos brindan servicio, entonces el sistema opera de manera óptima.",
        "Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc.",
        "Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A.",
        "Esto resulta en un mayor nivel de cooperación que con la historia privada.",
        "El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión.",
        "En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio.",
        "Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población.",
        "La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B.",
        "El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema.",
        "Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo.",
        "Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene.",
        "Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema.",
        "La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares.",
        "El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión).",
        "La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas.",
        "Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación.",
        "El resto del documento está organizado de la siguiente manera.",
        "Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3.",
        "Luego procedemos a las técnicas de incentivos en la Sección 4.",
        "En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido.",
        "En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga.",
        "En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes.",
        "En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos.",
        "Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6.",
        "MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio.",
        "Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4).",
        "Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad.",
        "Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población.",
        "Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes.",
        "Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4).",
        "No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios.",
        "En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación.",
        "Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea.",
        "Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra.",
        "Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo.",
        "Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado.",
        "T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social.",
        "Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar.",
        "Dependiendo de cómo actúe cada uno, los jugadores reciben un pago.",
        "Los jugadores utilizan una estrategia para decidir cómo actuar.",
        "Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4].",
        "En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social.",
        "En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción.",
        "Un jugador puede ser un cliente en un juego y un servidor en otro.",
        "El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2).",
        "Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores.",
        "Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1.",
        "La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2.",
        "La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3.",
        "La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan.",
        "Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social.",
        "Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables.",
        "Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador.",
        "Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3.",
        "Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar.",
        "Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor.",
        "Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación.",
        "Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua.",
        "Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones.",
        "Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos.",
        "Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente.",
        "No especifica qué jugadores específicos cambiaron.",
        "Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real.",
        "Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños.",
        "En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población.",
        "El tiempo consiste en rondas.",
        "En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor.",
        "Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual.",
        "Si un jugador muta, cambia a una estrategia elegida al azar.",
        "Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación).",
        "Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora.",
        "Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira.",
        "Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias.",
        "Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa.",
        "Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias.",
        "Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia.",
        "La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad).",
        "Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación.",
        "Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3.",
        "FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo.",
        "Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador.",
        "Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos.",
        "Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social.",
        "Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación.",
        "Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema.",
        "En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema.",
        "Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección.",
        "Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios.",
        "Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables.",
        "En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente.",
        "La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada.",
        "La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido.",
        "Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse.",
        "Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente.",
        "La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad.",
        "Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3).",
        "Esto hará que los jugadores recíprocos se traicionen mutuamente.",
        "Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares.",
        "La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js.",
        "Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base.",
        "Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2.",
        "Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición.",
        "Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos.",
        "La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación.",
        "La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular.",
        "En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular.",
        "Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip.",
        "Figura 4: La evolución de las poblaciones de estrategias con el tiempo.",
        "Cronometra el número de rondas transcurridas.",
        "La población es el número de jugadores que utilizan una estrategia.",
        "En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados.",
        "Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador.",
        "Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado.",
        "Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda.",
        "Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden).",
        "A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo.",
        "Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos.",
        "La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores.",
        "Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema.",
        "No lo hace debido a la asimetría de intereses.",
        "Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado.",
        "El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin.",
        "El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo.",
        "Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4.",
        "TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación.",
        "Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar.",
        "Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo.",
        "Además, la historia privada no lidia bien con la asimetría de intereses.",
        "Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él.",
        "Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor.",
        "Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección.",
        "A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas.",
        "Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción.",
        "Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos.",
        "Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad.",
        "De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder.",
        "En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3).",
        "Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos.",
        "La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes.",
        "Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico).",
        "La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones.",
        "En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación.",
        "Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable.",
        "Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida.",
        "La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no.",
        "Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas.",
        "Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses.",
        "Algunos ejemplos de esquemas de historia compartida son [20] [23] [28].",
        "La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación.",
        "En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación.",
        "Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1.",
        "Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda.",
        "En la Figura 6 fijamos la rotación y variamos el tamaño de la población.",
        "Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población.",
        "Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones.",
        "Sin embargo, la historia compartida tiene dos desventajas.",
        "Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs.",
        "Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación.",
        "El eje x es la tasa de rotación.",
        "El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización.",
        "Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento.",
        "Segundo, y más fundamental, la historia compartida es vulnerable a la colusión.",
        "En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión.",
        "La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas).",
        "La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva).",
        "Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28].",
        "El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas.",
        "En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B.",
        "Nuestro algoritmo subjetivo se basa en maxflow [24] [32].",
        "El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad.",
        "Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él.",
        "El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones.",
        "En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis.",
        "El flujo máximo entre la fuente y el destino en el grafo es 2.",
        "Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos.",
        "Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí.",
        "Esta información se puede almacenar utilizando los mismos métodos que la historia compartida.",
        "Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación.",
        "Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación.",
        "La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás.",
        "Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0.",
        "Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j.",
        "La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores.",
        "Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución.",
        "El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3).",
        "En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno.",
        "La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo.",
        "Esto limita los costos adicionales, pero también limita la efectividad.",
        "A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado.",
        "La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores.",
        "En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro.",
        "Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9.",
        "Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla.",
        "La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población.",
        "Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones.",
        "Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores.",
        "Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0.",
        "Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo.",
        "Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza).",
        "Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado.",
        "Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento).",
        "A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados.",
        "Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices.",
        "Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio.",
        "Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado.",
        "Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia.",
        "Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad.",
        "Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio.",
        "Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente.",
        "Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante.",
        "Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad.",
        "Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio.",
        "Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo.",
        "Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo.",
        "Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora.",
        "Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación.",
        "Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella.",
        "En estos casos, un atacante solo puede disminuir la reputación de la víctima.",
        "La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes.",
        "Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno.",
        "Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema.",
        "Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear).",
        "Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente.",
        "Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo.",
        "Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento.",
        "Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción.",
        "Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores.",
        "Nuestra solución es la política de adaptación al extraño.",
        "La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños.",
        "Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente.",
        "La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs.",
        "Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4).",
        "Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2).",
        "Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs.",
        "Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido.",
        "El eje x es la tasa de rotación en una escala logarítmica.",
        "El eje y es el puntaje promedio general por ronda.",
        "Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos.",
        "Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado.",
        "El eje x es la tasa de rotación en una escala logarítmica.",
        "El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada.",
        "En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos.",
        "Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación.",
        "Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo.",
        "Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido.",
        "Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán.",
        "Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores.",
        "Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado.",
        "Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar.",
        "En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros.",
        "Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella.",
        "Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí.",
        "En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente.",
        "La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción).",
        "En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación.",
        "Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta.",
        "Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva.",
        "Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema.",
        "Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores.",
        "Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores.",
        "Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial).",
        "En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector.",
        "Utilizamos los valores predeterminados para los otros parámetros.",
        "Sin traidores, las estrategias cooperativas prosperan.",
        "Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas.",
        "A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje.",
        "La cooperación eventualmente colapsa.",
        "Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13.",
        "Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5.",
        "TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular.",
        "Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3].",
        "El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos.",
        "El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores.",
        "En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina.",
        "Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques.",
        "Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida.",
        "Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral.",
        "Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto).",
        "En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet.",
        "El diseño de mecanismos es el inverso de la teoría de juegos.",
        "Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado.",
        "El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast.",
        "Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38].",
        "En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas.",
        "Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31].",
        "Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados.",
        "Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado.",
        "Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente.",
        "Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos.",
        "Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión.",
        "BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6.",
        "CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer.",
        "Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema.",
        "Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses.",
        "Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos.",
        "Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7.",
        "AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo.",
        "Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811.",
        "Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos.",
        "REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B.",
        "I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish?",
        "Viajando gratis en Gnutella.",
        "Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación.",
        "BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P.",
        "En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas.",
        "En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B.",
        "Los incentivos construyen robustez en BitTorrent.",
        "En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc.",
        "En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil.",
        "En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo.",
        "En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast.",
        "En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras.",
        "En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos.",
        "Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos.",
        "El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer.",
        "En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A.",
        "¿Equilibrios de poder en eBay: ¿Pares o desiguales?",
        "En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica.",
        "En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes.",
        "Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND.",
        "Juegos evolutivos y dinámica de poblaciones.",
        "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P.",
        "En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed.",
        "O'Reilly & Associates, Inc., marzo de 2001, cap.",
        "Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero.",
        "En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B.",
        "Grupos de compañeros cooperativos en Niza.",
        "En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública.",
        "En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y.",
        "Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad.",
        "En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
        "En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc.",
        "En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen.",
        "Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos.",
        "Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A.",
        "Reenvío prioritario en redes ad hoc con partes auto-interesadas.",
        "En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I.",
        "Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos.",
        "En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación.",
        "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer.",
        "En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos.",
        "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo.",
        "En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P.",
        "En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B.",
        "Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos.",
        "En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc.",
        "En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111"
    ],
    "error_count": 3,
    "keys": {
        "p2p system": {
            "translated_key": "sistema P2P",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the <br>p2p system</br> using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a <br>p2p system</br>. 2.1 Assumptions We assume a <br>p2p system</br> in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real <br>p2p system</br>.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "To tackle these challenges we model the <br>p2p system</br> using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a <br>p2p system</br>. 2.1 Assumptions We assume a <br>p2p system</br> in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real <br>p2p system</br>."
            ],
            "translated_annotated_samples": [
                "Para abordar estos desafíos modelamos el <br>sistema P2P</br> utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos.",
                "MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un <br>sistema P2P</br>. 2.1 Supuestos Suponemos un <br>sistema P2P</br> en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio.",
                "Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un <br>sistema P2P</br> real."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el <br>sistema P2P</br> utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un <br>sistema P2P</br>. 2.1 Supuestos Suponemos un <br>sistema P2P</br> en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un <br>sistema P2P</br> real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "selfinterested user": {
            "translated_key": "usuarios con intereses propios",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among <br>selfinterested user</br>s.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among <br>selfinterested user</br>s."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre <br>usuarios con intereses propios</br>."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre <br>usuarios con intereses propios</br>. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "incentive for cooperation": {
            "translated_key": "incentivo para la cooperación",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "game-theoretic approach": {
            "translated_key": "teoría de juegos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a <br>game-theoretic approach</br> in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "We adopt a <br>game-theoretic approach</br> in addressing this problem."
            ],
            "translated_annotated_samples": [
                "Adoptamos un enfoque de <br>teoría de juegos</br> para abordar este problema."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de <br>teoría de juegos</br> para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "maxflow-based algorithm": {
            "translated_key": "algoritmo basado en flujo máximo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a <br>maxflow-based algorithm</br> that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the <br>maxflow-based algorithm</br> scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "We show that a <br>maxflow-based algorithm</br> that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "We show that the <br>maxflow-based algorithm</br> scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped."
            ],
            "translated_annotated_samples": [
                "Demostramos que un <br>algoritmo basado en flujo máximo</br> que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población.",
                "Mostramos que el <br>algoritmo basado en flujo máximo</br> escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un <br>algoritmo basado en flujo máximo</br> que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el <br>algoritmo basado en flujo máximo</br> escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reciprocative peer": {
            "translated_key": "par reciproco",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adaptive stranger policy": {
            "translated_key": "política del extraño adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • <br>adaptive stranger policy</br>: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The <br>adaptive stranger policy</br> does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an <br>adaptive stranger policy</br> promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger <br>adaptive stranger policy</br>.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • <br>adaptive stranger policy</br>: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "The <br>adaptive stranger policy</br> does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an <br>adaptive stranger policy</br> promotes persistent identities.",
                "Our solution is the Stranger <br>adaptive stranger policy</br>."
            ],
            "translated_annotated_samples": [
                "Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene.",
                "La <br>política del extraño adaptativo</br> logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares.",
                "En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una <br>política de extraños adaptativa</br> promueve identidades persistentes.",
                "Nuestra solución es la <br>política de adaptación al extraño</br>."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La <br>política del extraño adaptativo</br> logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una <br>política de extraños adaptativa</br> promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la <br>política de adaptación al extraño</br>. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    "política del extraño adaptativo",
                    "política de extraños adaptativa",
                    "política de adaptación al extraño"
                ]
            ]
        },
        "mutual cooperation": {
            "translated_key": "cooperación mutua",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "<br>mutual cooperation</br> leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "<br>mutual cooperation</br> leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "<br>mutual cooperation</br> leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "<br>mutual cooperation</br> leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3."
            ],
            "translated_annotated_samples": [
                "La <br>cooperación mutua</br> conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2.",
                "La <br>cooperación mutua</br> conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La <br>cooperación mutua</br> conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La <br>cooperación mutua</br> conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "asymmetric payoff": {
            "translated_key": "pagos asimétricas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, <br>asymmetric payoff</br> matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific <br>asymmetric payoff</br> matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an <br>asymmetric payoff</br> matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, <br>asymmetric payoff</br> matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "Unfortunately, existing work either uses a specific <br>asymmetric payoff</br> matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an <br>asymmetric payoff</br> matrix that preserves the social dilemma."
            ],
            "translated_annotated_samples": [
                "En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de <br>pagos asimétricas</br> para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente.",
                "Desafortunadamente, el trabajo existente utiliza o bien una <br>matriz de pagos asimétrica</br> específica o solo proporciona la forma general para una simétrica [4].",
                "En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de <br>pagos asimétrica</br> que preserva el dilema social."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de <br>pagos asimétricas</br> para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una <br>matriz de pagos asimétrica</br> específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de <br>pagos asimétrica</br> que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    "pagos asimétricas",
                    "matriz de pagos asimétrica",
                    "pagos asimétrica"
                ]
            ]
        },
        "generosity": {
            "translated_key": "generosidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized <br>generosity</br>.",
                "<br>generosity</br> measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is <br>generosity</br> is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the <br>generosity</br>.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own <br>generosity</br> as a measuring stick to judge its peers <br>generosity</br>.",
                "Normalized <br>generosity</br> measures entity is <br>generosity</br> relative to entity js generosity.",
                "More concretely, entity is normalized <br>generosity</br> as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs <br>generosity</br>, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent <br>generosity</br> of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized <br>generosity</br>.",
                "<br>generosity</br> measures the benefit an entity has provided relative to the benefit it has consumed.",
                "Entity is <br>generosity</br> is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the <br>generosity</br>.",
                "To prevent this situation, a Reciprocative player uses its own <br>generosity</br> as a measuring stick to judge its peers <br>generosity</br>.",
                "Normalized <br>generosity</br> measures entity is <br>generosity</br> relative to entity js generosity."
            ],
            "translated_annotated_samples": [
                "La probabilidad de que un jugador Recíproco coopere con un par es una función de su <br>generosidad</br> normalizada.",
                "La <br>generosidad</br> mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido.",
                "La <br>generosidad</br> de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la <br>generosidad</br>.",
                "Para prevenir esta situación, un jugador Recíproco utiliza su propia <br>generosidad</br> como una vara de medir para juzgar la <br>generosidad</br> de sus pares.",
                "La medida de <br>generosidad</br> normalizada mide la <br>generosidad</br> de una entidad en relación con la generosidad de la entidad js."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su <br>generosidad</br> normalizada. La <br>generosidad</br> mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La <br>generosidad</br> de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la <br>generosidad</br>. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia <br>generosidad</br> como una vara de medir para juzgar la <br>generosidad</br> de sus pares. La medida de <br>generosidad</br> normalizada mide la <br>generosidad</br> de una entidad en relación con la generosidad de la entidad js. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "parameter nominal value": {
            "translated_key": "valor nominal del parámetro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "<br>parameter nominal value</br> Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "<br>parameter nominal value</br> Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2."
            ],
            "translated_annotated_samples": [
                "Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "reciprocative decision function": {
            "translated_key": "función de decisión Recíproca",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the <br>reciprocative decision function</br> as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel <br>reciprocative decision function</br>, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the <br>reciprocative decision function</br> can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the <br>reciprocative decision function</br> in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "<br>reciprocative decision function</br> In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline <br>reciprocative decision function</br>.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the <br>reciprocative decision function</br> using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the <br>reciprocative decision function</br> with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based <br>reciprocative decision function</br> scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective <br>reciprocative decision function</br> scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the <br>reciprocative decision function</br>, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the <br>reciprocative decision function</br> as the basis of a family of incentives techniques.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel <br>reciprocative decision function</br>, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the <br>reciprocative decision function</br> can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "We describe the model in Section 2 and the <br>reciprocative decision function</br> in Section 3.",
                "<br>reciprocative decision function</br> In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques."
            ],
            "translated_annotated_samples": [
                "Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la <br>función de decisión Recíproca</br> como base de una familia de técnicas de incentivos.",
                "Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa <br>función de decisión Recíproca</br>, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente.",
                "Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la <br>función de decisión Recíproca</br> puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido.",
                "Describimos el modelo en la Sección 2 y la <br>función de decisión recíproca</br> en la Sección 3.",
                "FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la <br>función de decisión Recíproca</br> como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa <br>función de decisión Recíproca</br>, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la <br>función de decisión Recíproca</br> puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la <br>función de decisión recíproca</br> en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "whitewasher": {
            "translated_key": "blanqueador",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a <br>whitewasher</br> or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "Unfortunately, a player cannot tell if a stranger is a <br>whitewasher</br> or a legitimate newcomer."
            ],
            "translated_annotated_samples": [
                "Desafortunadamente, un jugador no puede saber si un desconocido es un <br>blanqueador</br> o un recién llegado legítimo."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un <br>blanqueador</br> o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "stranger adaptive": {
            "translated_key": "adaptación al extraño",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the <br>stranger adaptive</br> stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using <br>stranger adaptive</br> helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect <br>stranger adaptive</br> Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect <br>stranger adaptive</br> Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the <br>stranger adaptive</br> policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, <br>stranger adaptive</br> is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than <br>stranger adaptive</br> at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, <br>stranger adaptive</br> is the most effective.",
                "By using <br>stranger adaptive</br>, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "Our solution is the <br>stranger adaptive</br> stranger policy.",
                "The probability that a player using <br>stranger adaptive</br> helps a stranger is ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect <br>stranger adaptive</br> Figure 11: Different stranger policies for Reciprocative with shared history.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect <br>stranger adaptive</br> Figure 12: Different stranger policies for Reciprocative with private history.",
                "Figure 11 shows that with shared history, the <br>stranger adaptive</br> policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction)."
            ],
            "translated_annotated_samples": [
                "Nuestra solución es la política de <br>adaptación al extraño</br>.",
                "La probabilidad de que un jugador que usa <br>Stranger Adaptive</br> ayude a un desconocido es ps/cs.",
                "Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido.",
                "Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado.",
                "La Figura 11 muestra que con una historia compartida, la <br>política de Adaptación al Extraño</br> funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción)."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de <br>adaptación al extraño</br>. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa <br>Stranger Adaptive</br> ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la <br>política de Adaptación al Extraño</br> funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). ",
            "candidates": [],
            "error": [
                [
                    "adaptación al extraño",
                    "Stranger Adaptive",
                    "política de Adaptación al Extraño"
                ]
            ]
        },
        "stranger defect": {
            "translated_key": "Defecto del Extraño",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the <br>stranger defect</br> policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate <br>stranger defect</br> Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate <br>stranger defect</br> Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the <br>stranger defect</br> policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that <br>stranger defect</br> is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the <br>stranger defect</br> policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the <br>stranger defect</br> stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as <br>stranger defect</br> policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than <br>stranger defect</br> policy with private history because it can bootstrap cooperation.",
                "Although the <br>stranger defect</br> policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the <br>stranger defect</br> policy in 4.3).",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate <br>stranger defect</br> Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate <br>stranger defect</br> Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "In contrast, Figure 11 shows that the <br>stranger defect</br> policy is effective with shared history.",
                "However, Figure 12 shows that <br>stranger defect</br> is not effective with private history."
            ],
            "translated_annotated_samples": [
                "Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de <br>Defecto del Extraño</br> en 4.3).",
                "Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido.",
                "Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado.",
                "Por el contrario, la Figura 11 muestra que la política de <br>Defecto del Extraño</br> es efectiva con historial compartido.",
                "Sin embargo, la Figura 12 muestra que el <br>Defecto del Extraño</br> no es efectivo con historial privado."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de <br>Defecto del Extraño</br> en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de <br>Defecto del Extraño</br> es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el <br>Defecto del Extraño</br> no es efectivo con historial privado. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "incentive": {
            "translated_key": "incentivos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust <br>incentive</br> Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust <br>incentive</br> techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little <br>incentive</br> to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the <br>incentive</br> techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the <br>incentive</br> schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our <br>incentive</br> techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of <br>incentive</br> techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our <br>incentive</br> techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED <br>incentive</br> TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong <br>incentive</br> to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high <br>incentive</br> for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the <br>incentive</br> problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide <br>incentive</br> mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the <br>incentive</br> restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust <br>incentive</br> techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer <br>incentive</br> Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "Robust <br>incentive</br> Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Therefore, we propose a family of scalable and robust <br>incentive</br> techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little <br>incentive</br> to whitewash and whitewashing can be nearly eliminated from the system.",
                "We then proceed to the <br>incentive</br> techniques in Section 4.",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the <br>incentive</br> schemes, in this section we present a model of the users behaviors."
            ],
            "translated_annotated_samples": [
                "Técnicas de <br>incentivos</br> robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C.",
                "Por lo tanto, proponemos una familia de técnicas de <br>incentivos</br> escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente.",
                "Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco <br>incentivo</br> para encubrir y el encubrimiento puede ser casi eliminado del sistema.",
                "Luego procedemos a las técnicas de <br>incentivos</br> en la Sección 4.",
                "No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de <br>incentivos</br>, en esta sección presentamos un modelo de los comportamientos de los usuarios."
            ],
            "translated_text": "Técnicas de <br>incentivos</br> robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de <br>incentivos</br> escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco <br>incentivo</br> para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de <br>incentivos</br> en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de <br>incentivos</br>, en esta sección presentamos un modelo de los comportamientos de los usuarios. ",
            "candidates": [],
            "error": [
                [
                    "incentivos",
                    "incentivos",
                    "incentivo",
                    "incentivos",
                    "incentivos"
                ]
            ]
        },
        "peer-to-peer": {
            "translated_key": "peer-to-peer",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for <br>peer-to-peer</br> Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many <br>peer-to-peer</br> (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a <br>peer-to-peer</br> storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and <br>peer-to-peer</br> systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by <br>peer-to-peer</br> systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for <br>peer-to-peer</br> systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in <br>peer-to-peer</br> networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on <br>peer-to-peer</br> Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured <br>peer-to-peer</br> Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of <br>peer-to-peer</br> Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on <br>peer-to-peer</br> Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in <br>peer-to-peer</br> Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of <br>peer-to-peer</br> networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of <br>peer-to-peer</br> Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. <br>peer-to-peer</br>: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of <br>peer-to-peer</br> Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of <br>peer-to-peer</br> Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of <br>peer-to-peer</br> File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of <br>peer-to-peer</br> Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to <br>peer-to-peer</br> Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of <br>peer-to-peer</br> Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "Robust Incentive Techniques for <br>peer-to-peer</br> Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "INTRODUCTION Many <br>peer-to-peer</br> (P2P) systems rely on cooperation among selfinterested users.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a <br>peer-to-peer</br> storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and <br>peer-to-peer</br> systems in particular.",
                "The unique challenges imposed by <br>peer-to-peer</br> systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31]."
            ],
            "translated_annotated_samples": [
                "Técnicas de incentivos robustas para redes de <br>pares a pares</br> Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C.",
                "INTRODUCCIÓN Muchos <br>sistemas peer-to-peer</br> (P2P) dependen de la cooperación entre usuarios con intereses propios.",
                "Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento <br>peer-to-peer</br> [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento.",
                "TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas <br>peer-to-peer</br> en particular.",
                "Los desafíos únicos impuestos por los sistemas <br>peer-to-peer</br> inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de <br>pares a pares</br> Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos <br>sistemas peer-to-peer</br> (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento <br>peer-to-peer</br> [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas <br>peer-to-peer</br> en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas <br>peer-to-peer</br> inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. ",
            "candidates": [],
            "error": [
                [
                    "pares a pares",
                    "sistemas peer-to-peer",
                    "peer-to-peer",
                    "peer-to-peer",
                    "peer-to-peer"
                ]
            ]
        },
        "free-ride": {
            "translated_key": "libre circulación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who <br>free-ride</br> on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who <br>free-ride</br> on the systems resources, and is especially common in large networks [29] [3]."
            ],
            "translated_annotated_samples": [
                "Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (blanquean) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "reputation": {
            "translated_key": "reputación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective <br>reputation</br>, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective <br>reputation</br>: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes <br>reputation</br> subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good <br>reputation</br> will turn traitor and use his good <br>reputation</br> to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective <br>reputation</br> mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the <br>reputation</br> of a player (objective <br>reputation</br>).",
                "An example of objective <br>reputation</br> is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute <br>reputation</br> subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high <br>reputation</br> values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of <br>reputation</br> the source can give to the sink without violating <br>reputation</br> capacity constraints.",
                "As a result, nodes who dishonestly report high <br>reputation</br> values will not be able to subvert the <br>reputation</br> system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high <br>reputation</br> values for each other.",
                "When node A computes the subjective <br>reputation</br> of B using the maxflow algorithm, it will not be affected by the local false <br>reputation</br> values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective <br>reputation</br> of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective <br>reputation</br> to objective <br>reputation</br> in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their <br>reputation</br> to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own <br>reputation</br> and/or lower the <br>reputation</br> of another entity.",
                "An entity may want to lower another entitys <br>reputation</br> in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own <br>reputation</br>.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the <br>reputation</br> of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high <br>reputation</br> scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her <br>reputation</br> to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for <br>reputation</br> Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective <br>reputation</br>, and adaptive stranger policies.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective <br>reputation</br>: Shared history creates the possibility for collusion.",
                "We show that a maxflow-based algorithm that computes <br>reputation</br> subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good <br>reputation</br> will turn traitor and use his good <br>reputation</br> to exploit other peers.",
                "In Section 4.2, we describe collusion and demonstrate how subjective <br>reputation</br> mitigates it."
            ],
            "translated_annotated_samples": [
                "Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, <br>reputación</br> subjetiva basada en flujo máximo y políticas de extraños adaptativas.",
                "El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión.",
                "Demostramos que un algoritmo basado en flujo máximo que calcula la <br>reputación</br> de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población.",
                "La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena <br>reputación</br> se convierta en traidor y use su buena <br>reputación</br> para explotar a otros pares.",
                "En la Sección 4.2, describimos la colusión y demostramos cómo la <br>reputación</br> subjetiva la mitiga."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, <br>reputación</br> subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la <br>reputación</br> de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena <br>reputación</br> se convierta en traidor y use su buena <br>reputación</br> para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la <br>reputación</br> subjetiva la mitiga. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "collusion": {
            "translated_key": "colusión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, <br>collusion</br>, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for <br>collusion</br>.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite <br>collusion</br> among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe <br>collusion</br> and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to <br>collusion</br>.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 <br>collusion</br> and Other Shared History Attacks 4.2.1 <br>collusion</br> While shared history is scalable, it is vulnerable to collusion.",
                "<br>collusion</br> can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "<br>collusion</br> subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of <br>collusion</br> is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with <br>collusion</br>, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to <br>collusion</br> or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of <br>collusion</br> described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of <br>collusion</br> to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple <br>collusion</br> attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no <br>collusion</br>, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and <br>collusion</br> can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and <br>collusion</br>.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, <br>collusion</br>, zero-cost identities, and traitors.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for <br>collusion</br>.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite <br>collusion</br> among 1/3 of the population.",
                "In Section 4.2, we describe <br>collusion</br> and demonstrate how subjective reputation mitigates it.",
                "Second, and more fundamental, shared history is vulnerable to <br>collusion</br>."
            ],
            "translated_annotated_samples": [
                "Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, <br>colusión</br>, identidades de costo cero y traidores.",
                "El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de <br>colusión</br>.",
                "Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la <br>colusión</br> entre un tercio de la población.",
                "En la Sección 4.2, describimos la <br>colusión</br> y demostramos cómo la reputación subjetiva la mitiga.",
                "Segundo, y más fundamental, la historia compartida es vulnerable a la <br>colusión</br>."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, <br>colusión</br>, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de <br>colusión</br>. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la <br>colusión</br> entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la <br>colusión</br> y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la <br>colusión</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cheap pseudonym": {
            "translated_key": "seudónimo barato",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "whitewash": {
            "translated_key": "blanquear",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized Prisoners Dilemma (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a prisoners dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., <br>whitewash</br>).",
                "Strategies that work well in traditional prisoners dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to <br>whitewash</br> and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoners Dilemma.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized Prisoners Dilemma The Prisoners Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized Prisoners Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic Prisoners Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated Prisoners Dilemma games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the prisoners dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (<br>whitewash</br>) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The prisoners dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. Prisoners Dilemma.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., <br>whitewash</br>).",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to <br>whitewash</br> and whitewashing can be nearly eliminated from the system.",
                "In both figures, the players using the 100% Defect strategy change their identity (<br>whitewash</br>) after every transaction and are indistinguishable from legitimate newcomers."
            ],
            "translated_annotated_samples": [
                "En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, <br>blanquear</br>).",
                "Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para <br>encubrir</br> y el encubrimiento puede ser casi eliminado del sistema.",
                "En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (<br>blanquean</br>) después de cada transacción y son indistinguibles de los recién llegados legítimos."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el Dilema del Prisionero Generalizado (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de dilema del prisionero para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, <br>blanquear</br>). Las estrategias que funcionan bien en juegos tradicionales de dilema del prisionero, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para <br>encubrir</br> y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 Dilema del Prisionero Generalizado El Dilema del Prisionero, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. Cada juego consiste en dos jugadores que pueden optar por traicionar o cooperar. Dependiendo de cómo actúe cada uno, los jugadores reciben un pago. Los jugadores utilizan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente utiliza o bien una matriz de pagos asimétrica específica o solo proporciona la forma general para una simétrica [4]. En cambio, utilizamos el Dilema del Prisionero Generalizado (DPG), que especifica la forma general de una matriz de pagos asimétrica que preserva el dilema social. En el GPD, un jugador es el cliente y otro jugador es el servidor en cada juego, y solo la decisión del servidor es significativa para determinar el resultado de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pagos generalizada (Figura 2). Rc, Sc, Tc y Pc son las ganancias de los clientes, y Rs, Ss, Ts y Ps son las ganancias de los servidores. Una matriz de pagos de GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a mayores ganancias que la defección mutua (Rs + Rc > Ps + Pc). 2. La cooperación mutua conlleva a mayores ganancias que un jugador engañando al otro (Rs + Rc > Sc + Ts y Rs + Rc > Ss + Tc). 3. La deserción domina la cooperación (al menos débilmente) a nivel individual para la entidad que decide si cooperar o desertar: (Ts ≥ Rs y Ps ≥ Ss y (Ts > Rs o Ps > Ss)) El último conjunto de desigualdades asume que los clientes no incurren en un costo independientemente de si cooperan o desertan, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del clásico Dilema del Prisionero y permiten cualquier forma de transacción asimétrica mientras aún se crea un dilema social. Además, una o más de las cuatro posibles acciones (cliente cooperar y traicionar, y servidor cooperar y traicionar) pueden ser inrastreables. Si un jugador realiza una acción no rastreable, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como el intercambio de archivos o enrutamiento de superposición, utilizamos los valores específicos de la matriz de pagos mostrados en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pagos particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pagos que utilizamos en los resultados de nuestra simulación. Solicitar servicio No solicitar 7 / -1 0 / 0 0 / 0 0 / 0 Proporcionar servicio Ignorar solicitud Cliente Servidor Figura 3: La matriz de pagos para una aplicación como el intercambio de archivos P2P o enrutamiento de superposición. 2.4 Dinámica de poblaciones Una característica de los sistemas P2P es que los pares cambian su comportamiento y entran o salen del sistema de forma independiente y continua. Varios estudios [4] [28] de juegos repetidos del Dilema del Prisionero utilizan un modelo evolutivo [19] [34] de dinámica de poblaciones. Un modelo evolutivo no es adecuado para sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población de 5 jugadores 100% Cooperadores y 5 jugadores 100% Defectores evoluciona en una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, todos los cambios se producen al final de una generación en lugar de ser continuos, como en un sistema P2P real. Como resultado, la dinámica evolutiva de la población no modela con precisión el recambio, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. El tiempo consiste en rondas. En cada ronda, cada jugador juega un juego como cliente y otro juego como servidor. Al final de una ronda, un jugador puede: 1) mutar, 2) aprender, 3) cambiar de bando o 4) mantenerse igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si ella aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si ella mantiene su identidad después de cambiar de estrategias, entonces se le llama traidora. Si un jugador sufre una pérdida, abandona el sistema y es reemplazado por un recién llegado que utiliza la misma estrategia que el jugador que se retira. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste tanto en sus observaciones personales del rendimiento de la estrategia como en las observaciones de los jugadores con los que interactúa. Este modelo representa a los usuarios comunicándose fuera de banda sobre cómo funcionan las estrategias. Sea s el promedio móvil del rendimiento de la estrategia actual de un jugador por ronda y edad el número de rondas que ha estado utilizando la estrategia. La calificación de una estrategia es RunningAverage(s ∗ edad) RunningAverage(edad). Utilizamos la edad y calculamos el promedio móvil antes de la proporción para evitar que las muestras jóvenes (que tienen más probabilidades de ser valores atípicos) sesguen la calificación. Al final de una ronda, un jugador cambia a la estrategia mejor valorada con una probabilidad proporcional a la diferencia de puntuación entre su estrategia actual y la estrategia mejor valorada. 104 3. FUNCION DE DECISIÓN RECIPROCATIVA En esta sección, presentamos la nueva función de decisión, Reciprocative, que es la base de nuestras técnicas de incentivo. Una función de decisión mapea desde la historia de las acciones de un jugador hasta una decisión de cooperar o traicionar a ese jugador. Una estrategia consiste en una función de decisión, historial privado o compartido, un mecanismo de selección de servidor y una política de desconocidos. Nuestro enfoque en incentivos es diseñar estrategias que maximicen tanto el beneficio individual como el social. Los usuarios estratégicos elegirán utilizar tales estrategias y, de esta manera, impulsarán el sistema hacia altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son 100% Cooperar y 100% Defecto. 100% Cooperar modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotado. 100% Defecto modela a un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivo, los usuarios con 100% de Defectores dominarán rápidamente a los usuarios con 100% de Cooperadores y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) pueda utilizar historias compartidas y subjetivas, (2) pueda manejar defecciones no rastreables y (3) sea robusta contra diferentes patrones de defección. Las funciones de decisión anteriores como Tit-for-Tat y Image (ver Sección 5) no cumplen con estos criterios. Por ejemplo, Tit-for-Tat e Image basan sus decisiones tanto en cooperaciones como en defecciones, por lo tanto no pueden lidiar con defecciones no rastreables. En esta sección y en las secciones restantes demostramos cómo las estrategias basadas en el Reciprocative cumplen con todos los requisitos mencionados anteriormente. La probabilidad de que un jugador Recíproco coopere con un par es una función de su generosidad normalizada. La generosidad mide el beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que proporcionan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad i, dejemos que pi y ci sean los servicios que i ha proporcionado y consumido, respectivamente. La generosidad de una entidad es simplemente la proporción del servicio que proporciona al servicio que consume: g(i) = pi/ci. Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador Recíproco puede consumir más de lo que proporciona (por ejemplo, al usar inicialmente la política de Defecto del Extraño en 4.3). Esto hará que los jugadores recíprocos se traicionen mutuamente. Para prevenir esta situación, un jugador Recíproco utiliza su propia generosidad como una vara de medir para juzgar la generosidad de sus pares. La medida de generosidad normalizada mide la generosidad de una entidad en relación con la generosidad de la entidad js. Más concretamente, la entidad es la generosidad normalizada tal como la percibe la entidad j, que es gj(i) = g(i)/g(j). En el resto de esta sección, describimos nuestro marco de simulación y lo utilizamos para demostrar los beneficios de la función de decisión Recíproca base. Valor nominal del parámetro Tamaño de la población 100 2.4 Tiempo de ejecución 1000 rondas 2.4 Matriz de recompensa Compartir archivos 2.3 Proporción utilizando 100% Cooperar 1/3 3 Proporción utilizando 100% Defecto 1/3 3 Proporción utilizando Recíproco 1/3 3 Probabilidad de mutación 0.0 2.4 Probabilidad de aprendizaje 0.05 2.4 Probabilidad de rotación 0.0001 2.4 Tasa de acierto 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados. 3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos asimétrica para el intercambio de archivos (Figura 3) con defecciones no rastreables porque modela transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en redes ad-hoc y de superposición. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de los parámetros nominales utilizados en nuestra simulación. La proporción utilizando filas se refiere a la proporción inicial de la población total que utiliza una estrategia particular. En cada escenario variamos el rango de valores de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia Recíproca para defendernos contra esa situación o ataque. 3.2 Resultados de referencia 0 20 40 60 80 100 120 0 200 400 600 800 1000 Población Tiempo (a) Población total: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Defector Cooperador Recip. Figura 4: La evolución de las poblaciones de estrategias con el tiempo. Cronometra el número de rondas transcurridas. La población es el número de jugadores que utilizan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea base para escenarios más complicados. Las figuras 4(a) (60 jugadores) y (b) (120 jugadores) muestran cómo los jugadores cambian a estrategias de puntuación más altas con el tiempo en dos ejecuciones separadas del simulador. Cada punto en el gráfico representa el número de jugadores que utilizan una estrategia particular en un momento dado. Las figuras 5(a) y (b) muestran la puntuación media general correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (alcanzado cuando todos cooperan) y 0 es el mínimo (alcanzado cuando todos se desentienden). A partir de la matriz de beneficios compartidos de archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie puede hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5(a) muestra que la estrategia Recíproca utilizando historial privado hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero disminuye a 0.5 para 120 jugadores. Se esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores son eliminados del sistema. No lo hace debido a la asimetría de intereses. Por ejemplo, supongamos que el jugador B está utilizando Recíproco con historial privado. El jugador A puede llegar a pedir servicio al jugador B dos veces seguidas sin proporcionar servicio al jugador B en el ínterin. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de intereses y al fracaso del Reciprocative en el sistema de 120 jugadores en la Sección 4.1. 4. TÉCNICAS DE INCENTIVOS BASADAS EN LA RECIPROCIDAD En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento mediante simulación. Para hacer clara la exposición, agrupamos nuestras técnicas según los desafíos que abordan: poblaciones grandes y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4). 4.1 Poblaciones Grandes y Alta Rotación Las poblaciones grandes y la alta rotación de los sistemas P2P hacen menos probable que ocurran interacciones repetidas con una entidad familiar. Bajo estas condiciones, basar decisiones únicamente en la historia privada (registros sobre interacciones en las que el par ha estado directamente involucrado) no es efectivo. Además, la historia privada no lidia bien con la asimetría de intereses. Por ejemplo, si el jugador B ha cooperado con otros pero no con el jugador A en el pasado, el jugador A no tiene indicación de la generosidad del jugador B, por lo tanto, puede defectar injustamente en él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: selección de servidor e historial compartido. 4.1.1 Selección de servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminando la selección de servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pagos del dilema del prisionero, donde los jugadores pueden beneficiarse mutuamente dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que elige a su donante por segunda vez sin contribuir a ella en el ínterin puede enfrentar una deserción. Además, debido a la imposibilidad de rastrear las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para hacer frente a transacciones asimétricas, cada jugador mantiene listas (de tamaño fijo) de donantes y receptores pasados, y selecciona un servidor de una de estas listas al azar con igual probabilidad. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños; por lo tanto, asumimos que cada jugador puede realizar el servicio solicitado con una probabilidad p (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminatorios) eligen servidores para los juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con tamaños de población grandes. Fijamos la proporción inicial de Reciprocative en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (Nótese que, mientras en las Figuras 4(a) y (b), los puntos de datos muestran la evolución del sistema con el tiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión recíproca utilizando historial privado en conjunto con comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para tasas de rotación bajas, a medida que la rotación aumenta, el comportamiento selectivo no es escalable. Esto ocurre porque la selección solo es efectiva mientras los jugadores del pasado permanezcan vivos el tiempo suficiente para poder ser seleccionados en futuros juegos. 4.1.2 Historia compartida Para mitigar la asimetría de interés y escalar a una tasa de rotación más alta, se necesita una historia compartida. La historia compartida significa que cada par mantiene registros de todas las interacciones que ocurren en el sistema, independientemente de si estuvo directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo se requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, es más escalable para poblaciones grandes y altas rotaciones, y también tolera la asimetría de intereses. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Mientras que los jugadores selectivos con historial privado solo pueden tolerar un volumen de rotación moderado, el historial compartido se extiende a rotaciones de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Se muestra que la historia compartida hace que el sistema converja hacia la cooperación y el rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que la historia compartida aborda los tres desafíos de poblaciones grandes, alta rotación y asimetría de transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 PuntajePromedio/Ronda NumJugadores Compartido No-Seleccionado Privado No-Seleccionado Privado Selectivo Figura 6: Privado vs. Historia compartida como función del tamaño de la población. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 PuntuaciónMediaGlobal/Ronda Rotación Compartida No-Seleccionada Privada No-Seleccionada Privada Selectiva Figura 7: Rendimiento del mecanismo de selección bajo rotación. El eje x es la tasa de rotación. El eje y es la puntuación media general por ronda. Mientras que una implementación descentralizada de historial privado es sencilla, la implementación de historial compartido requiere sobrecarga de comunicación o centralización. Una historia compartida descentralizada puede implementarse, por ejemplo, sobre un DHT, utilizando un sistema de almacenamiento peer-to-peer [36] o mediante la difusión de información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y más fundamental, la historia compartida es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborda este problema. 4.2 Colusión y Otros Ataques de Historial Compartido 4.2.1 Colusión Aunque el historial compartido es escalable, es vulnerable a la colusión. La colusión puede ser tanto positiva (por ejemplo, entidades que se desvían afirman que otras entidades que se desvían cooperaron con ellas) como negativa (por ejemplo, entidades afirman que otras entidades cooperativas se desviaron de ellas). La colusión socava cualquier estrategia en la que todos en el sistema estén de acuerdo en la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión Recíproca con historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades en el sistema; otro ejemplo es la estrategia de Imagen [28]. El efecto de la colusión se magnifica en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación de forma subjetiva, donde el jugador A pondera las opiniones del jugador B en función de cuánto confía el jugador A en el jugador B. Nuestro algoritmo subjetivo se basa en maxflow [24] [32]. El flujo máximo es un problema teórico de grafos, que dado un grafo dirigido con aristas ponderadas pregunta cuál es la mayor tasa a la que se puede enviar material desde la fuente al destino sin violar ninguna restricción de capacidad. Por ejemplo, en la figura 8 cada borde está etiquetado con la cantidad de tráfico que puede transitar por él. El algoritmo de flujo máximo calcula la cantidad máxima de tráfico que puede ir desde la fuente (s) hasta el objetivo (t) sin violar las restricciones. En este ejemplo, aunque hay un bucle de aristas de alta capacidad, el flujo máximo entre la fuente y el destino es solo de 2 (los números entre paréntesis representan el flujo real en cada arista en la solución). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figura 8: Cada arista en el grafo está etiquetada con su capacidad y el flujo real que lleva entre paréntesis. El flujo máximo entre la fuente y el destino en el grafo es 2. Figura 9: Este gráfico ilustra la robustez del flujo máximo en presencia de cómplices que informan valores de reputación alta falsos. Aplicamos el algoritmo de flujo máximo construyendo un grafo cuyos vértices son entidades y las aristas son los servicios que las entidades han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que la historia compartida. Un maxflow es el mayor nivel de reputación que la fuente puede dar al sumidero sin violar las restricciones de capacidad de reputación. Como resultado, los nodos que informen de manera deshonesta valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludidores (etiquetados con C) reportan altos valores de reputación para los demás. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo de flujo máximo, no se verá afectado por los valores de reputación falsos locales, sino que en este caso el flujo máximo será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los coludidores. En nuestro algoritmo, el beneficio que la entidad i ha recibido (indirectamente) de la entidad j es el flujo máximo de j a i. Por el contrario, el beneficio que la entidad i ha proporcionado indirectamente a j es el flujo máximo de i a j. La reputación subjetiva de la entidad j tal como es percibida por i es: flujo máximo (j a i) flujo máximo (i a j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 Puntuación Media General/Ronda Población Compartida Privada Subjetiva Figura 10: Historia compartida subjetiva comparada con la historia compartida objetiva y la historia privada en presencia de coludidores. Algoritmo 1 FLUJO MÁXIMO EN TIEMPO CONSTANTE Limita el tiempo de ejecución promedio de Maxflow a una constante. método CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Usa la media de ejecución como predicción.} 2: si random() > (0.5∗self.surplus/self.mean iteraciones) entonces 3: return None {No hay suficiente excedente para ejecutar.} 4: fin si {Obtener el flujo y el número de iteraciones utilizadas del algoritmo de flujo máximo.} 5: flujo, iteraciones ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iteraciones {Mantener una media de las iteraciones utilizadas.} 7: self.mean iteraciones ← self.α ∗ self.mean iteraciones + (1 − self.α) ∗ iteraciones 8: return flujo El costo de maxflow es su largo tiempo de ejecución. El algoritmo de preflujo-pulso de flujo máximo estándar tiene un tiempo de ejecución en el peor de los casos de O(V^3). En cambio, utilizamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo aunque exista uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita los costos adicionales, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión Reciprocative basada en flujo máximo escala a poblaciones más grandes que una que utiliza historial privado. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coludidores. En estos escenarios, los desertores coluden al afirmar que otros coludidores que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el Algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, la reciprocidad con historial privado resulta en cooperación hasta cierto punto, más allá del cual falla. La diferencia aquí es que la historia objetiva compartida falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los coludidores debido a sus altas reputaciones. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coludidores. Esto se debe a que no hay caminos de alto peso en el grafo de cooperación desde los coludidores hacia cualquier no coludidor, por lo que el flujo máximo desde un coludidor hacia cualquier no coludidor es 0. Por lo tanto, un jugador Recíproco subjetivo concluirá que el tramposo no le ha proporcionado ningún servicio y rechazará prestarle servicio al tramposo. Por lo tanto, el algoritmo de flujo máximo permite a Reciprocative mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que hemos acotado el tiempo de ejecución del algoritmo de flujo máximo, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión Recíproca subjetiva se adapta a poblaciones más grandes que aquella que utiliza historial privado. Esta ventaja solo aumenta con el tiempo a medida que la potencia de la CPU aumenta y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (aumentando el parámetro de incremento). A pesar de la robustez del algoritmo de flujo máximo ante la forma simple de colusión descrita anteriormente, aún presenta vulnerabilidades ante ataques más sofisticados. Una forma es que una entidad (el topo) proporcione servicio y luego mienta positivamente sobre otros cómplices. Los otros conspiradores pueden entonces aprovechar su reputación para recibir servicio. Sin embargo, la efectividad de este ataque depende de la cantidad de servicio que proporcione el infiltrado. Dado que el topo está pagando todos los costos de brindar el servicio y no está recibiendo ninguno de los beneficios, tiene un fuerte incentivo para dejar de coludir e intentar otra estrategia. Esto obliga a los coludidores a utilizar mecanismos para mantener la cooperación dentro de su grupo, lo que puede hacer que el costo de la colusión supere el beneficio. 4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o proporcionar un servicio a otra entidad. Hay cuatro acciones posibles sobre las que se puede mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Falsamente afirmar recibir un servicio es el simple ataque de colusión descrito anteriormente. Falsamente afirmar que no se ha proporcionado el servicio no proporciona ningún beneficio al atacante. Falsamente afirmar haber proporcionado un servicio o no haberlo recibido permite a un atacante mejorar su propia reputación y/o disminuir la reputación de otra entidad. Una entidad puede querer disminuir la reputación de otra entidad para desalentar a otros de seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias donde una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo de flujo máximo para que una entidad siempre crea en la entidad que está más cerca de él en el grafo de flujo. Si ambas entidades están igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación evita aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella de la entidad evaluadora, lo que le impide mejorar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber proporcionado un servicio a una entidad víctima que está más lejos del evaluador que ella, o que no lo haya recibido de ella. En estos casos, un atacante solo puede disminuir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios proporcionados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso. 108 4.3 La historia de las identidades sin costo asume que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades no tienen costo alguno. Esto es deseable para el crecimiento de la red, ya que anima a los recién llegados a unirse al sistema. Sin embargo, esto también permite a los usuarios que se comportan mal escapar de las consecuencias de sus acciones al cambiar a nuevas identidades (es decir, blanquear). Los encubridores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede saber si un desconocido es un blanqueador o un recién llegado legítimo. Siempre cooperar con desconocidos anima a los recién llegados a unirse, pero al mismo tiempo fomenta comportamientos de encubrimiento. Siempre desertar de los extraños evita el lavado de cara, pero desanima a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política de extraños que tenga una probabilidad fija de cooperar con extraños fracasará al ser demasiado tacaña cuando la mayoría de los extraños son recién llegados o demasiado generosa cuando la mayoría de los extraños son encubridores. Nuestra solución es la política de adaptación al extraño. La idea es ser generoso con los desconocidos cuando son generosos y tacaño cuando son tacaños. Sean ps y cs el número de servicios que los extraños han proporcionado y consumido, respectivamente. La probabilidad de que un jugador que usa Stranger Adaptive ayude a un desconocido es ps/cs. Sin embargo, no deseamos mantener estos conteos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños desertan porque las deserciones son inrastreables (como se describe en la Sección 2). Por lo tanto, en lugar de mantener ps y cs, asumimos que k = ps + cs, donde k es una constante y mantenemos la proporción en ejecución r = ps/cs. Cuando necesitamos incrementar ps o cs, generamos los valores actuales de ps y cs a partir de k y r: cs = k/(1 + r) ps = cs ∗ r Luego calculamos el nuevo r de la siguiente manera: r = (ps + 1)/cs, si el extraño proporcionó servicio r = ps/(cs + 1), si el extraño consumió servicio Este método nos permite mantener una proporción en curso que refleja la generosidad reciente de los extraños sin saber cuándo los extraños han desertado. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Desertar Extraño Adaptativo Figura 11: Diferentes políticas de extraños para Recíproco con historial compartido. El eje x es la tasa de rotación en una escala logarítmica. El eje y es el puntaje promedio general por ronda. Las figuras 11 y 12 comparan la efectividad de la estrategia Recíproca utilizando diferentes políticas hacia los desconocidos. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 PuntuaciónMediaGlobal/Ronda Rotación Extraño Cooperar Extraño Defecto Extraño Adaptativo Figura 12: Diferentes políticas de extraños para Recíproco con historial privado. El eje x es la tasa de rotación en una escala logarítmica. El eje y es la puntuación media general por ronda. compara diferentes políticas de extraños para Reciprocative con historia compartida, mientras que la Figura 12 es con historia privada. En ambas figuras, los jugadores que utilizan la estrategia de Defecto al 100% cambian su identidad (<br>blanquean</br>) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores recíprocos que utilizan la política de Cooperar con Extraños fracasan completamente en lograr la cooperación. Esta política de extraños permite a los blanqueadores maximizar sus ganancias y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien al blanqueo. Por el contrario, la Figura 11 muestra que la política de Defecto del Extraño es efectiva con historial compartido. Esto se debe a que los encubridores siempre parecen ser extraños y, por lo tanto, los jugadores Recíprocos siempre los traicionarán. Esto es consistente con trabajos anteriores [13] que muestran que castigar a los extraños lidia con los encubridores. Sin embargo, la Figura 12 muestra que el Defecto del Extraño no es efectivo con historial privado. Esto se debe a que Reciprocative requiere cierta cooperación inicial para arrancar. En el caso de la historia compartida, un jugador Recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador Recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la defección inicial dictada por la política del Defecto del Extraño llevará a futuras defecciones, lo que evitará que los jugadores Recíprocos cooperen entre sí. En otras simulaciones no mostradas aquí, la política de extraño Defecto falla incluso con historial compartido cuando no hay jugadores que Cooperen al 100% inicialmente. La Figura 11 muestra que con una historia compartida, la política de Adaptación al Extraño funciona tan bien como la política de Defecto del Extraño hasta que la tasa de rotación es muy alta (10% de la población que cambia después de cada transacción). En estos escenarios, Stranger Adaptive está utilizando k = 10 y cada jugador mantiene un r privado. Más importante aún, es significativamente mejor que la política de Stranger Defect con historial privado porque puede fomentar la cooperación. Aunque la política de Defecto de Extraño es ligeramente más efectiva que la Adaptativa de Extraño a tasas muy altas de rotación, es poco probable que los sistemas P2P operen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una rotación muy alta. Concluimos que de las políticas de extraños que hemos explorado, la de Adaptación a Extraños es la más efectiva. Al utilizar Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades. 109 4.4 Traidores Los traidores son jugadores que adquieren altas puntuaciones de reputación cooperando por un tiempo, y luego traicionan al convertirse en desertores antes de abandonar el sistema. Modelan tanto a los usuarios que recurren deliberadamente para obtener una puntuación más alta como a los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene un historial a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos superiores en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen un historial a largo plazo (nunca borran el historial). En estas simulaciones, corremos durante 2000 rondas y permitimos que los jugadores cooperativos mantengan sus identidades al cambiar a la estrategia de 100% Defector. Utilizamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que esta cooperadora aprovecha su reputación para lograr una puntuación alta, otros jugadores cooperativos se dan cuenta y hacen lo mismo a través del aprendizaje. La cooperación eventualmente colapsa. Por otro lado, si mantenemos un historial a corto plazo y/o descontamos la historia antigua en comparación con la historia reciente, los traidores pueden ser detectados rápidamente, y el nivel general de cooperación se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 13. Compartido 0 20 40 60 80 100 1K 2K Historial a corto plazo Tiempo Población 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Manteniendo historial a largo plazo vs. a corto plazo tanto con como sin traidores. 5. TRABAJO RELACIONADO Trabajos anteriores han examinado el problema de incentivos aplicado a sociedades en general y más recientemente a aplicaciones de Internet y sistemas peer-to-peer en particular. Un fenómeno bien conocido en este contexto es la tragedia de los comunes [18], donde los recursos son subprovisionados debido a usuarios egoístas que se aprovechan de los recursos del sistema, y es especialmente común en redes grandes [29] [3]. El problema ha sido estudiado extensamente adoptando un enfoque de teoría de juegos. El modelo del dilema del prisionero proporciona un marco natural para estudiar la efectividad de diferentes estrategias en establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia de Tit-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de la Imagen y demuestran su capacidad para establecer la cooperación entre jugadores a pesar de pocas transacciones repetidas mediante el uso de la historia compartida. Los jugadores que utilizan la imagen cooperan con los jugadores cuya cantidad global de cooperaciones menos defecciones excede cierto umbral. Como resultado, un jugador de Imagen es vulnerable a los defraudadores parciales (si el umbral se establece demasiado bajo) o no coopera con otros jugadores de Imagen (si el umbral se establece demasiado alto). En los últimos años, los investigadores han utilizado la teoría del diseño de mecanismos económicos para abordar el problema de la cooperación en las aplicaciones de Internet. El diseño de mecanismos es el inverso de la teoría de juegos. Se pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El Diseño de Mecanismos Algorítmicos Distribuidos busca soluciones dentro de este marco que sean tanto completamente distribuidas como computacionalmente viables [12]. [10] y [11] son ejemplos de la aplicación de DAMD al enrutamiento BGP y el reparto de costos de multicast. Más recientemente, DAMD también ha sido estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es coherente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas peer-to-peer inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], y compartición de archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y encuentran que en tales sistemas es inevitable castigar a todos los recién llegados. Utilizando un modelo teórico, demuestran que dicho sistema puede converger hacia la cooperación solo para tasas de rotación suficientemente bajas, lo cual confirman nuestros resultados. [6] y [9] muestran que el lavado de imagen y la colusión pueden tener consecuencias graves para los sistemas peer-to-peer y son difíciles de prevenir en un sistema completamente descentralizado. Algunos clientes comerciales de intercambio de archivos [1] [2] proporcionan mecanismos de incentivos que se hacen efectivos al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser eludidos por un usuario experto o por una empresa competidora que lance un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos siguen siendo vulnerables a identidades de costo cero y colusión. BitTorrent utiliza el método de Tit-for-Tat como un mecanismo de asignación de recursos, donde la velocidad de carga de un usuario dicta su velocidad de descarga. 6. CONCLUSIONES En este artículo adoptamos un enfoque de teoría de juegos para abordar el problema de la cooperación en redes peer-to-peer. Abordando los desafíos impuestos por los sistemas P2P, incluyendo grandes poblaciones, alta rotación, asimetría de intereses e identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión Recíproca, para apoyar el comportamiento cooperativo y mejorar el rendimiento general del sistema. Encontramos que la adopción de técnicas de historia compartida y selección discriminada de servidores puede mitigar el desafío de pocas transacciones repetidas que surge debido al gran tamaño de la población, alta rotación y asimetría de intereses. Además, la cooperación puede establecerse incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los desconocidos. Finalmente, los coludidores y traidores pueden ser controlados a través de reputaciones subjetivas y de la historia a corto plazo, respectivamente. 110 7. AGRADECIMIENTOS Agradecemos a Mary Baker, T.J. Giuli, Petros Maniatis, al revisor anónimo y a nuestra editora, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el artículo. Este trabajo cuenta con el apoyo parcial de la Fundación Nacional de Ciencias bajo los premios ITR ANI-0085879 y ANI-0331659, y el premio Career ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como representativas de las políticas oficiales, ya sea expresadas o implícitas, de la NSF o del gobierno de los Estados Unidos. REFERENCIAS [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., Y HUBERMAN, B. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Viajando gratis en Gnutella. Primer lunes 5, 10 (octubre de 2000). [4] AXELROD, R. La evolución de la cooperación. BURAGOHAIN, C., AGRAWAL, D., Y SURI, S. Un marco teórico de teoría de juegos para incentivos en sistemas P2P. En Conferencia Internacional sobre Computación entre Pares (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., Y WALLACH, D. S. Seguridad para Redes Superpuestas entre Pares Estructuradas. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [7] COHEN, B. Los incentivos construyen robustez en BitTorrent. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., Y OSTRING, S. Modelando Incentivos para la Colaboración en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [9] DOUCEUR, J. R. El Ataque Sybil. En Actas Electrónicas del Taller Internacional sobre Sistemas Peer-to-Peer (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., Y SHENKER, S. Un Mecanismo basado en BGP para Enrutamiento de Menor Costo. En Actas del Simposio de la ACM sobre Principios de Computación Distribuida (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., Y SHENKER, S. Compartiendo el Costo de las Transmisiones Multicast. En Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., Y SHENKER, S. Diseño de Mecanismos Algorítmicos Distribuidos: Resultados Recientes y Direcciones Futuras. En Actas del Taller Internacional sobre Algoritmos Discretos y Métodos para la Computación y Comunicaciones Móviles (2002). [13] FRIEDMAN, E., Y RESNICK, P. El Costo Social de los Seudónimos Baratos. Revista de Estrategia Económica y de Gestión 10, 2 (1998), 173-199. [14] FUDENBERG, D., Y LEVINE, D. K. La Teoría del Aprendizaje en los Juegos. El MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., Y LILLIBRIDGE, M. Incentivos para compartir en redes peer-to-peer. En Actas de la 3ra conferencia de ACM sobre Comercio Electrónico, octubre de 2001 (2001). [16] GROSS, B., Y ACQUISTI, A. ¿Equilibrios de poder en eBay: ¿Pares o desiguales? En Taller sobre economía de redes peer-to-peer (2003). [17] GU, B., Y JARVENPAA, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos? - Una investigación empírica. En el 1er Taller sobre Economía de Sistemas Peer-to-Peer (2003). [18] HARDIN, G. La tragedia de los comunes. Ciencia 162 (1968), 1243-1248. [19] JOSEF HOFBAUER Y KARL SIGMUND. Juegos evolutivos y dinámica de poblaciones. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., Y GARCIA-MOLINA, H. El algoritmo EigenTrust para la gestión de reputación en redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide Web (mayo de 2003). [21] KAN, G. Peer-to-Peer: Aprovechando el Poder de las Tecnologías Disruptivas, 1ra ed. O'Reilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122. [22] KUHN, S. Dilema del prisionero. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, Ed., edición de verano de 2003. [23] LEE, S., SHERWOOD, R., Y BHATTACHARJEE, B. Grupos de compañeros cooperativos en Niza. En Actas de IEEE INFOCOM (2003). [24] LEVIEN, R., Y AIKEN, A. Métricas de confianza resistentes a ataques para la certificación de clave pública. En Actas del Simposio de Seguridad USENIX (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., Y MULIADI, Y. Preservación de réplicas entre pares mediante votación muestreada con límite de velocidad. En ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., Y BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. En Actas de MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., Y MOLVA, R. Un Enfoque Teórico del Juego para Evaluar Mecanismos de Aplicación de la Cooperación en Redes Móviles Ad-Hoc. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [28] NOWAK, M. A., Y SIGMUND, K. Evolución de la Reciprocidad Indirecta mediante Puntuación de Imagen. Naturaleza 393 (1998), 573-577. [29] OLSON, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] RAGHAVAN, B., Y SNOEREN, A. Reenvío prioritario en redes ad hoc con partes auto-interesadas. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., Y FOSTER, I. Compartir o no compartir: Un análisis de los incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). [32] REITER, M. K., Y STUBBLEBINE, S. G. Análisis y Diseño de Métricas de Autenticación. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., Y GRIBBLE, S. D. Un estudio de medición de sistemas de intercambio de archivos peer-to-peer. En Actas de Computación Multimedia y Redes 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolución y la Teoría de Juegos. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., Y GIORDANO, S. Modelado de la cooperación en redes móviles ad-hoc: una descripción formal del egoísmo. En Modelado y Optimización en Redes Móviles, Ad-hoc e Inalámbricas (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., Y SIRER, E. G. KARMA: Un Marco Económico Seguro para el Compartir de Recursos P2P. En Taller sobre Economía de Redes Peer-to-Peer (2003). [37] WANG, W., Y LI, B. Jugar o Controlar: Un Enfoque de Ingeniería de Incentivos Peer-to-Peer basado en Teoría del Control de Juegos. En el Taller Internacional sobre Calidad de Servicio (junio de 2003). [38] WOODARD, C. J., Y PARKES, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas Peer-to-Peer (junio de 2003). 111 ",
            "candidates": [],
            "error": [
                [
                    "blanquear",
                    "encubrir",
                    "blanquean"
                ]
            ]
        },
        "prisoners dilemma": {
            "translated_key": "dilema del prisionero",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Incentive Techniques for Peer-to-Peer Networks Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 School of Information Management and Systems U.C.",
                "Berkeley 2 HP Labs 3 Computer Science Division U.C.",
                "Berkeley ABSTRACT Lack of cooperation (free riding) is one of the key problems that confronts todays P2P systems.",
                "What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, asymmetry of interest, collusion, zero-cost identities, and traitors.",
                "To tackle these challenges we model the P2P system using the Generalized <br>prisoners dilemma</br> (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "These techniques are fully distributed and include: discriminating server selection, maxflowbased subjective reputation, and adaptive stranger policies.",
                "Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.",
                "Categories and Subject Descriptors C.2.4 [Computer-Communication Networks]: Distributed Systems; J.4 [Social And Behavioral Sciences]: Economics General Terms Design, Economics 1.",
                "INTRODUCTION Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users.",
                "For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].",
                "In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].",
                "Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].",
                "In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.",
                "As a result, each users attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.",
                "A wants service from B, B wants service form C, and C wants service from A. utility of the system.",
                "Avoiding this tragedy of the commons [18] requires incentives for cooperation.",
                "We adopt a game-theoretic approach in addressing this problem.",
                "In particular, we use a <br>prisoners dilemma</br> model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.",
                "In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).",
                "Strategies that work well in traditional <br>prisoners dilemma</br> games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.",
                "However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.",
                "Consider the example in Figure 1.",
                "If everyone provides service, then the system operates optimally.",
                "However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.",
                "We show that with shared history, B knows that A served C and consequently will serve A.",
                "This results in a higher level of cooperation than with private history.",
                "The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion.",
                "In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.",
                "We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.",
                "The basic idea is that B would only believe C if C had already provided service to B.",
                "The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.",
                "To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation.",
                "We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped.",
                "We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.",
                "The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.",
                "The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).",
                "Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions.",
                "We show that short-term history prevents traitors from disrupting cooperation.",
                "The rest of the paper is organized as follows.",
                "We describe the model in Section 2 and the reciprocative decision function in Section 3.",
                "We then proceed to the incentive techniques in Section 4.",
                "In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.",
                "In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it.",
                "In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities.",
                "In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them.",
                "We discuss related work in Section 5 and conclude in Section 6. 2.",
                "MODEL AND ASSUMPTIONS In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system. 2.1 Assumptions We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit.",
                "However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).",
                "For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate.",
                "A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 .",
                "Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.",
                "We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4).",
                "We do not assume any centralized trust or centralized infrastructure. 2.2 Model To aid the development and study of the incentive schemes, in this section we present a model of the users behaviors.",
                "In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.",
                "Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants.",
                "Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her.",
                "This models the difficulty or expense of determining that a peer could have provided a service, but didnt.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized <br>prisoners dilemma</br>.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized <br>prisoners dilemma</br> The <br>prisoners dilemma</br>, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement.",
                "Each game consists of two players who can defect or cooperate.",
                "Depending how each acts, the players receive a payoff.",
                "The players use a strategy to decide how to act.",
                "Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].",
                "Instead, we use the Generalized <br>prisoners dilemma</br> (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma.",
                "In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction.",
                "A player can be a client in one game and a server in another.",
                "The client and server receive the payoff from a generalized payoff matrix (Figure 2).",
                "Rc, Sc, Tc, and Pc are the clients payoff and Rs, Ss, Ts, and Ps are the servers payoff.",
                "A GPD payoff matrix must have the following properties to create a social dilemma: 1.",
                "Mutual cooperation leads to higher payoffs than mutual defection (Rs + Rc > Ps + Pc). 2.",
                "Mutual cooperation leads to higher payoffs than one player suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc). 3.",
                "Defection dominates cooperation (at least weakly) at the individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate.",
                "These properties correspond to similar properties of the classic <br>prisoners dilemma</br> and allow any form of asymmetric transaction while still creating a social dilemma.",
                "Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable.",
                "If one player makes an untraceable action, the other player does not know the identity of the first player.",
                "For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3.",
                "This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting.",
                "In addition, for this particular payoff matrix, clients are unable to trace server defections.",
                "This is the payoff matrix that we use in our simulation results.",
                "Request Service Dont Request 7 / -1 0 / 0 0 / 0 0 / 0 Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing. 2.4 Population Dynamics A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.",
                "Several studies [4] [28] of repeated <br>prisoners dilemma</br> games use an evolutionary model [19] [34] of population dynamics.",
                "An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.",
                "For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively.",
                "It does not specify which specific players switched.",
                "Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system.",
                "As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.",
                "In our model, entities take independent and continuous actions that change the composition of the population.",
                "Time consists of rounds.",
                "In each round, every player plays one game as a client and one game as a server.",
                "At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same.",
                "If a player mutates, she switches to a randomly picked strategy.",
                "If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below).",
                "If she maintains her identity after switching strategies, then she is referred to as a traitor.",
                "If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.",
                "To learn, a player collects local information about the performance of different strategies.",
                "This information consists of both her personal observations of strategy performance and the observations of those players she interacts with.",
                "This models users communicating out-of-band about how strategies perform.",
                "Let s be the running average of the performance of a players current strategy per round and age be the number of rounds she has been using the strategy.",
                "A strategys rating is RunningAverage(s ∗ age) RunningAverage(age) .",
                "We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating.",
                "At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104 3.",
                "RECIPROCATIVE DECISION FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques.",
                "A decision function maps from a history of a players actions to a decision whether to cooperate with or defect on that player.",
                "A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy.",
                "Our approach to incentives is to design strategies which maximize both individual and social benefit.",
                "Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation.",
                "Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system.",
                "In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.",
                "Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.",
                "Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria.",
                "For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections .",
                "In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.",
                "The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity.",
                "Generosity measures the benefit an entity has provided relative to the benefit it has consumed.",
                "This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse.",
                "For some entity i, let pi and ci be the services i has provided and consumed, respectively.",
                "Entity is generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity.",
                "Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3).",
                "This will cause Reciprocative players to defect on each other.",
                "To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peers generosity.",
                "Normalized generosity measures entity is generosity relative to entity js generosity.",
                "More concretely, entity is normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.",
                "Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters. 3.1 Simulation Framework Our simulator implements the model described in Section 2.",
                "We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks.",
                "Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.",
                "Table 1 presents the nominal parameter values used in our simulation.",
                "The Ratio using rows refer to the initial ratio of the total population using a particular strategy.",
                "In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack.",
                "We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack. 3.2 Baseline Results 0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip.",
                "Private Figure 4: The evolution of strategy populations over time.",
                "Time the number of elapsed rounds.",
                "Population is the number of players using a strategy.",
                "In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios.",
                "Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator.",
                "Each point in the graph represents the number of players using a particular strategy at one point in time.",
                "Figures 5(a) and (b) show the corresponding mean overall score per round.",
                "This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects).",
                "From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so.",
                "We use this metric in all later results to evaluate our incentive techniques.",
                "Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players.",
                "One would expect the 60 player system to reach the optimal level of cooperation (6) because all the defectors are eliminated from the system.",
                "It does not because of asymmetry of interest.",
                "For example, suppose player B is using Reciprocative with private history.",
                "Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim.",
                "Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative.",
                "We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1. 4.",
                "RECIPROCATIVE-BASED INCENTIVE TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation.",
                "To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4). 4.1 Large Populations and High Turnover The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.",
                "Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective.",
                "In addition, private history does not deal well with asymmetry of interest.",
                "For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player Bs generosity, thus may unduly defect on him.",
                "We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history. 4.1.1 Server Selection A natural way to increase the probability of interacting with familiar peers is by discriminating server selection.",
                "However, the asymmetry of transactions challenges selection mechanisms.",
                "Unlike in the <br>prisoners dilemma</br> payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric.",
                "As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.",
                "In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.",
                "In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities.",
                "This way, users approach their past recipients and give them a chance to reciprocate.",
                "In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3).",
                "In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.",
                "Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes.",
                "We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario).",
                "The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.",
                "In Figure 7 we fix the population size and vary the turnover rate.",
                "It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale.",
                "This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games. 4.1.2 Shared history In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history.",
                "Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not.",
                "It allows players to leverage off of the experiences of others in cases of few repeat transactions.",
                "It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest.",
                "Some examples of shared history schemes are [20] [23] [28].",
                "Figure 7 shows the effectiveness of shared history under high turnover rates.",
                "In this figure, we fix the population size and vary the turnover rate.",
                "While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1.",
                "This means that 10% of the players leave the system at the end of each round.",
                "In Figure 6 we fix the turnover and vary the population size.",
                "It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.",
                "These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions.",
                "Nevertheless, shared history has two disadvantages.",
                "First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs.",
                "Shared History as a function of population size. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover.",
                "The x-axis is the turnover rate.",
                "The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization.",
                "A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols.",
                "Second, and more fundamental, shared history is vulnerable to collusion.",
                "In the next section we propose a mechanism that addresses this problem. 4.2 Collusion and Other Shared History Attacks 4.2.1 Collusion While shared history is scalable, it is vulnerable to collusion.",
                "Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them).",
                "Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation).",
                "An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28].",
                "The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.",
                "Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player Bs opinions based on how much player A trusts player B.",
                "Our subjective algorithm is based on maxflow [24] [32].",
                "Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints.",
                "For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.",
                "The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints.",
                "In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only 2 (the numbers in brackets represent the actual flow on each edge in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets.",
                "The maxflow between the source and the target in the graph is 2.",
                "C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.",
                "We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other.",
                "This information can be stored using the same methods as the shared history.",
                "A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints.",
                "As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.",
                "Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other.",
                "When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0.",
                "This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j.",
                "The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.",
                "Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time.",
                "The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).",
                "Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists.",
                "The essential idea is to bound the mean number of nodes examined during the maxflow computation.",
                "This bounds the overhead, but also bounds the effectiveness.",
                "Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.",
                "Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders.",
                "In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter.",
                "Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.",
                "As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails.",
                "The difference here is that objective shared history fails for all population sizes.",
                "This is because the Reciprocative players cooperate with the colluders because of their high reputations.",
                "However, subjective history can reach high levels of cooperation regardless of colluders.",
                "This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0.",
                "Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder.",
                "Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers).",
                "Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history.",
                "This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).",
                "Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks.",
                "One is for an entity (the mole) to provide service and then lie positively about other colluders.",
                "The other colluders can then exploit their reputation to receive service.",
                "However, the effectiveness of this attack relies on the amount of service that the mole provides.",
                "Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy.",
                "This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit. 4.2.2 False reports Another attack is for a defector to lie about receiving or providing service to another entity.",
                "There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service.",
                "Falsely claiming to receive service is the simple collusion attack described above.",
                "Falsely claiming not to have provided service provides no benefit to the attacker.",
                "Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity.",
                "An entity may want to lower another entitys reputation in order to discourage others from selecting it and exclusively use its service.",
                "These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not.",
                "To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph.",
                "If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored.",
                "This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation.",
                "The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her.",
                "In these cases, an attacker can only lower the reputation of the victim.",
                "The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108 4.3 Zero-Cost Identities History assumes that entities maintain persistent identities.",
                "However, in most P2P systems, identities are zero-cost.",
                "This is desirable for network growth as it encourages newcomers to join the system.",
                "However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing).",
                "Whitewashers can cause the system to collapse if they are not punished appropriately.",
                "Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer.",
                "Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior.",
                "Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.",
                "This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers.",
                "Our solution is the Stranger Adaptive stranger policy.",
                "The idea is to be generous to strangers when they are being generous and stingy when they are stingy.",
                "Let ps and cs be the number of services that strangers have provided and consumed, respectively.",
                "The probability that a player using Stranger Adaptive helps a stranger is ps/cs.",
                "However, we do not wish to keep these counts permanently (for reasons described in Section 4.4).",
                "Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).",
                "Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.",
                "When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score.",
                "Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers.",
                "Figure 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history.",
                "The x-axis is the turnover rate on a log scale.",
                "The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history.",
                "In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers.",
                "The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.",
                "This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.",
                "In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history.",
                "This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them.",
                "This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.",
                "However, Figure 12 shows that Stranger Defect is not effective with private history.",
                "This is because Reciprocative requires some initial cooperation to bootstrap.",
                "In the shared history case, a Reciprocative player can observe that another player has already cooperated with others.",
                "With private history, the Reciprocative player only knows about the other players actions toward her.",
                "Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other.",
                "In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.",
                "Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction).",
                "In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation.",
                "Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.",
                "We conclude that of the stranger policies that we have explored, Stranger Adaptive is the most effective.",
                "By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109 4.4 Traitors Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system.",
                "They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors.",
                "A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.",
                "The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history).",
                "In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy.",
                "We use the default values for the other parameters.",
                "Without traitors, the cooperative strategies thrive.",
                "With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds.",
                "As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning.",
                "Cooperation eventually collapses.",
                "On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip.",
                "Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors. 5.",
                "RELATED WORK Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular.",
                "A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the systems resources, and is especially common in large networks [29] [3].",
                "The problem has been extensively studied adopting a game theoretic approach.",
                "The <br>prisoners dilemma</br> model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players.",
                "In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.",
                "Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.",
                "Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history.",
                "Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold.",
                "As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).",
                "In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications.",
                "Mechanism design is the inverse of game theory.",
                "It asks how to design a game in which the behavior of strategic players results in the socially desired outcome.",
                "Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing.",
                "More recently, DAMD has been also studied in dynamic environments [38].",
                "In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.",
                "The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31].",
                "Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable.",
                "Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.",
                "Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code.",
                "These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions.",
                "Also, these mechanisms are still vulnerable to zero-cost identities and collusion.",
                "BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a users upload rate dictates his download rate. 6.",
                "CONCLUSIONS In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks.",
                "Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.",
                "We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest.",
                "Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers.",
                "Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110 7.",
                "ACKNOWLEDGMENTS We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper.",
                "This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.",
                "Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government. 8.",
                "REFERENCES [1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B.",
                "A.",
                "Free Riding on Gnutella.",
                "First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation.",
                "Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems.",
                "In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [7] COHEN, B.",
                "Incentives build robustness in bittorrent.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S. Modeling Incentives for Collaboration in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack.",
                "In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER, S. A BGP-based Mechanism for Lowest-Cost Routing.",
                "In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions.",
                "In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions.",
                "In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms.",
                "Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games.",
                "The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.",
                "In Proceedings of the 3rd ACM conference on Electronic Commerce, October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A.",
                "Balances of Power on EBay: Peers or Unquals?",
                "In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation.",
                "In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons.",
                "Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND.",
                "Evolutionary Games and Population Dynamics.",
                "Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H. The EigenTrust Algorithm for Reputation Management in P2P Networks.",
                "In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed.",
                "OReilly & Associates, Inc., March 2001, ch.",
                "Gnutella, pp. 94-122. [22] KUHN, S. <br>prisoners dilemma</br>.",
                "In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B.",
                "Cooperative Peer Groups in Nice.",
                "In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification.",
                "In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL, D. S. H., BAKER, M., AND MULIADI, Y.",
                "Preserving Peer Replicas by Rate-Limited Sampled Voting.",
                "In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks.",
                "In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring.",
                "Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups.",
                "Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A.",
                "Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.",
                "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design.",
                "ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems.",
                "In Proceedings of Multimedia Computing and Networking 2002 (MMCN 02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games.",
                "Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness.",
                "In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G. KARMA : A Secure Economic Framework for P2P Resource Sharing.",
                "In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B.",
                "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.",
                "In International Workshop on Quality of Service (June 2003). [38] WOODARD, C. J., AND PARKES, D. C. Strategyproof mechanisms for ad-hoc network formation.",
                "In Workshop on Economics of Peer-to-Peer Systems (June 2003). 111"
            ],
            "original_annotated_samples": [
                "To tackle these challenges we model the P2P system using the Generalized <br>prisoners dilemma</br> (GPD), and propose the Reciprocative decision function as the basis of a family of incentives techniques.",
                "In particular, we use a <br>prisoners dilemma</br> model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.",
                "Strategies that work well in traditional <br>prisoners dilemma</br> games such as Tit-for-Tat [4] will not fare well in the P2P context.",
                "For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized <br>prisoners dilemma</br>.",
                "T, R, P, and S stand for temptation, reward, punishment and sucker, respectively. 2.3 Generalized <br>prisoners dilemma</br> The <br>prisoners dilemma</br>, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement."
            ],
            "translated_annotated_samples": [
                "Para abordar estos desafíos modelamos el sistema P2P utilizando el <br>Dilema del Prisionero Generalizado</br> (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos.",
                "En particular, utilizamos un modelo de <br>dilema del prisionero</br> para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente.",
                "Las estrategias que funcionan bien en juegos tradicionales de <br>dilema del prisionero</br>, como Tit-for-Tat, no funcionarán bien en el contexto de P2P.",
                "Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado.",
                "T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 <br>Dilema del Prisionero</br> Generalizado El <br>Dilema del Prisionero</br>, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social."
            ],
            "translated_text": "Técnicas de incentivos robustas para redes de pares a pares Michal Feldman1 mfeldman@sims.berkeley.edu Kevin Lai2 klai@hp.com Ion Stoica3 istoica@cs.berkeley.edu John Chuang1 chuang@sims.berkeley.edu 1 Escuela de Gestión de Información y Sistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (aprovechamiento gratuito) es uno de los problemas clave que enfrentan los sistemas P2P de hoy en día. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de intereses, colusión, identidades de costo cero y traidores. Para abordar estos desafíos modelamos el sistema P2P utilizando el <br>Dilema del Prisionero Generalizado</br> (GPD) y proponemos la función de decisión Recíproca como base de una familia de técnicas de incentivos. Estas técnicas son completamente distribuidas e incluyen: selección de servidor discriminatorio, reputación subjetiva basada en flujo máximo y políticas de extraños adaptativas. A través de la simulación, demostramos que estas técnicas pueden llevar a un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos; J.4 [Ciencias Sociales y del Comportamiento]: Economía Términos Generales Diseño, Economía 1. INTRODUCCIÓN Muchos sistemas peer-to-peer (P2P) dependen de la cooperación entre usuarios con intereses propios. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general y la tasa de fallos aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia y la tasa de pérdida de paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos incluyen la preservación de archivos [25], los foros de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, el intento de cada usuario por maximizar su propia utilidad efectivamente reduce el A BC general. Figura 1: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere servicio de C, y C quiere servicio de A. utilidad del sistema. Evitar esta tragedia de los comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque de teoría de juegos para abordar este problema. En particular, utilizamos un modelo de <br>dilema del prisionero</br> para capturar la tensión esencial entre la utilidad individual y social, matrices de pagos asimétricas para permitir transacciones asimétricas entre pares, y un modelo dinámico de población basado en el aprendizaje para especificar el comportamiento de los pares individuales, que puede cambiar continuamente. Si bien los dilemas sociales han sido estudiados extensamente, las aplicaciones P2P imponen un conjunto único de desafíos, incluyendo: • Grandes poblaciones y alta rotación: Un sistema de intercambio de archivos como Gnutella y KaZaa puede superar los 100,000 usuarios simultáneos, y los nodos pueden tener una vida útil promedio del orden de minutos [33]. • Asimetría de interés: Las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: Muchos sistemas P2P permiten a los pares cambiar continuamente de identidades (es decir, blanquear). Las estrategias que funcionan bien en juegos tradicionales de <br>dilema del prisionero</br>, como Tit-for-Tat, no funcionarán bien en el contexto de P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una novedosa función de decisión Recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Selección Discriminativa de Servidores: la cooperación requiere familiaridad entre entidades ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen menos probable que se produzcan interacciones repetidas con una entidad conocida. Mostramos que al hacer que cada par mantenga un historial privado de 102 acciones de otros pares hacia ella, y utilizando una selección de servidores discriminatoria, la función de decisión Recíproca puede escalar a grandes poblaciones y niveles moderados de rotación. • Historial Compartido: Escalar a una mayor rotación y mitigar la asimetría de intereses requiere un historial compartido. Considera el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema opera de manera óptima. Sin embargo, si todos mantienen solo historiales privados, nadie proporcionará servicio porque B no sabe que A ha servido a C, etc. Mostramos que con una historia compartida, B sabe que A sirvió a C y consecuentemente servirá a A. Esto resulta en un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento basado en tablas hash distribuidas) para almacenar la historia. • Reputación subjetiva basada en flujo máximo: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que le proporcione servicio. Demostramos que un algoritmo basado en flujo máximo que calcula la reputación de manera subjetiva promueve la cooperación a pesar de la colusión entre un tercio de la población. La idea básica es que B solo creería en C si C ya hubiera proporcionado un servicio a B. El costo del algoritmo de flujo máximo es su tiempo de ejecución O(V^3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución constante promedio, que intercambia efectividad por complejidad de cálculo. Mostramos que el algoritmo basado en flujo máximo escala mejor que el historial privado en presencia de coludidores sin la confianza centralizada requerida en trabajos anteriores [9] [20]. • Política de Extraño Adaptativo: Las identidades de costo cero permiten a los pares no cooperativos escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detiene. Mostramos que si los pares recíprocos tratan a los desconocidos (pares sin historial) utilizando una política que se adapta al comportamiento de los desconocidos anteriores, los pares tienen poco incentivo para encubrir y el encubrimiento puede ser casi eliminado del sistema. La política del extraño adaptativo logra esto sin requerir una asignación centralizada de identidades, una tarifa de entrada para los recién llegados o limitación de velocidad. • Historia a corto plazo: La historia también crea la posibilidad de que un par previamente bien comportado con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El par podría estar tomando una decisión estratégica o alguien podría haber secuestrado su identidad (por ejemplo, comprometiendo su anfitrión). La historia a largo plazo empeora este problema al permitir que los pares con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores interrumpan la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión recíproca en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación, y mostramos la efectividad de la selección discriminada de servidores y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política de extraños adaptativa promueve identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo lidia con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. MODELO Y SUPUESTOS En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e introducimos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P. 2.1 Supuestos Suponemos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos que los usuarios cambien aleatoriamente su comportamiento con una baja probabilidad (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma velocidad. Un par puede satisfacer cualquier solicitud, y, a menos que se especifique lo contrario, los pares solicitan el servicio de manera uniformemente aleatoria de la población. Finalmente, asumimos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio para todos los clientes. Suponemos que los usuarios pueden contaminar la historia compartida con recomendaciones falsas (Sección 4.2), cambiar de identidad sin costo alguno (Sección 4.3) y suplantar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada ni infraestructura centralizada. 2.2 Modelo Para ayudar al desarrollo y estudio de los esquemas de incentivos, en esta sección presentamos un modelo de los comportamientos de los usuarios. En particular, modelamos los beneficios y costos de las interacciones P2P (el juego) y la dinámica de la población causada por mutación, aprendizaje y rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan a un gran conjunto de sistemas P2P: • Dilema Social: La cooperación universal debería resultar en una utilidad general óptima, pero los individuos que explotan la cooperación de otros sin cooperar ellos mismos (es decir, desertar) deberían beneficiarse más que los usuarios que sí cooperan. • Transacciones Asimétricas: Un par puede querer un servicio de otro par sin poder proporcionar actualmente el servicio que el segundo par desea. Las transacciones deben poder tener resultados asimétricos. • Deserciones no rastreables: Un par no debería poder determinar la identidad de los pares que han desertado en su contra. Esto modela la dificultad o el costo de determinar que un par podría haber proporcionado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par que realiza la consulta identifique al par que incumple. • Población Dinámica: Los pares deben poder cambiar su comportamiento y entrar o salir del sistema de forma independiente y continua. La excepción se discute en la Sección 4.1.1. Cooperar Defecto Cooperar Defecto Cliente Servidor sc RR / sc ST / sc PP / sc TS / Figura 2: Matriz de pagos para el Dilema del Prisionero Generalizado. T, R, P y S representan tentación, recompensa, castigo y tonto, respectivamente. 2.3 <br>Dilema del Prisionero</br> Generalizado El <br>Dilema del Prisionero</br>, desarrollado por Flood, Dresher y Tucker en 1950, es un juego no cooperativo repetido que cumple con el requisito del dilema social. ",
            "candidates": [],
            "error": [
                [
                    "Dilema del Prisionero Generalizado",
                    "dilema del prisionero",
                    "dilema del prisionero",
                    "Dilema del Prisionero",
                    "Dilema del Prisionero"
                ]
            ]
        }
    }
}