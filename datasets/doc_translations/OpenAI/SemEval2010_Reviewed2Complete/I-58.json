{
    "id": "I-58",
    "original_text": "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue. This paper focuses on domains where these threats come from unknown adversaries. These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games. However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies. In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium. Previous work has shown this problem of optimal strategy selection to be NP-hard. Therefore, we present a heuristic called ASAP, with three key advantages to address the problem. First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement. Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches. Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1. INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries. A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing. For example, a security robot may need to make a choice about which areas to patrol, and how often [16]. However, it will not know in advance exactly where a robber will choose to strike. A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy. They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location. It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely. However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day. A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games. A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs. The distribution of adversary types that an agent will face may be known or inferred from historical data. Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games. However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5]. In some settings, one player can (or must) commit to a strategy before the other players choose their strategies. These scenarios are known as Stackelberg games [6]. In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader. For example, the security agent (leader) must first commit to a strategy for patrolling various areas. This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers). The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob). Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously. To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1. The leader is the row player and the follower is the column player. Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game. The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability). The leaders payoff would then be 4 (3 and 5 with equal probability). In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5). However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium. Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium. The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers. Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue. Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8]. If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game. However, by transforming the game, the compact structure of the Bayesian game is lost. In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered. This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies). This method has three key advantages. First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example. Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12]. This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy. Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11]. The rest of the paper is organized as follows. In Section 2 we fully describe the patrolling domain and its properties. Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game. Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries. Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2. THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time. Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.). It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14]. To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game. The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order). The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen. For example, the robber can observe over time how often the security agent patrols each area. With this knowledge, the robber must choose a single house to rob. We assume that the robber generally takes a long time to rob a house. If the house chosen by the robber is not on the security agents route, then the robber successfully robs it. Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it. We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l). The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house). The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i. With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently. If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3. BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn. For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types. Since there is only one type of security agent, θ1 contains only one element. During the game, the robber knows its type but the security agent does not know the robbers type. For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → . A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8]. Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game. While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward. Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent. However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix. The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game. Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations. Let us assume there are two robber types a and b in the Bayesian game. Robber a will be active with probability α, and robber b will be active with probability 1 − α. The rules described in Section 2 allow us to construct simple payoff tables. Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}. The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l). Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α. First, consider robber type a. Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Using these values we construct a base payoff table as the payoff for the game against robber type a. For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber. The payoffs for the game against robber type b are constructed using different values. Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3]. The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game. The transformed, normal-form game is shown in Table 3. In the transformed game, the security agent is the column player, and the set of all robber types together is the row player. Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}. Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance. Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy. Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5]. From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3. The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types. Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices. Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j. A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1. Here, pxi is the probability that the security agent will choose its ith pure strategy. The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5]. For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader). The pxi variables give the optimal strategy for the security agent. Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types. Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs. This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4. HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach. In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown. In our domain, if utilities are not considered, this method will result in uniform-distribution strategies. One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations. We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards. Thus, the ASAP heuristic will produce strategies which are k-uniform. A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3. ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward. Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation. This is because the different follower (robber) types are independent of each other. Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types. This independence property is exploited in ASAP to yield a decomposition scheme. Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems. Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5]. For a single follower type, the algorithm works the following way. Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy. We then take the mixed strategy with the highest payoff. We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards. If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy. Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise. This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies. Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming. We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2]. Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y. Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality. In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ . These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i. Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y. In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower. Let the leader be the row player and the follower the column player. We denote by x the vector of strategies of the leader and q the vector of strategies of the follower. We also denote X and Q the index sets of the leader and followers pure strategies, respectively. The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j. Let k be the size of the multiset. We first fix the policy of the leader to some k-uniform policy x. The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k. We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t. P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower. The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy. Therefore each of these pure strategies is optimal. Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x). Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t. P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions. To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x). The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number. The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower. The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality). In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0. We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite. Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question. In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1. Problems (5) and (6) are equivalent. Proof: Consider x, q a feasible solution of (5). We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value. The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction. The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6). Constraint 3 of (6) is satisfied because P i∈X zij = kqj. Let us now consider q, z feasible for (6). We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value. In fact all constraints of (5) are readily satisfied by construction. To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij. This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof. Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5. DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower. Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types. We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types. We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively. We also index the payoff matrices on each follower l, considering the matrices Rl and Cl . Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above. We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy. Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game. In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl . The same relation holds between C and Cl . These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2. Problems (7) and (8) are equivalent. Proof: Consider x, ql , al with l ∈ L a feasible solution of (7). We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value. The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction. The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8). Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j. Lets now consider ql , zl , al feasible for (8). We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value. In fact all constraints of (7) are readily satisfied by construction. To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0. Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl. In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8). Therefore xiql j = zl ijl ql j = zl ij. This last equality is because both are 0 when j = jl. Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent. This shows that the transformation preserves the objective function value, completing the proof. We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables. We implemented the decomposed MILP and the results are shown in the following section. 6. EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3. We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses. The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions. The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case. All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively. Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1. Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8]. The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes. We use this method as a simple baseline to measure the performance of our heuristics. We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14]. The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain. Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP. In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied. The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains. Each of the three rows of graphs corresponds to a different randomly-generated scenario. The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds. All experiments that were not concluded in 30 minutes (1800 seconds) were cut off. The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown. The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime. For a 316 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios. MIP-Nash solves for even fewer robber types within the cutoff time. On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time. The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required. The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains. This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased. The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward. The ASAP method remains consistently close to the optimal, even as the number of robber types increases. The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP. This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy. The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward. Results here are for the three-house domain. The trend is that as as the multiset size is increased, the runtime and reward level both increase. Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain. In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset. The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds. In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space. However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7. SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments. In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information. Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent. Agents acting in the real world quite frequently have such incomplete information about other agents. Bayesian games have been a popular choice to model such incomplete information games [3]. The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games. Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18]. Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]). However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case. Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem. First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement. Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches. Our k-uniform strategies are similar to the k-uniform strategies of [12]. While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies. This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions. Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13]. However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14]. Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE). It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010. Sarit Kraus is also affiliated with UMIACS. 8. REFERENCES [1] R. W. Beard and T. McLain. Multiple UAV cooperative search under collision avoidance and limited range communication constraints. In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg. Bayesian games for threat prediction and situation analysis. In FUSION, 2004. [4] Y. Chevaleyre. Theoretical analysis of multi-agent patrolling problem. In AAMAS, 2004. [5] V. Conitzer and T. Sandholm. Choosing the best strategy to commit to. In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991. [7] C. Gui and P. Mohapatra. Virtual patrol: A new power conservation design for surveillance using sensor networks. In IPSN, 2005. [8] J. C. Harsanyi and R. Selten. A generalized Nash solution for two-person bargaining games with incomplete information. Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer. Generating and solving imperfect information games. In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer. Representations and solutions for game-theoretic problems. Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson. Equilibrium points of bimatrix games. Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta. Playing large games using simple strategies. In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul. Multi-agent patrolling: an empirical analysis on alternative architectures. In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In AAMAS, 2006. [15] T. Roughgarden. Stackelberg scheduling strategies. In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp. Patrolling in a stochastic environment. In 10th Intl. Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer. Mixed-integer programming methods for finding nash equilibria. In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman. Computing approximate Bayes-Nash equilibria with tree-games of incomplete information. In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)",
    "original_sentences": [
        "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
        "This paper focuses on domains where these threats come from unknown adversaries.",
        "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
        "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
        "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
        "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
        "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
        "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
        "Second, it provides strategies which are simple to understand, represent, and implement.",
        "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
        "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
        "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
        "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
        "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
        "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
        "However, it will not know in advance exactly where a robber will choose to strike.",
        "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
        "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
        "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
        "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
        "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
        "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
        "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
        "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
        "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
        "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
        "These scenarios are known as Stackelberg games [6].",
        "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
        "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
        "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
        "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
        "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
        "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
        "The leader is the row player and the follower is the column player.",
        "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
        "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
        "The leaders payoff would then be 4 (3 and 5 with equal probability).",
        "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
        "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
        "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
        "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
        "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
        "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
        "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
        "However, by transforming the game, the compact structure of the Bayesian game is lost.",
        "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
        "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
        "This method has three key advantages.",
        "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
        "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
        "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
        "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
        "The rest of the paper is organized as follows.",
        "In Section 2 we fully describe the patrolling domain and its properties.",
        "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
        "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
        "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
        "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
        "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
        "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
        "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
        "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
        "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
        "For example, the robber can observe over time how often the security agent patrols each area.",
        "With this knowledge, the robber must choose a single house to rob.",
        "We assume that the robber generally takes a long time to rob a house.",
        "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
        "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
        "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
        "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
        "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
        "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
        "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
        "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
        "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
        "Since there is only one type of security agent, θ1 contains only one element.",
        "During the game, the robber knows its type but the security agent does not know the robbers type.",
        "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
        "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
        "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
        "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
        "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
        "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
        "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
        "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
        "Let us assume there are two robber types a and b in the Bayesian game.",
        "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
        "The rules described in Section 2 allow us to construct simple payoff tables.",
        "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
        "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
        "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
        "First, consider robber type a.",
        "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
        "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
        "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
        "The payoffs for the game against robber type b are constructed using different values.",
        "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
        "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
        "The transformed, normal-form game is shown in Table 3.",
        "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
        "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
        "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
        "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
        "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
        "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
        "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
        "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
        "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
        "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
        "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
        "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
        "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
        "The pxi variables give the optimal strategy for the security agent.",
        "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
        "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
        "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
        "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
        "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
        "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
        "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
        "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
        "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
        "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
        "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
        "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
        "This is because the different follower (robber) types are independent of each other.",
        "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
        "This independence property is exploited in ASAP to yield a decomposition scheme.",
        "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
        "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
        "For a single follower type, the algorithm works the following way.",
        "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
        "We then take the mixed strategy with the highest payoff.",
        "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
        "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
        "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
        "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
        "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
        "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
        "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
        "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
        "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
        "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
        "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
        "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
        "Let the leader be the row player and the follower the column player.",
        "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
        "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
        "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
        "Let k be the size of the multiset.",
        "We first fix the policy of the leader to some k-uniform policy x.",
        "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
        "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
        "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
        "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
        "Therefore each of these pure strategies is optimal.",
        "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
        "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
        "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
        "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
        "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
        "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
        "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
        "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
        "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
        "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
        "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
        "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
        "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
        "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
        "Problems (5) and (6) are equivalent.",
        "Proof: Consider x, q a feasible solution of (5).",
        "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
        "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
        "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
        "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
        "Let us now consider q, z feasible for (6).",
        "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
        "In fact all constraints of (5) are readily satisfied by construction.",
        "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
        "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
        "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
        "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
        "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
        "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
        "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
        "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
        "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
        "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
        "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
        "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
        "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
        "The same relation holds between C and Cl .",
        "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
        "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
        "Problems (7) and (8) are equivalent.",
        "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
        "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
        "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
        "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
        "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
        "Lets now consider ql , zl , al feasible for (8).",
        "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
        "In fact all constraints of (7) are readily satisfied by construction.",
        "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
        "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
        "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
        "Therefore xiql j = zl ijl ql j = zl ij.",
        "This last equality is because both are 0 when j = jl.",
        "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
        "This shows that the transformation preserves the objective function value, completing the proof.",
        "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
        "We implemented the decomposed MILP and the results are shown in the following section. 6.",
        "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
        "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
        "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
        "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
        "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
        "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
        "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
        "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
        "We use this method as a simple baseline to measure the performance of our heuristics.",
        "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
        "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
        "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
        "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
        "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
        "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
        "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
        "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
        "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
        "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
        "For a 316 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
        "MIP-Nash solves for even fewer robber types within the cutoff time.",
        "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
        "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
        "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
        "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
        "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
        "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
        "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
        "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
        "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
        "Results here are for the three-house domain.",
        "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
        "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
        "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
        "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
        "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
        "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
        "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
        "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
        "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
        "Agents acting in the real world quite frequently have such incomplete information about other agents.",
        "Bayesian games have been a popular choice to model such incomplete information games [3].",
        "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
        "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
        "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
        "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
        "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
        "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
        "Second, it provides strategies which are simple to understand, represent, and implement.",
        "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
        "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
        "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
        "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
        "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
        "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
        "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
        "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
        "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
        "NBCHD030010.",
        "Sarit Kraus is also affiliated with UMIACS. 8.",
        "REFERENCES [1] R. W. Beard and T. McLain.",
        "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
        "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
        "Introduction to Linear Optimization.",
        "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
        "Bayesian games for threat prediction and situation analysis.",
        "In FUSION, 2004. [4] Y. Chevaleyre.",
        "Theoretical analysis of multi-agent patrolling problem.",
        "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
        "Choosing the best strategy to commit to.",
        "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
        "Game Theory.",
        "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
        "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
        "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
        "A generalized Nash solution for two-person bargaining games with incomplete information.",
        "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
        "Generating and solving imperfect information games.",
        "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
        "Representations and solutions for game-theoretic problems.",
        "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
        "Equilibrium points of bimatrix games.",
        "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
        "Playing large games using simple strategies.",
        "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
        "Multi-agent patrolling: an empirical analysis on alternative architectures.",
        "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
        "Security in multiagent systems by policy randomization.",
        "In AAMAS, 2006. [15] T. Roughgarden.",
        "Stackelberg scheduling strategies.",
        "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
        "Patrolling in a stochastic environment.",
        "In 10th Intl.",
        "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
        "Mixed-integer programming methods for finding nash equilibria.",
        "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
        "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
        "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "translated_text_sentences": [
        "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico.",
        "Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos.",
        "Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos.",
        "Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias.",
        "En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio.",
        "Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro.",
        "Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema.",
        "Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego.",
        "Segundo, proporciona estrategias que son simples de entender, representar e implementar.",
        "Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal.",
        "Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1.",
        "INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios.",
        "Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse.",
        "Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16].",
        "Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar.",
        "Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje.",
        "Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado.",
        "De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios.",
        "Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado.",
        "Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos.",
        "Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas.",
        "La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos.",
        "Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos.",
        "Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5].",
        "En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias.",
        "Estos escenarios son conocidos como juegos de Stackelberg [6].",
        "En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder.",
        "Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas.",
        "Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores).",
        "Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar).",
        "A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente.",
        "Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1.",
        "El líder es el jugador de la fila y el seguidor es el jugador de la columna.",
        "Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo.",
        "El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad).",
        "El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad).",
        "En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5).",
        "Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash.",
        "Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash.",
        "El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores.",
        "Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente.",
        "Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8].",
        "Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado.",
        "Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano.",
        "Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder.",
        "Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas).",
        "Este método tiene tres ventajas clave.",
        "Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior.",
        "Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12].",
        "Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política.",
        "Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11].",
        "El resto del documento está organizado de la siguiente manera.",
        "En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades.",
        "La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg.",
        "Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos.",
        "Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7.",
        "EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible.",
        "En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.).",
        "Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14].",
        "Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego.",
        "La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden).",
        "El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido.",
        "Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área.",
        "Con este conocimiento, el ladrón debe elegir una sola casa para robar.",
        "Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa.",
        "Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito.",
        "De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla.",
        "Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l).",
        "El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa).",
        "El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i.",
        "Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas.",
        "Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3.",
        "JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn.",
        "Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones.",
        "Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento.",
        "Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones.",
        "Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → .",
        "Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8].",
        "Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano.",
        "Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa.",
        "El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado.",
        "Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi.",
        "Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal.",
        "Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas.",
        "Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano.",
        "El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α.",
        "Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples.",
        "Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}.",
        "El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l).",
        "Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α.",
        "Primero, considera el tipo de ladrón a.",
        "Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
        "Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a.",
        "Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón.",
        "Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores.",
        "El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3].",
        "El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta.",
        "El juego en forma normal transformado se muestra en la Tabla 3.",
        "En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila.",
        "Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}.",
        "Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano.",
        "Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura.",
        "Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5].",
        "A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3.",
        "Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores.",
        "Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes.",
        "Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j.",
        "Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1.",
        "Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura.",
        "La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5].",
        "Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder).",
        "Las variables pxi dan la estrategia óptima para el agente de seguridad.",
        "Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones.",
        "Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados.",
        "Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4.",
        "Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico.",
        "En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas.",
        "En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme.",
        "Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales.",
        "Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad.",
        "Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes.",
        "Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3.",
        "ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa.",
        "Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi.",
        "Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí.",
        "Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores.",
        "Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición.",
        "Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash.",
        "Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5].",
        "Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera.",
        "Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa.",
        "Luego elegimos la estrategia mixta con el mayor beneficio.",
        "Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas.",
        "Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta.",
        "También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates.",
        "Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas.",
        "Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal.",
        "Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2].",
        "Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y.",
        "Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte.",
        "De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗.",
        "Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i.",
        "Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y.",
        "En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor.",
        "Que el líder sea el jugador de la fila y el seguidor el jugador de la columna.",
        "Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor.",
        "También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente.",
        "Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j.",
        "Sea k el tamaño del multiconjunto.",
        "Primero fijamos la política del líder a alguna política x k-uniforme.",
        "El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k.",
        "Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
        "La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor.",
        "El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima.",
        "Por lo tanto, cada una de estas estrategias puras es óptima.",
        "Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
        "Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x).",
        "Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a.",
        "El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal.",
        "Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x).",
        "El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
        "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande.",
        "Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor.",
        "La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha).",
        "De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0.",
        "Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva.",
        "Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante.",
        "En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
        "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1.",
        "Los problemas (5) y (6) son equivalentes.",
        "Prueba: Considera x, q una solución factible de (5).",
        "Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo.",
        "La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción.",
        "El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6).",
        "La restricción 3 de (6) se cumple porque P i∈X zij = kqj.",
        "Consideremos ahora q, z factibles para (6).",
        "Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo.",
        "De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción.",
        "Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij.",
        "Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba.",
        "Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5.",
        "DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor.",
        "Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores.",
        "Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores.",
        "También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente.",
        "También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl.",
        "Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior.",
        "Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme.",
        "Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
        "El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego.",
        "De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl.",
        "La misma relación se mantiene entre C y Cl.",
        "Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
        "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2.",
        "Los problemas (7) y (8) son equivalentes.",
        "Prueba: Considera x, ql, al con l ∈ L una solución factible de (7).",
        "Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo.",
        "La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción.",
        "El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8).",
        "La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j.",
        "Consideremos ahora ql, zl, al factibles para (8).",
        "Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo.",
        "De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción.",
        "Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0.",
        "Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl.",
        "En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8).",
        "Por lo tanto xiql j = zl ijl ql j = zl ij.",
        "Esta última igualdad se debe a que ambos son 0 cuando j = jl.",
        "Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente.",
        "Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba.",
        "Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras.",
        "Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6.",
        "RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3.",
        "Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas.",
        "La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón.",
        "Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base.",
        "Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente.",
        "Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1.",
        "Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8].",
        "El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla.",
        "Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas.",
        "Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14].",
        "Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad.",
        "Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP.",
        "En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía.",
        "El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha).",
        "Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente.",
        "El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos.",
        "Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos.",
        "El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra.",
        "El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución.",
        "Para un 316 La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios.",
        "MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite.",
        "Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite.",
        "El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido.",
        "El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha).",
        "Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones.",
        "La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima.",
        "El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones.",
        "Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP.",
        "Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash.",
        "El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas.",
        "Los resultados aquí son para el dominio de las tres casas.",
        "La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan.",
        "No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio.",
        "En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos.",
        "El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos.",
        "En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande.",
        "Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema.",
        "RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles.",
        "En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta.",
        "Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad.",
        "Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes.",
        "Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3].",
        "El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos.",
        "Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18].",
        "Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]).",
        "Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano.",
        "Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema.",
        "Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego.",
        "Segundo, proporciona estrategias que son simples de entender, representar e implementar.",
        "Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal.",
        "Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques.",
        "Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12].",
        "Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes.",
        "Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio.",
        "Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13].",
        "Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14].",
        "Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE).",
        "También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No.",
        "NBCHD030010.",
        "Sarit Kraus también está afiliada a UMIACS. 8.",
        "REFERENCIAS [1] R. W. Beard y T. McLain.",
        "Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado.",
        "En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis.",
        "Introducción a la Optimización Lineal.",
        "Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg.",
        "Juegos bayesianos para la predicción de amenazas y análisis de situaciones.",
        "En FUSION, 2004. [4] Y. Chevaleyre.",
        "Análisis teórico del problema de patrullaje multiagente.",
        "En AAMAS, 2004. [5] V. Conitzer y T. Sandholm.",
        "Elegir la mejor estrategia a comprometerse.",
        "En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole.",
        "Teoría de juegos.",
        "MIT Press, 1991. [7] C. Gui y P. Mohapatra.",
        "Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores.",
        "En IPSN, 2005. [8] J. C. Harsanyi y R. Selten.",
        "Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta.",
        "Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer.",
        "Generando y resolviendo juegos de información imperfecta.",
        "En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer.",
        "Representaciones y soluciones para problemas de teoría de juegos.",
        "Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson.",
        "Puntos de equilibrio de juegos bimatrix.",
        "Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta.",
        "Jugando juegos grandes utilizando estrategias simples.",
        "En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul.",
        "Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas.",
        "En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus.",
        "Seguridad en sistemas multiagentes mediante la aleatorización de políticas.",
        "En AAMAS, 2006. [15] T. Roughgarden.",
        "Estrategias de programación de Stackelberg.",
        "En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp.",
        "Patrullando en un entorno estocástico.",
        "En la 10ª Conferencia Internacional.",
        "Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer.",
        "Métodos de programación entera mixta para encontrar equilibrios de Nash.",
        "En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman.",
        "Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta.",
        "En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)"
    ],
    "error_count": 5,
    "keys": {
        "adversarial multiagent domain": {
            "translated_key": "dominios multiagentes adversariales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In <br>adversarial multiagent domain</br>s, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In <br>adversarial multiagent domain</br>s, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue."
            ],
            "translated_annotated_samples": [
                "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En <br>dominios multiagentes adversariales</br>, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En <br>dominios multiagentes adversariales</br>, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent security via approximate policy": {
            "translated_key": "seguridad del agente a través de una política aproximada",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "security of agent system": {
            "translated_key": "seguridad del sistema de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "agent system security": {
            "translated_key": "seguridad del sistema de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "bayesian game": {
            "translated_key": "juego bayesiano",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, <br>bayesian game</br> representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A <br>bayesian game</br> is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a <br>bayesian game</br> with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the <br>bayesian game</br> into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the <br>bayesian game</br> is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the <br>bayesian game</br>, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a <br>bayesian game</br> [6]. 3.",
                "BAYESIAN GAMES A <br>bayesian game</br> contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A <br>bayesian game</br> can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the <br>bayesian game</br>.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the <br>bayesian game</br> into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the <br>bayesian game</br>.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the <br>bayesian game</br> in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a <br>bayesian game</br> thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a <br>bayesian game</br> with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a <br>bayesian game</br> (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, <br>bayesian game</br> representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Third, it operates directly on the compact, <br>bayesian game</br> representation, without requiring conversion to normal form.",
                "A <br>bayesian game</br> is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a <br>bayesian game</br> with multiple types of followers.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the <br>bayesian game</br> into a normal-form game by the Harsanyi transformation [8].",
                "However, by transforming the game, the compact structure of the <br>bayesian game</br> is lost."
            ],
            "translated_annotated_samples": [
                "Tercero, opera directamente sobre la representación compacta del <br>juego bayesiano</br>, sin necesidad de convertirla a forma normal.",
                "Un <br>juego bayesiano</br> es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas.",
                "El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un <br>juego bayesiano</br> con múltiples tipos de seguidores.",
                "Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el <br>juego bayesiano</br> en un juego en forma normal mediante la transformación de Harsanyi [8].",
                "Sin embargo, al transformar el juego, se pierde la estructura compacta del <br>juego bayesiano</br>."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del <br>juego bayesiano</br>, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un <br>juego bayesiano</br> es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un <br>juego bayesiano</br> con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el <br>juego bayesiano</br> en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del <br>juego bayesiano</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "bayes-nash equilibrium": {
            "translated_key": "equilibrio de Bayes-Nash",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a <br>bayes-nash equilibrium</br>, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a <br>bayes-nash equilibrium</br>, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or <br>bayes-nash equilibrium</br> is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward <br>bayes-nash equilibrium</br> is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward <br>bayes-nash equilibrium</br>, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward <br>bayes-nash equilibrium</br> with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a <br>bayes-nash equilibrium</br>, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "First, ASAP searches for the highest-reward strategy, rather than a <br>bayes-nash equilibrium</br>, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Usually, these games are analyzed according to the solution concept of a <br>bayes-nash equilibrium</br>, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or <br>bayes-nash equilibrium</br> is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward <br>bayes-nash equilibrium</br> is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward <br>bayes-nash equilibrium</br>, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1."
            ],
            "translated_annotated_samples": [
                "Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un <br>equilibrio de Bayes-Nash</br>, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego.",
                "Por lo general, estos juegos se analizan según el concepto de solución de <br>equilibrio de Bayes-Nash</br>, una extensión del equilibrio de Nash para juegos bayesianos.",
                "Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5].",
                "Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el <br>equilibrio de Bayes-Nash</br> de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado.",
                "Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el <br>equilibrio de Bayes-Nash</br> de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un <br>equilibrio de Bayes-Nash</br>, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de <br>equilibrio de Bayes-Nash</br>, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el <br>equilibrio de Bayes-Nash</br> de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el <br>equilibrio de Bayes-Nash</br> de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "bayesian and stackelberg game": {
            "translated_key": "juego bayesiano y juego de Stackelberg",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "np-hard": {
            "translated_key": "NP-duro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be <br>np-hard</br>.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be <br>np-hard</br> in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is <br>np-hard</br> [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is <br>np-hard</br>, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be <br>np-hard</br> for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be <br>np-hard</br> in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Previous work has shown this problem of optimal strategy selection to be <br>np-hard</br>.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be <br>np-hard</br> in the case of a Bayesian game with multiple types of followers.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is <br>np-hard</br> [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is <br>np-hard</br>, we provide a heuristic approach.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be <br>np-hard</br> for Bayes-Nash problems."
            ],
            "translated_annotated_samples": [
                "Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es <br>NP-duro</br>.",
                "El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es <br>NP-duro</br> en el caso de un juego bayesiano con múltiples tipos de seguidores.",
                "Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es <br>NP-difícil</br> [5]. 4.",
                "Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es <br>NP-duro</br>, proporcionamos un enfoque heurístico.",
                "Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es <br>NP-duro</br> para problemas de Bayes-Nash."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es <br>NP-duro</br>. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es <br>NP-duro</br> en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es <br>NP-difícil</br> [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es <br>NP-duro</br>, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es <br>NP-duro</br> para problemas de Bayes-Nash. ",
            "candidates": [],
            "error": [
                [
                    "NP-duro",
                    "NP-duro",
                    "NP-difícil",
                    "NP-duro",
                    "NP-duro"
                ]
            ]
        },
        "patrolling domain": {
            "translated_key": "dominio de patrullaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the <br>patrolling domain</br> and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE <br>patrolling domain</br> In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our <br>patrolling domain</br>, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our <br>patrolling domain</br> without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The <br>patrolling domain</br> and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the <br>patrolling domain</br> from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "In Section 2 we fully describe the <br>patrolling domain</br> and its properties.",
                "THE <br>patrolling domain</br> In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "For our <br>patrolling domain</br>, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our <br>patrolling domain</br> without introducing the mathematical formulations.",
                "EXPERIMENTAL RESULTS The <br>patrolling domain</br> and the payoffs for the associated game are detailed in Sections 2 and 3."
            ],
            "translated_annotated_samples": [
                "En la Sección 2 describimos completamente el <br>dominio de patrullaje</br> y sus propiedades.",
                "EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible.",
                "Para nuestro <br>dominio de patrullaje</br>, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones.",
                "Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro <br>dominio de patrullaje</br> sin introducir las formulaciones matemáticas.",
                "RESULTADOS EXPERIMENTALES El <br>dominio de patrullaje</br> y las recompensas para el juego asociado se detallan en las Secciones 2 y 3."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el <br>dominio de patrullaje</br> y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro <br>dominio de patrullaje</br>, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro <br>dominio de patrullaje</br> sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El <br>dominio de patrullaje</br> y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "heuristic approach": {
            "translated_key": "enfoque heurístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient <br>heuristic approach</br> for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a <br>heuristic approach</br>.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient <br>heuristic approach</br> that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "An Efficient <br>heuristic approach</br> for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a <br>heuristic approach</br>.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient <br>heuristic approach</br> that is not focused on equilibrium solutions."
            ],
            "translated_annotated_samples": [
                "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico.",
                "Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un <br>enfoque heurístico</br>.",
                "Esto contrasta con ASAP, donde nuestro énfasis está en un <br>enfoque heurístico</br> altamente eficiente que no se centra en soluciones de equilibrio."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un <br>enfoque heurístico</br>. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un <br>enfoque heurístico</br> altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "decomposition for multiple adversary": {
            "translated_key": "descomposición para múltiples adversarios",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "mixed-integer linear program": {
            "translated_key": "programación lineal entera mixta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient <br>mixed-integer linear program</br> techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 <br>mixed-integer linear program</br> We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a <br>mixed-integer linear program</br> (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Recent work [17] has led to efficient <br>mixed-integer linear program</br> techniques to find the best Nash equilibrium for a given agent.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 <br>mixed-integer linear program</br> We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "Given this transformation to a <br>mixed-integer linear program</br> (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5."
            ],
            "translated_annotated_samples": [
                "El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de <br>programación lineal entera mixta</br> para encontrar el mejor equilibrio de Nash para un agente dado.",
                "En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "Dada esta transformación a un <br>programa lineal entero mixto</br> (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de <br>programación lineal entera mixta</br> para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un <br>programa lineal entero mixto</br> (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "programación lineal entera mixta",
                    "programa lineal entero mixto"
                ]
            ]
        },
        "game theory": {
            "translated_key": "teoría de juegos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in <br>game theory</br>, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "<br>game theory</br>.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Given that the Harsanyi transformation is a standard concept in <br>game theory</br>, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "<br>game theory</br>."
            ],
            "translated_annotated_samples": [
                "Dado que la transformación de Harsanyi es un concepto estándar en la <br>teoría de juegos</br>, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas.",
                "<br>Teoría de juegos</br>."
            ],
            "translated_text": "Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la <br>teoría de juegos</br>, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. <br>Teoría de juegos</br>. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}