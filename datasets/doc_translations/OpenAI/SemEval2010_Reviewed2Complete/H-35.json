{
    "id": "H-35",
    "original_text": "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1. INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15]. Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. Several methods for learning to rank have been developed and applied to document retrieval. For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost. All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank. AdaRank utilizes a linear combination of weak rankers as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker. We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. Tuning ranking models using certain training data and a performance measure is a common practice in IR [1]. As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning. Recently, direct optimization of performance measures in learning has become a hot research topic. Several methods for classification [17] and ranking [5, 19] have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach. The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discussions are given in Section 4. Section 5 concludes this paper and gives future work. 2. RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1]. For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue. Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained. For example, Joachims [16] applies Ranking SVM to document retrieval. He utilizes click-through data to deduce training data for the model creation. Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR. Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents. Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval. The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work. They are learning to rank, boosting, and direct optimization of performance measures. Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores. Several approaches have been proposed to tackle the problem. One major approach to learning to rank is that of transforming it into binary classification on instance pairs. This pair-wise approach fits well with information retrieval and thus is widely used in IR. Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3]. For other approaches to learning to rank, refer to [2, 11, 31]. In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16]. In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures. Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted. Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction). Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26]. Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8]. In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26]. Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR. Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification. Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15]. Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning. AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3. OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval. In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores. The relevance scores are calculated with a ranking function (model). In learning (training), a number of queries and their corresponding retrieved documents are given. Furthermore, the relevance levels of the documents with respect to the queries are also provided. The relevance levels are represented as ranks (i.e., categories in a total order). The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing. Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks. There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship. In training, a set of queries Q = {q1, q2, · · · , qm} is given. Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j. A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Thus, the training set can be represented as S = {(qi, di, yi)}m i=1. The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores. Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself. We use π( j) to denote the position of item j (i.e., di j). The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries. Table 1: Notations and explanations. Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers. In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function. By query based measure, we mean a measure defined over a ranking list of documents with respect to a query. These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15]. We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures. The first argument of E is the permutation π created using the ranking function f on di. The second argument is the list of ranks yi given by humans. E measures the agreement between π and yi. Table 1 gives a summary of notations described above. Next, as examples of performance measures, we present the definitions of MAP and NDCG. Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j. Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. The algorithm is referred to as AdaRank and is shown in Figure 1. AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters. AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T). Finally, it outputs a ranking model f by linearly combining the weak rankers. At each round, AdaRank maintains a distribution of weights over the queries in the training data. We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m. For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . End For Output ranking model: f(x) = fT (x). Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i). Initially, AdaRank sets equal weights to the queries. At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far. As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries. At each round, a weak ranker ht is constructed based on training data with weight distribution Pt. The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Several methods for weak ranker construction can be considered. For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt. In this paper, we use single features as weak rankers, as will be explained in Section 3.6. Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker. Intuitively, αt measures the importance of ht. A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). In contrast, AdaRank tries to optimize a loss function based on queries. Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1]. We next explain why this is the case. Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions. This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle. We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ . We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Several ways of computing coefficients αt and weak rankers ht may be considered. Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1. It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1. T 1. The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix. The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above. In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet. First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1]. Notice that the major IR measures meet this requirement. In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16]. Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms. The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data. The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8]. Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods. Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs. As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed. In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval. The existing methods cannot focus on the training on the tops, as indicated in [4]. Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem. In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4]. AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm. In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost. First, the types of instances are different. AdaRank makes use of queries and their corresponding document lists as instances. The labels in training data are lists of ranks (relevance levels). AdaBoost makes use of feature vectors as instances. The labels in training data are simply +1 and −1. Second, the performance measures are different. In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query. In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25]. Third, the ways of updating weights are also different. In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner. In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1. Note that AdaBoost can also adopt the weight updating method used in AdaRank. For AdaBoost they are equivalent (cf., [12] page 305). However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments. In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero. 4. EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov. Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets. C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods. Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ). For AdaRank, the parameter T was determined automatically during each experiment. Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined). As the measure E, MAP and NDCG@5 were utilized. The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank. The OHSUMED dataset consists of 348,566 documents and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant). The data have been used in many experiments in IR, for example [4, 29]. As features, we adopted those used in document retrieval [4]. Table 2 shows the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features. BM25 score itself is also a feature. Stop words were removed and stemming was conducted in the data. We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments. We tuned the parameters for BM25 during one of the trials and applied them to the other trials. The results reported in Figure 2 are those averaged over four trials. In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets. Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant. From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures. We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP. The results indicate that all the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5. The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300). Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query). Following the practice in [28], the queries that have less than 10 relevant documents were discarded. Table 3 shows the statistics on the two datasets. In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. We also conducted 4-fold cross-validation experiments. The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively. From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP. We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP. The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05). However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval. The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002. There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset. Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used. The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant. The number of relevant pages vary from query to query (from 1 to 86). We extracted 14 features from each query-document pair. Table 4 gives a list of the features. They are the outputs of some well-known algorithms (systems). These features are different from those in Table 2, because the task is different. Again, we conducted 4-fold cross-validation experiments. The results averaged over four trials are reported in Figure 5. From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures. We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost. Some of the improvements are not statistically significant. This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples. First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost. Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. The results averaged over four trials in the 4-fold cross validation are shown in Figure 6. We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant. From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively. We also made statistics on the number of document pairs per query in the training data (for trial 1). The queries are clustered into different groups based on the the number of their associated document pairs. Figure 7 shows the distribution of the query groups. In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999. We can see that the numbers of document pairs really vary from query to query. Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group. The results are reported in Figure 8. We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost. Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k). The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs. For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5. We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5. The experiment was conducted for each trial. Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively. We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training. Finally, we tried to verify the correctness of Theorem 1. That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation. From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak. The result agrees well with Theorem 1. 5. CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures. It employs a boosting technique in ranking model learning. AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank. Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6. ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7. REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. In Advances in Neural Information Processing Systems 18, pages 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking SVM to document retrieval. In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang. Subset ranking using regression. In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Overview of the TREC 2003 web track. In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold. Boosting methods for regression. Mach. Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting. The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram. Learning rankings via convex hull separation. In Advances in Neural Information Processing Systems 18, pages 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin rank boundaries for ordinal regression. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam. Ohsumed: an interactive retrieval evaluation and new large test collection for research. In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR 23, pages 41-48, 2000. [16] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims. A support vector method for multivariate performance measures. In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum. Direct maximization of rank-based metrics for information retrieval. Technical report, CIIR, 2005. [20] R. Nallapati. Discriminative models for information retrieval. In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A study of relevance propagation for web search. In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A. Hull. The TREC-9 filtering track final report. In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Mach. Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y. Ma. Microsoft Research Asia at web track and terabyte track of TREC 2004. In TREC, 2004. [28] A. Trotman. Learning to rank. Inf. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang. Cost-sensitive learning of SVM for ranking. In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu, and Z. Chen. Exploiting the hierarchical structure for link analysis. In SIGIR 28, pages 186-193, 2005. [31] H. Yu. SVM selective sampling for ranking with application to data retrieval. In SIGKDD 11, pages 354-363, 2005. APPENDIX Here we give the proof of Theorem 1. P. Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)). According to the definition of αt, we know that eαt = φ(t) 1−φ(t) . ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2.",
    "original_translation": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2.",
    "original_sentences": [
        "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
        "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
        "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
        "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
        "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
        "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
        "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
        "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
        "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
        "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
        "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
        "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
        "When applied to document retrieval, learning to rank becomes a task as follows.",
        "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
        "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
        "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
        "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
        "Several methods for learning to rank have been developed and applied to document retrieval.",
        "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
        "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
        "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
        "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
        "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
        "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
        "AdaRank utilizes a linear combination of weak rankers as its model.",
        "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
        "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
        "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
        "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
        "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
        "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
        "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
        "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
        "Recently, direct optimization of performance measures in learning has become a hot research topic.",
        "Several methods for classification [17] and ranking [5, 19] have been proposed.",
        "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
        "The rest of the paper is organized as follows.",
        "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
        "Experimental results and discussions are given in Section 4.",
        "Section 5 concludes this paper and gives future work. 2.",
        "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
        "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
        "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
        "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
        "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
        "For example, Joachims [16] applies Ranking SVM to document retrieval.",
        "He utilizes click-through data to deduce training data for the model creation.",
        "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
        "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
        "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
        "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
        "They are learning to rank, boosting, and direct optimization of performance measures.",
        "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
        "Several approaches have been proposed to tackle the problem.",
        "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
        "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
        "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
        "For other approaches to learning to rank, refer to [2, 11, 31].",
        "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
        "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
        "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
        "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
        "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
        "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
        "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
        "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
        "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
        "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
        "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
        "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
        "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
        "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
        "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
        "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
        "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
        "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
        "The relevance scores are calculated with a ranking function (model).",
        "In learning (training), a number of queries and their corresponding retrieved documents are given.",
        "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
        "The relevance levels are represented as ranks (i.e., categories in a total order).",
        "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
        "Ideally the loss function is defined on the basis of the performance measure used in testing.",
        "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
        "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
        "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
        "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
        "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
        "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
        "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
        "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
        "We use π( j) to denote the position of item j (i.e., di j).",
        "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
        "Table 1: Notations and explanations.",
        "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
        "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
        "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
        "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
        "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
        "The first argument of E is the permutation π created using the ranking function f on di.",
        "The second argument is the list of ranks yi given by humans.",
        "E measures the agreement between π and yi.",
        "Table 1 gives a summary of notations described above.",
        "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
        "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
        "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
        "The algorithm is referred to as AdaRank and is shown in Figure 1.",
        "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
        "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
        "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
        "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
        "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
        "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
        "End For Output ranking model: f(x) = fT (x).",
        "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
        "Initially, AdaRank sets equal weights to the queries.",
        "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
        "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
        "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
        "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
        "Several methods for weak ranker construction can be considered.",
        "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
        "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
        "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
        "Intuitively, αt measures the importance of ht.",
        "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
        "In contrast, AdaRank tries to optimize a loss function based on queries.",
        "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
        "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
        "We next explain why this is the case.",
        "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
        "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
        "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
        "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
        "Several ways of computing coefficients αt and weak rankers ht may be considered.",
        "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
        "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
        "T 1.",
        "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
        "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
        "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
        "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
        "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
        "Notice that the major IR measures meet this requirement.",
        "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
        "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
        "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
        "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
        "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
        "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
        "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
        "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
        "The existing methods cannot focus on the training on the tops, as indicated in [4].",
        "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
        "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
        "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
        "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
        "First, the types of instances are different.",
        "AdaRank makes use of queries and their corresponding document lists as instances.",
        "The labels in training data are lists of ranks (relevance levels).",
        "AdaBoost makes use of feature vectors as instances.",
        "The labels in training data are simply +1 and −1.",
        "Second, the performance measures are different.",
        "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
        "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
        "Third, the ways of updating weights are also different.",
        "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
        "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
        "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
        "For AdaBoost they are equivalent (cf., [12] page 305).",
        "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
        "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
        "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
        "Note that features which are not selected in the training phase will have a weight of zero. 4.",
        "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
        "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
        "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
        "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
        "For AdaRank, the parameter T was determined automatically during each experiment.",
        "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
        "As the measure E, MAP and NDCG@5 were utilized.",
        "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
        "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
        "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
        "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
        "The data have been used in many experiments in IR, for example [4, 29].",
        "As features, we adopted those used in document retrieval [4].",
        "Table 2 shows the features.",
        "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
        "BM25 score itself is also a feature.",
        "Stop words were removed and stemming was conducted in the data.",
        "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
        "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
        "The results reported in Figure 2 are those averaged over four trials.",
        "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
        "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
        "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
        "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
        "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
        "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
        "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
        "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
        "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
        "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
        "Table 3 shows the statistics on the two datasets.",
        "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
        "We also conducted 4-fold cross-validation experiments.",
        "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
        "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
        "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
        "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
        "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
        "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
        "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
        "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
        "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
        "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
        "The number of relevant pages vary from query to query (from 1 to 86).",
        "We extracted 14 features from each query-document pair.",
        "Table 4 gives a list of the features.",
        "They are the outputs of some well-known algorithms (systems).",
        "These features are different from those in Table 2, because the task is different.",
        "Again, we conducted 4-fold cross-validation experiments.",
        "The results averaged over four trials are reported in Figure 5.",
        "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
        "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
        "Some of the improvements are not statistically significant.",
        "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
        "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
        "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
        "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
        "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
        "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
        "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
        "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
        "The queries are clustered into different groups based on the the number of their associated document pairs.",
        "Figure 7 shows the distribution of the query groups.",
        "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
        "We can see that the numbers of document pairs really vary from query to query.",
        "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
        "The results are reported in Figure 8.",
        "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
        "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
        "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
        "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
        "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
        "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
        "The experiment was conducted for each trial.",
        "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
        "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
        "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
        "Finally, we tried to verify the correctness of Theorem 1.",
        "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
        "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
        "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
        "The result agrees well with Theorem 1. 5.",
        "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
        "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
        "It employs a boosting technique in ranking model learning.",
        "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
        "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
        "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
        "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
        "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
        "Le.",
        "Learning to rank with nonsmooth cost functions.",
        "In Advances in Neural Information Processing Systems 18, pages 395-402.",
        "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
        "Learning to rank using gradient descent.",
        "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
        "Liu, H. Li, Y. Huang, and H.-W. Hon.",
        "Adapting ranking SVM to document retrieval.",
        "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
        "Subset ranking using regression.",
        "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
        "Overview of the TREC 2003 web track.",
        "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
        "Boosting methods for regression.",
        "Mach.",
        "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
        "Singer.",
        "An efficient boosting algorithm for combining preferences.",
        "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
        "A decision-theoretic generalization of on-line learning and an application to boosting.",
        "J. Comput.",
        "Syst.",
        "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
        "Additive logistic regression: A statistical view of boosting.",
        "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
        "Learning rankings via convex hull separation.",
        "In Advances in Neural Information Processing Systems 18, pages 395-402.",
        "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
        "The Elements of Statistical Learning.",
        "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
        "Large Margin rank boundaries for ordinal regression.",
        "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
        "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
        "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
        "A support vector method for multivariate performance measures.",
        "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
        "Document language models, query models, and risk minimization for information retrieval.",
        "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
        "Direct maximization of rank-based metrics for information retrieval.",
        "Technical report, CIIR, 2005. [20] R. Nallapati.",
        "Discriminative models for information retrieval.",
        "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
        "The pagerank citation ranking: Bringing order to the web.",
        "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
        "A language modeling approach to information retrieval.",
        "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
        "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
        "Ma.",
        "A study of relevance propagation for web search.",
        "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
        "Hull.",
        "The TREC-9 filtering track final report.",
        "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
        "Boosting the margin: A new explanation for the effectiveness of voting methods.",
        "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
        "Singer.",
        "Improved boosting algorithms using confidence-rated predictions.",
        "Mach.",
        "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
        "Ma.",
        "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
        "In TREC, 2004. [28] A. Trotman.",
        "Learning to rank.",
        "Inf.",
        "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
        "Cost-sensitive learning of SVM for ranking.",
        "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
        "Zeng, Y. Yu, and Z. Chen.",
        "Exploiting the hierarchical structure for link analysis.",
        "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
        "SVM selective sampling for ranking with application to data retrieval.",
        "In SIGKDD 11, pages 354-363, 2005.",
        "APPENDIX Here we give the proof of Theorem 1.",
        "P.",
        "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
        "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
        "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
        "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
    ],
    "translated_text_sentences": [
        "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No.",
        "En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos.",
        "En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos.",
        "La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada).",
        "Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento.",
        "Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento.",
        "Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias.",
        "Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento.",
        "Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación.",
        "Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada.",
        "Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1.",
        "INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático.",
        "Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera.",
        "En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos.",
        "En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado.",
        "En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15].",
        "Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento.",
        "Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos.",
        "Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM.",
        "Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost.",
        "Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas.",
        "Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias.",
        "En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos.",
        "Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank.",
        "AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo.",
        "En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador.",
        "Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR.",
        "Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento.",
        "AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación.",
        "Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov.",
        "Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1].",
        "A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil.",
        "Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación.",
        "Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente.",
        "Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19].",
        "AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente.",
        "El resto del documento está organizado de la siguiente manera.",
        "Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3.",
        "Los resultados experimentales y las discusiones se presentan en la Sección 4.",
        "La sección 5 concluye este artículo y presenta el trabajo futuro.",
        "TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada.",
        "Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1].",
        "Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar.",
        "A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante.",
        "Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores.",
        "Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos.",
        "Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo.",
        "Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI.",
        "Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados.",
        "Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos.",
        "El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual.",
        "Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento.",
        "Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones.",
        "Se han propuesto varios enfoques para abordar el problema.",
        "Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias.",
        "Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR.",
        "Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3].",
        "Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31].",
        "En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados).",
        "De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16].",
        "En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR.",
        "El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático.",
        "La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado.",
        "Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1).",
        "Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26].",
        "Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8].",
        "De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26].",
        "Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR.",
        "Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje.",
        "Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación.",
        "Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15].",
        "Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades.",
        "AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente.",
        "AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento.",
        "NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información.",
        "En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia.",
        "Las puntuaciones de relevancia se calculan con una función de clasificación (modelo).",
        "En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes.",
        "Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas.",
        "Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total).",
        "El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida.",
        "Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas.",
        "Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos.",
        "Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia.",
        "En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}.",
        "Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j.",
        "Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
        "Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1.",
        "El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes.",
        "Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma.",
        "Usamos π(j) para denotar la posición del elemento j (es decir, di j).",
        "El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas.",
        "Tabla 1: Notaciones y explicaciones.",
        "Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers.",
        "En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación.",
        "Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta.",
        "Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15].",
        "Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento.",
        "El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di.",
        "El segundo argumento es la lista de rangos yi proporcionada por los humanos.",
        "E mide la concordancia entre π y yi.",
        "La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente.",
        "A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG.",
        "Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j.",
        "Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR.",
        "El algoritmo se conoce como AdaRank y se muestra en la Figura 1.",
        "AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros.",
        "AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T).",
        "Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles.",
        "En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento.",
        "Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m.",
        "Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
        "Para el modelo de clasificación de salida: f(x) = fT (x).",
        "Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i).",
        "Inicialmente, AdaRank asigna pesos iguales a las consultas.",
        "En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento.",
        "Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles.",
        "En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt.",
        "La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
        "Varios métodos para la construcción de clasificadores débiles pueden ser considerados.",
        "Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt.",
        "En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6.",
        "Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil.",
        "Intuitivamente, αt mide la importancia de ht.",
        "Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos).",
        "Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas.",
        "Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR.",
        "Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1].",
        "A continuación explicamos por qué esto es así.",
        "Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación.",
        "Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar.",
        "En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ .",
        "Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
        "Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas.",
        "Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1.",
        "Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1.",
        "Teorema 1.",
        "El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice.",
        "El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso.",
        "Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente.",
        "Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet.",
        "Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1].",
        "Ten en cuenta que las principales medidas de IR cumplen con este requisito.",
        "En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16].",
        "Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes.",
        "La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento.",
        "La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8].",
        "En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes.",
        "Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos.",
        "Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente.",
        "En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos.",
        "Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4].",
        "Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema.",
        "Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4].",
        "AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento.",
        "En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost.",
        "Primero, los tipos de instancias son diferentes.",
        "AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias.",
        "Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia).",
        "AdaBoost utiliza vectores de características como instancias.",
        "Las etiquetas en los datos de entrenamiento son simplemente +1 y −1.",
        "Segundo, las medidas de rendimiento son diferentes.",
        "En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta.",
        "En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25].",
        "Tercero, las formas de actualizar los pesos también son diferentes.",
        "En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual.",
        "En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1.",
        "Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank.",
        "Para AdaBoost son equivalentes (cf., [12] página 305).",
        "Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos.",
        "En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
        "Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas.",
        "Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4.",
        "RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov.",
        "Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP.",
        "C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados.",
        "Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1).",
        "Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento.",
        "Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T).",
        "Se utilizaron las medidas E, MAP y NDCG@5.",
        "Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank.",
        "El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas.",
        "Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia.",
        "Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante).",
        "Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29].",
        "Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4].",
        "La tabla 2 muestra las características.",
        "Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características.",
        "El puntaje BM25 en sí mismo también es una característica.",
        "Se eliminaron las palabras vacías y se realizó el stemming en los datos.",
        "Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues.",
        "Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos.",
        "Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos.",
        "En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP.",
        "Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes.",
        "A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas.",
        "Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP.",
        "Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05).",
        "También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5.",
        "Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank.",
        "WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300).",
        "Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta).",
        "Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas.",
        "La Tabla 3 muestra las estadísticas de los dos conjuntos de datos.",
        "De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación.",
        "También realizamos experimentos de validación cruzada de 4 pliegues.",
        "Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente.",
        "A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP.",
        "Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP.",
        "Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05).",
        "Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web.",
        "El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002.",
        "Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov.",
        "Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos.",
        "Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003.",
        "Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante.",
        "El número de páginas relevantes varía de una consulta a otra (de 1 a 86).",
        "Extrajimos 14 características de cada par de consulta-documento.",
        "La tabla 4 proporciona una lista de las características.",
        "Son los resultados de algunos algoritmos (sistemas) conocidos.",
        "Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta.",
        "Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues.",
        "Los resultados promediados de cuatro pruebas se informan en la Figura 5.",
        "De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas.",
        "Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost.",
        "Algunas de las mejoras no son estadísticamente significativas.",
        "Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos.",
        "Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost.",
        "Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba.",
        "Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6.",
        "Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante.",
        "A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes.",
        "Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente.",
        "También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1).",
        "Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas.",
        "La Figura 7 muestra la distribución de los grupos de consulta.",
        "En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999.",
        "Podemos ver que los números de pares de documentos realmente varían de una consulta a otra.",
        "A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas.",
        "Los resultados se informan en la Figura 8.",
        "Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost.",
        "Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k).",
        "Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos.",
        "Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5.",
        "Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento.",
        "Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5.",
        "El experimento se llevó a cabo para cada prueba.",
        "La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente.",
        "Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5.",
        "Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento.",
        "Finalmente, intentamos verificar la corrección del Teorema 1.",
        "Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1.",
        "Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada.",
        "Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo.",
        "El resultado concuerda bien con el Teorema 1.5.",
        "CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank.",
        "A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento.",
        "Emplea una técnica de aumento en el aprendizaje del modelo de clasificación.",
        "AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación.",
        "Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank.",
        "El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento.",
        "AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7.",
        "REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto.",
        "Recuperación de información moderna.",
        "Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q.",
        "Lo.",
        "Aprendizaje para clasificar con funciones de costo no suaves.",
        "En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402.",
        "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender.",
        "Aprendiendo a clasificar utilizando descenso de gradiente.",
        "En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
        "Liu, H. Li, Y. Huang y H.-W. Hon.",
        "Adaptando el SVM de clasificación para la recuperación de documentos.",
        "En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang.",
        "Clasificación de subconjuntos utilizando regresión.",
        "En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu.",
        "Resumen de la pista web TREC 2003.",
        "En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold.",
        "Métodos de aumento para regresión.",
        "I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish?",
        "Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y.",
        "Cantante.",
        "Un algoritmo de refuerzo eficiente para combinar preferencias.",
        "Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire.",
        "Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting.",
        "J. Comput.",
        "This is not a complete sentence. Please provide more context or a complete sentence to be translated.",
        "Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani.",
        "Regresión logística aditiva: Una perspectiva estadística del boosting.",
        "Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram.",
        "Aprendizaje de clasificaciones mediante separación de envolvente convexa.",
        "En Advances in Neural Information Processing Systems 18, páginas 395-402.",
        "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman.",
        "Los Elementos del Aprendizaje Estadístico.",
        "Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer.",
        "Límites de rango de gran margen para regresión ordinal.",
        "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam.",
        "Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación.",
        "En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen.",
        "Métodos de evaluación IR para recuperar documentos altamente relevantes.",
        "En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims.",
        "Optimización de motores de búsqueda utilizando datos de clics.",
        "En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims.",
        "Un método de vector de soporte para medidas de rendimiento multivariadas.",
        "En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai.",
        "Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información.",
        "En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum.",
        "Maximización directa de métricas basadas en rangos para la recuperación de información.",
        "Informe técnico, CIIR, 2005. [20] R. Nallapati.",
        "Modelos discriminativos para la recuperación de información.",
        "En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd.",
        "El ranking de citas de pagerank: Trayendo orden a la web.",
        "Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft.",
        "Un enfoque de modelado del lenguaje para la recuperación de información.",
        "En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y.",
        "Liu, X.-D. Zhang, Z. Chen, y W.-Y.",
        "This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish.",
        "Un estudio de propagación de relevancia para la búsqueda en la web.",
        "En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A.",
        "Casco.",
        "El informe final de la pista de filtrado TREC-9.",
        "En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee.",
        "Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación.",
        "En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y.",
        "Cantante.",
        "Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza.",
        "I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish?",
        "Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y.",
        "This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish.",
        "Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004.",
        "En TREC, 2004. [28] A. Trotman.",
        "Aprendizaje para clasificar.",
        "I'm sorry, but I need a complete sentence to provide an accurate translation.",
        "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang.",
        "Aprendizaje sensible al costo de SVM para clasificación.",
        "En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
        "Zeng, Y. Yu y Z. Chen.",
        "Explotando la estructura jerárquica para el análisis de enlaces.",
        "En SIGIR 28, páginas 186-193, 2005. [31] H. Yu.",
        "Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos.",
        "En SIGKDD 11, páginas 354-363, 2005.",
        "APÉNDICE Aquí presentamos la prueba del Teorema 1.",
        "PRUEBA.",
        "Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)).",
        "Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t).",
        "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
        "Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
    ],
    "error_count": 3,
    "keys": {
        "ranking model": {
            "translated_key": "modelo de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a <br>ranking model</br> that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a <br>ranking model</br> is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained <br>ranking model</br>.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the <br>ranking model</br> gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model</br> tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the <br>ranking model</br> (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a <br>ranking model</br> using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to <br>ranking model</br> construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ <br>ranking model</br> π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a <br>ranking model</br> f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output <br>ranking model</br>: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A <br>ranking model</br> ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our <br>ranking model</br>: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the <br>ranking model</br> created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in <br>ranking model</br> learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "Ideally a learning algorithm would train a <br>ranking model</br> that could directly optimize the performance measures with respect to the training data.",
                "In training, a <br>ranking model</br> is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained <br>ranking model</br>.",
                "As the number of features in the <br>ranking model</br> gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model</br> tuning."
            ],
            "translated_annotated_samples": [
                "Idealmente, un algoritmo de aprendizaje entrenaría un <br>modelo de clasificación</br> que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento.",
                "En el entrenamiento, se construye un <br>modelo de clasificación</br> con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos.",
                "En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el <br>modelo de ranking</br> entrenado.",
                "A medida que el número de características en el <br>modelo de clasificación</br> aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil.",
                "Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un <br>modelo de clasificación</br> que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un <br>modelo de clasificación</br> con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el <br>modelo de ranking</br> entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el <br>modelo de clasificación</br> aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. ",
            "candidates": [],
            "error": [
                [
                    "modelo de clasificación",
                    "modelo de clasificación",
                    "modelo de ranking",
                    "modelo de clasificación"
                ]
            ]
        },
        "novel learning algorithm": {
            "translated_key": "algoritmo de aprendizaje novedoso",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a <br>novel learning algorithm</br> within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "To deal with the problem, we propose a <br>novel learning algorithm</br> within the framework of boosting, which can minimize a loss function directly defined on the performance measures."
            ],
            "translated_annotated_samples": [
                "Para abordar el problema, proponemos un <br>algoritmo de aprendizaje novedoso</br> dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un <br>algoritmo de aprendizaje novedoso</br> dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "re-weighted training datum": {
            "translated_key": "dato de entrenamiento reponderado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "machine learning": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and <br>machine learning</br>.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a <br>machine learning</br> method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a <br>machine learning</br> method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 <br>machine learning</br> There are three topics in <br>machine learning</br> which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of <br>machine learning</br> algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of <br>machine learning</br> Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and <br>machine learning</br>.",
                "From the viewpoint of IR, AdaRank can be viewed as a <br>machine learning</br> method for ranking model tuning.",
                "AdaRank can be viewed as a <br>machine learning</br> method for direct optimization of performance measures, based on a different approach.",
                "The method is referred to as RankNet. 2.2 <br>machine learning</br> There are three topics in <br>machine learning</br> which are related to our current work.",
                "Boosting is a general technique for improving the accuracies of <br>machine learning</br> algorithms."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y <br>aprendizaje automático</br>.",
                "Desde el punto de vista de IR, AdaRank puede ser visto como un método de <br>aprendizaje automático</br> para ajuste de modelos de clasificación.",
                "AdaRank se puede ver como un método de <br>aprendizaje automático</br> para la optimización directa de medidas de rendimiento, basado en un enfoque diferente.",
                "El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el <br>aprendizaje automático</br> que están relacionados con nuestro trabajo actual.",
                "El boosting es una técnica general para mejorar las precisiones de los algoritmos de <br>aprendizaje automático</br>."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y <br>aprendizaje automático</br>. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de <br>aprendizaje automático</br> para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de <br>aprendizaje automático</br> para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el <br>aprendizaje automático</br> que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de <br>aprendizaje automático</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "trained ranking model": {
            "translated_key": "modelo de ranking entrenado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the <br>trained ranking model</br>.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the <br>trained ranking model</br>."
            ],
            "translated_annotated_samples": [
                "En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el <br>modelo de ranking entrenado</br>."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el <br>modelo de ranking entrenado</br>. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "support vector machine": {
            "translated_key": "máquina de vectores de soporte",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "rankboost": {
            "translated_key": "RankBoost",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as <br>rankboost</br>.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], <br>rankboost</br> [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, <br>rankboost</br>, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, <br>rankboost</br>, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of <br>rankboost</br>, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and <br>rankboost</br> [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and <br>rankboost</br> in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and <br>rankboost</br> in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br> in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and <br>rankboost</br> in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br> on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br>.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and <br>rankboost</br>.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, <br>rankboost</br>, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and <br>rankboost</br> in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than <br>rankboost</br>.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than <br>rankboost</br> for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group <br>rankboost</br> AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as <br>rankboost</br>.",
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, Ranking SVM y <br>RankBoost</br> entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias.",
                "Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost.",
                "Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como <br>RankBoost</br>.",
                "Por ejemplo, Ranking SVM y <br>RankBoost</br> entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias.",
                "Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y <br>RankBoost</br>, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y <br>RankBoost</br> entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como <br>RankBoost</br>. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y <br>RankBoost</br> entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y <br>RankBoost</br>, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "new learning algorithm": {
            "translated_key": "algoritmo de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a <br>new learning algorithm</br> that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "In this paper, we aim to develop a <br>new learning algorithm</br> that can directly optimize any performance measure used in document retrieval."
            ],
            "translated_annotated_samples": [
                "En este artículo, nuestro objetivo es desarrollar un nuevo <br>algoritmo de aprendizaje</br> que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo <br>algoritmo de aprendizaje</br> que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "document retrieval": {
            "translated_key": "recuperación de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for <br>document retrieval</br>.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to <br>document retrieval</br>, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In <br>document retrieval</br>, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to <br>document retrieval</br>.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for <br>document retrieval</br> [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in <br>document retrieval</br>.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for <br>document retrieval</br> is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to <br>document retrieval</br>.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to <br>document retrieval</br> by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in <br>document retrieval</br>.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for <br>document retrieval</br>.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for <br>document retrieval</br>.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in <br>document retrieval</br> [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in <br>document retrieval</br>, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to <br>document retrieval</br>.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for <br>document retrieval</br>.",
                "When applied to <br>document retrieval</br>, learning to rank becomes a task as follows.",
                "In <br>document retrieval</br>, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Several methods for learning to rank have been developed and applied to <br>document retrieval</br>.",
                "All the existing methods used for <br>document retrieval</br> [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures."
            ],
            "translated_annotated_samples": [
                "En este documento abordamos el tema del aprendizaje para clasificar en la <br>recuperación de documentos</br>.",
                "Cuando se aplica a la <br>recuperación de documentos</br>, aprender a clasificar se convierte en una tarea de la siguiente manera.",
                "En la <br>recuperación de documentos</br>, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15].",
                "Se han desarrollado y aplicado varios métodos para aprender a clasificar en la <br>recuperación de documentos</br>.",
                "Todos los métodos existentes utilizados para la <br>recuperación de documentos</br> [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la <br>recuperación de documentos</br>. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la <br>recuperación de documentos</br>, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la <br>recuperación de documentos</br>, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la <br>recuperación de documentos</br>. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la <br>recuperación de documentos</br> [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "weak ranker": {
            "translated_key": "clasificador débil",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a <br>weak ranker</br>, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth <br>weak ranker</br> E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a <br>weak ranker</br>, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a <br>weak ranker</br> ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create <br>weak ranker</br> ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a <br>weak ranker</br> that can work on the ranking of those hard queries.",
                "At each round, a <br>weak ranker</br> ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a <br>weak ranker</br> is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for <br>weak ranker</br> construction can be considered.",
                "For example, a <br>weak ranker</br> can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a <br>weak ranker</br> ht is built, AdaRank chooses a weight αt > 0 for the <br>weak ranker</br>.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of <br>weak ranker</br> We consider an efficient implementation for <br>weak ranker</br> construction, which is also used in our experiments.",
                "In the implementation, as <br>weak ranker</br> we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "In learning, it repeats the process of re-weighting the training sample, creating a <br>weak ranker</br>, and calculating a weight for the ranker.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth <br>weak ranker</br> E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a <br>weak ranker</br>, αt is its weight, and T is the number of weak rankers.",
                "AdaRank runs T rounds and at each round it creates a <br>weak ranker</br> ht(t = 1, · · · , T).",
                "For t = 1, · · · , T • Create <br>weak ranker</br> ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "As a result, the learning at the next round will be focused on the creation of a <br>weak ranker</br> that can work on the ranking of those hard queries."
            ],
            "translated_annotated_samples": [
                "En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un <br>clasificador débil</br> y calcular un peso para el clasificador.",
                "Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo <br>weak ranker</br> E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de <br>weak ranker</br>s: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers.",
                "AdaRank realiza T rondas y en cada ronda crea un <br>clasificador débil</br> ht (t = 1, · · · , T).",
                "Para t = 1, · · · , T • Crear un <br>clasificador débil</br> ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un <br>clasificador débil</br> que pueda trabajar en la clasificación de esas consultas difíciles."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un <br>clasificador débil</br> y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo <br>weak ranker</br> E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de <br>weak ranker</br>s: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un <br>clasificador débil</br> ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un <br>clasificador débil</br> ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un <br>clasificador débil</br> que pueda trabajar en la clasificación de esas consultas difíciles. ",
            "candidates": [],
            "error": [
                [
                    "clasificador débil",
                    "weak ranker",
                    "weak ranker",
                    "clasificador débil",
                    "clasificador débil",
                    "clasificador débil"
                ]
            ]
        },
        "training process": {
            "translated_key": "proceso de entrenamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the <br>training process</br> of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the <br>training process</br>.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "We prove that the <br>training process</br> of AdaRank is exactly that of enhancing the performance measure used.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the <br>training process</br>."
            ],
            "translated_annotated_samples": [
                "Demostramos que el <br>proceso de entrenamiento</br> de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada.",
                "Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el <br>proceso de entrenamiento</br>."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el <br>proceso de entrenamiento</br> de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el <br>proceso de entrenamiento</br>. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ranking model tuning": {
            "translated_key": "ajuste de modelos de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model tuning</br>.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model tuning</br>."
            ],
            "translated_annotated_samples": [
                "Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para <br>ajuste de modelos de clasificación</br>."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para <br>ajuste de modelos de clasificación</br>. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, potenciar y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del boosting es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. Freund y Schapire han propuesto el primer algoritmo de boosting conocido llamado AdaBoost (Adaptive Boosting) [9], el cual está diseñado para clasificación binaria (predicción 0-1). Más tarde, Schapire y Singer han introducido una versión generalizada de AdaBoost en la que los aprendices débiles pueden proporcionar puntuaciones de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han realizado extensiones para abordar los problemas de clasificación multi-clase [10, 26], regresión [7] y ranking [8]. De hecho, AdaBoost es un algoritmo que ingeniosamente construye un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este artículo puede ser visto como un método de impulso desarrollado para la clasificación, especialmente para la clasificación en IR. Recientemente, varios autores han propuesto llevar a cabo la optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente medidas de rendimiento multivariadas no lineales como la medida F1 para clasificación. Cossock & Zhang [5] encuentran una forma de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al. [19] también proponen un método para maximizar directamente métricas basadas en rangos para la clasificación en base al aprendizaje de variedades. AdaRank es otro algoritmo que intenta optimizar directamente medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. AdaRank es único en que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de aumento. NUESTRO MÉTODO: ADARANK 3.1 Marco General Primero describimos el marco general del aprendizaje para clasificar la relevancia de documentos en la recuperación de información. En la recuperación (prueba), dado un query, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Las puntuaciones de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (entrenamiento), se proporciona un número de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en términos de minimización de una función de pérdida. Idealmente, la función de pérdida se define en base a la medida de rendimiento utilizada en las pruebas. Supongamos que Y = {r1, r2, · · · , r } es un conjunto de rangos, donde · · · denota el número de rangos. Existe un orden total entre los rangos r r −1 · · · r1, donde · denota una relación de preferencia. En el entrenamiento, se proporciona un conjunto de consultas Q = {q1, q2, · · · , qm}. Cada consulta qi está asociada con una lista de documentos recuperados di = {di1, di2, · · · , di,n(qi)} y una lista de etiquetas yi = {yi1, yi2, · · · , yi,n(qi)}, donde n(qi) denota los tamaños de las listas di y yi, dij denota el j-ésimo documento en di, y yij ∈ Y denota la clasificación del documento di j. Se crea un vector de características xij = Ψ(qi, di j) ∈ X a partir de cada par consulta-documento (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Por lo tanto, el conjunto de entrenamiento puede ser representado como S = {(qi, di, yi)}m i=1. El objetivo del aprendizaje es crear una función de clasificación f: X → Y, de modo que para cada consulta los elementos en su lista de documentos correspondiente puedan ser asignados puntajes de relevancia utilizando la función y luego ser clasificados según los puntajes. Específicamente, creamos una permutación de enteros π(qi, di, f) para la consulta qi, la lista correspondiente de documentos di y la función de clasificación f. Si di = {di1, di2, · · · , di,n(qi)} está identificado por la lista de enteros {1, 2, · · · , n(qi)}, entonces la permutación π(qi, di, f) se define como una biyección de {1, 2, · · · , n(qi)} en sí misma. Usamos π(j) para denotar la posición del elemento j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa la discrepancia entre la permutación π(qi, di, f) y la lista de rangos yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de las notaciones qi ∈ Q con la consulta di = {di1, di2, · · · , di,n(qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · · , r } Rango de di j con respecto a qi yi = {yi1, yi2, · · · , yi,n(qi)} Lista de rangos para qi S = {(qi, di, yi)}m i=1 Conjunto de entrenamiento xij = Ψ(qi, dij) ∈ X Vector de características para (qi, di j) f(xij) ∈ Modelo de ranking π(qi, di, f) Permutación para qi, di y f ht(xi j) ∈ t-ésimo weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el artículo, definimos el modelo de rango como una combinación lineal de weak rankers: f(x) = T t=1 αtht(x), donde ht(x) es un weak ranker, αt es su peso y T es el número de weak rankers. En la recuperación de información, se utilizan medidas de rendimiento basadas en consultas para evaluar la eficacia de una función de clasificación. Por medida basada en consultas, nos referimos a una medida definida sobre una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL) y Precisión@n [1, 15]. Utilizamos una función general E(π(qi, di, f), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada utilizando la función de clasificación f en di. El segundo argumento es la lista de rangos yi proporcionada por los humanos. E mide la concordancia entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) donde yij toma los valores 1 y 0, representando relevante o irrelevante y Pi( j) se define como la precisión en la posición de dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) donde πi( j) denota la posición de di j. Dada una consulta qi, la lista de rangos yi y una permutación πi en di, NDCG en la posición m para qi se define como: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) donde yij toma rangos como valores y ni es una constante de normalización. ni se elige de manera que una clasificación perfecta π∗ i tenga una puntuación NDCG en la posición m de 1. 3.2 Algoritmo Inspirado en el algoritmo AdaBoost para clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en medidas de rendimiento de IR. El algoritmo se conoce como AdaRank y se muestra en la Figura 1. AdaRank toma un conjunto de entrenamiento S = {(qi, di, yi)}m i=1 como entrada y toma la función de medida de rendimiento E y el número de iteraciones T como parámetros. AdaRank realiza T rondas y en cada ronda crea un clasificador débil ht (t = 1, · · · , T). Finalmente, genera un modelo de clasificación f combinando linealmente los clasificadores débiles. En cada ronda, AdaRank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de pesos Entrada: S = {(qi, di, yi)}m i=1, y parámetros E y T. Inicializar P1(i) = 1/m. Para t = 1, · · · , T • Crear un clasificador débil ht con distribución ponderada Pt en los datos de entrenamiento S. • Elegir αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Crear ft ft(x) = t k=1 αkhk(x). • Actualizar Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . Para el modelo de clasificación de salida: f(x) = fT (x). Figura 1: El algoritmo AdaRank en la ronda t como Pt y el peso en la i-ésima consulta de entrenamiento qi en la ronda t como Pt(i). Inicialmente, AdaRank asigna pesos iguales a las consultas. En cada ronda, aumenta los pesos de aquellas consultas que no están bien clasificadas por ft, el modelo creado hasta el momento. Como resultado, el aprendizaje en la siguiente ronda se centrará en la creación de un clasificador débil que pueda trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un clasificador débil ht basado en los datos de entrenamiento con una distribución de pesos Pt. La bondad de un clasificador débil se mide por la medida de rendimiento E ponderada por Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Varios métodos para la construcción de clasificadores débiles pueden ser considerados. Por ejemplo, se puede crear un clasificador débil utilizando un subconjunto de consultas (junto con su lista de documentos y etiquetas) muestreado de acuerdo con la distribución Pt. En este artículo, utilizamos características individuales como clasificadores débiles, como se explicará en la Sección 3.6. Una vez que se construye un clasificador débil ht, AdaRank elige un peso αt > 0 para el clasificador débil. Intuitivamente, αt mide la importancia de ht. Un modelo de clasificación ft se crea en cada ronda combinando linealmente los clasificadores débiles construidos hasta el momento h1, · · · , ht con pesos α1, · · · , αt. Luego, ft se utiliza para actualizar la distribución Pt+1. 3.3 Análisis Teórico Los algoritmos de aprendizaje existentes para clasificación intentan minimizar una función de pérdida basada en pares de instancias (pares de documentos). Por el contrario, AdaRank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en AdaRank se define en base a medidas generales de rendimiento en IR. Las medidas pueden ser MAP, NDCG, WTA, MRR, u otras medidas cuyo rango esté dentro de [−1, +1]. A continuación explicamos por qué esto es así. Idealmente queremos maximizar la precisión de clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i=1 E(π(qi, di, f), yi), (4) donde F es el conjunto de posibles funciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i=1 (1 − E(π(qi, di, f), yi)). Es difícil optimizar directamente la pérdida, ya que E es una función no continua y, por lo tanto, puede ser difícil de manejar. En cambio, intentamos minimizar una cota superior de la pérdida en (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) porque e−x ≥ 1 − x se cumple para cualquier x ∈ . Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro modelo de clasificación: f(x) = Σ t=1 αtht(x). La minimización en (6) resulta ser min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, donde H es el conjunto de posibles clasificadores débiles, αt es un peso positivo, y ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Varias formas de calcular los coeficientes αt y los clasificadores débiles ht pueden ser consideradas. Siguiendo la idea de AdaBoost, en AdaRank tomamos el enfoque de modelado aditivo en etapas hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para AdaRank en los datos de entrenamiento, como se presenta en el Teorema 1. Teorema 1. El siguiente límite se cumple en la precisión de clasificación del algoritmo AdaRank en los datos de entrenamiento: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, donde ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, y δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), para todo i = 1, 2, · · · , m y t = 1, 2, · · · , T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla e−δt min 1 − ϕ(t)2 < 1. 3.4 Ventajas AdaRank es un método simple pero poderoso. Más importante aún, es un método que puede ser justificado desde el punto de vista teórico, como se discutió anteriormente. Además, AdaRank tiene varias ventajas adicionales en comparación con los métodos existentes de aprendizaje para clasificar, como Ranking SVM, RankBoost y RankNet. Primero, AdaRank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en consultas y esté en el rango de [−1, +1]. Ten en cuenta que las principales medidas de IR cumplen con este requisito. En contraste, los métodos existentes solo minimizan funciones de pérdida que están débilmente relacionadas con las medidas de IR [16]. Segundo, el proceso de aprendizaje de AdaRank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad temporal de AdaRank es del orden O((k+T)·m·n log n), donde k denota el número de características, T el número de rondas, m el número de consultas en los datos de entrenamiento, y n es el número máximo de documentos para las consultas en los datos de entrenamiento. La complejidad temporal de RankBoost, por ejemplo, es del orden O(T · m · n2) [8]. En tercer lugar, AdaRank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en AdaRank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, AdaRank no tiene las siguientes deficiencias que afectan a los métodos existentes. (a) Los métodos existentes tienen que hacer una suposición fuerte de que los pares de documentos de la misma consulta están distribuidos de forma independiente. En realidad, esto claramente no es el caso y este problema no existe para AdaRank. (b) Clasificar los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en el entrenamiento en las partes superiores, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, AdaRank puede enfocarse naturalmente en entrenar en la parte superior de las listas de documentos, ya que las medidas de rendimiento utilizadas favorecen las clasificaciones en las que los documentos relevantes se encuentran en la parte superior. En los métodos existentes, el número de pares de documentos varía de una consulta a otra, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. AdaRank no tiene esta desventaja, ya que trata las consultas en lugar de los pares de documentos como unidades básicas en el aprendizaje. 3.5 Diferencias con AdaBoost AdaRank es un algoritmo de aumento. En ese sentido, es similar a AdaBoost, pero también tiene varias diferencias llamativas con AdaBoost. Primero, los tipos de instancias son diferentes. AdaRank utiliza consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de entrenamiento son listas de rangos (niveles de relevancia). AdaBoost utiliza vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. Segundo, las medidas de rendimiento son diferentes. En AdaRank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de clasificación de una consulta. En AdaBoost, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En AdaBoost, la distribución de pesos en las instancias de entrenamiento se calcula según la distribución actual y el rendimiento del aprendiz débil actual. En AdaRank, en cambio, se calcula según el rendimiento del modelo de clasificación creado hasta el momento, como se muestra en la Figura 1. Ten en cuenta que AdaBoost también puede adoptar el método de actualización de pesos utilizado en AdaRank. Para AdaBoost son equivalentes (cf., [12] página 305). Sin embargo, esto no es cierto para AdaRank. 3.6 Construcción de un Clasificador Débil Consideramos una implementación eficiente para la construcción de un clasificador débil, la cual también se utiliza en nuestros experimentos. En la implementación, como clasificador débil elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta en la selección repetida de características y la combinación lineal de las características seleccionadas. Ten en cuenta que las características que no son seleccionadas en la fase de entrenamiento tendrán un peso de cero. 4. RESULTADOS EXPERIMENTALES Realizamos experimentos para probar el rendimiento de AdaRank utilizando cuatro conjuntos de datos de referencia: OHSUMED, WSJ, AP y .Gov. Tabla 2: Características utilizadas en los experimentos en los conjuntos de datos OHSUMED, WSJ y AP. C(w, d) representa la frecuencia de la palabra w en el documento d; C representa la colección completa; n denota el número de términos en la consulta; | · | denota la función de tamaño; e id f(·) denota la frecuencia inversa del documento. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(puntuación de BM25) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figura 2: Precisión de clasificación en los datos de OHSUMED. 4.1 Configuración del experimento SVM de clasificación [13, 16] y RankBoost [8] fueron seleccionados como baselines en los experimentos, porque son los métodos de aprendizaje de clasificación más avanzados. Además, se utilizó BM25 [24] como referencia, representando el método de RI de última generación (de hecho, utilizamos la herramienta Lemur1). Para AdaRank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Se utilizaron las medidas E, MAP y NDCG@5. Los resultados para AdaRank utilizando MAP y NDCG@5 como medidas en el entrenamiento se representan como AdaRank.MAP y AdaRank.NDCG, respectivamente. 4.2 Experimento con Datos de OHSUMED En este experimento, hicimos uso del conjunto de datos de OHSUMED [14] para probar el rendimiento de AdaRank. El conjunto de datos OHSUMED consta de 348,566 documentos y 106 consultas. Hay un total de 16,140 pares de consulta-documento sobre los cuales se realizan juicios de relevancia. Las valoraciones de relevancia son ya sea d (definitivamente relevante), p (posiblemente relevante) o n (no relevante). Los datos han sido utilizados en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos aquellas utilizadas en la recuperación de documentos [4]. La tabla 2 muestra las características. Por ejemplo, tf (frecuencia del término), idf (frecuencia inversa del documento), dl (longitud del documento) y combinaciones de ellos se definen como características. El puntaje BM25 en sí mismo también es una característica. Se eliminaron las palabras vacías y se realizó el stemming en los datos. Dividimos aleatoriamente las consultas en cuatro subconjuntos iguales y realizamos experimentos de validación cruzada de 4 pliegues. Ajustamos los parámetros para BM25 durante uno de los ensayos y los aplicamos a los otros ensayos. Los resultados reportados en la Figura 2 son aquellos promediados a lo largo de cuatro ensayos. En el cálculo de MAP, definimos la clasificación d como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas sobre los conjuntos de datos de WSJ y AP. Conjunto de datos # consultas # documentos recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 3: Precisión de ranking en el conjunto de datos WSJ. Los otros dos rangos se consideran irrelevantes. A partir de la Figura 2, vemos que tanto AdaRank.MAP como AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas. Realizamos pruebas significativas (prueba t) sobre las mejoras de AdaRank.MAP sobre BM25, Ranking SVM y RankBoost en términos de MAP. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p < 0.05). También realizamos una prueba t sobre las mejoras de AdaRank.NDCG en comparación con BM25, Ranking SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas. 4.3 Experimento con datos de WSJ y AP En este experimento, hicimos uso de los conjuntos de datos de WSJ y AP de la pista de recuperación ad-hoc de TREC, para probar el rendimiento de AdaRank. WSJ contiene 74,520 artículos del Wall Street Journal de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. Se seleccionan 200 consultas de los temas de TREC (No.101 a No.300). Cada consulta tiene asociado un número de documentos que están etiquetados como relevantes o irrelevantes (para la consulta). Siguiendo la práctica en [28], las consultas que tenían menos de 10 documentos relevantes fueron descartadas. La Tabla 3 muestra las estadísticas de los dos conjuntos de datos. De la misma manera que en la sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de validación cruzada de 4 pliegues. Los resultados reportados en la Figura 3 y 4 son aquellos promediados sobre cuatro pruebas en los conjuntos de datos de WSJ y AP, respectivamente. A partir de las Figuras 3 y 4, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a BM25, Ranking SVM y RankBoost en cuanto a todas las medidas tanto en WSJ como en AP. Realizamos pruebas t en las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p < 0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones de NDCG son bastante altas (1-2 puntos). 4.4 Experimento con datos .Gov En este experimento, utilizamos aún más los datos .Gov de TREC para probar el rendimiento de AdaRank en la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y ha sido utilizado en la TREC Web Track desde 2002. Hay un total de 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 4: Precisión de clasificación en el conjunto de datos AP. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figura 5: Precisión de clasificación en el conjunto de datos .Gov. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .Gov. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Propagación de Relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación de temas en la pista web de TREC 2003. Las verdades fundamentales para las consultas son proporcionadas por el comité TREC con juicios binarios: relevante o irrelevante. El número de páginas relevantes varía de una consulta a otra (de 1 a 86). Extrajimos 14 características de cada par de consulta-documento. La tabla 4 proporciona una lista de las características. Son los resultados de algunos algoritmos (sistemas) conocidos. Estas características son diferentes de las que se encuentran en la Tabla 2, porque la tarea es distinta. Nuevamente, realizamos experimentos de validación cruzada de 4 pliegues. Los resultados promediados de cuatro pruebas se informan en la Figura 5. De los resultados, podemos ver que AdaRank.MAP y AdaRank.NDCG superan a todos los baselines en cuanto a todas las medidas. Realizamos pruebas t sobre las mejoras de AdaRank.MAP y AdaRank.NDCG sobre BM25, Ranking SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeño. 4.5 Discusiones Investigamos las razones por las que AdaRank supera a los métodos de referencia, utilizando los resultados del conjunto de datos OHSUMED como ejemplos. Primero, examinamos la razón por la que AdaRank tiene un mejor rendimiento que Ranking SVM y RankBoost. Específicamente comparamos las tasas de error entre diferentes pares de clasificación realizados por Ranking SVM, RankBoost, AdaRank.MAP y AdaRank.NDCG en los datos de prueba. Los resultados promediados de cuatro pruebas en la validación cruzada de 4 pliegues se muestran en la Figura 6. Utilizamos d-n para representar los pares entre definitivamente relevante y no relevante, d-p los pares entre definitivamente relevante y parcialmente relevante, y p-n los pares entre parcialmente relevante y no relevante. A partir de la Figura 6, podemos ver que AdaRank.MAP y AdaRank.NDCG cometen menos errores para d-n y d-p, que están relacionados con los primeros puestos de las clasificaciones y son importantes. Esto se debe a que AdaRank.MAP y AdaRank.NDCG pueden enfocarse naturalmente en el entrenamiento en los primeros lugares optimizando MAP y NDCG@5, respectivamente. También realizamos estadísticas sobre el número de pares de documentos por consulta en los datos de entrenamiento (para la prueba 1). Las consultas se agrupan en diferentes grupos según el número de pares de documentos asociados a ellas. La Figura 7 muestra la distribución de los grupos de consulta. En la figura, por ejemplo, 0-1k es el grupo de consultas cuyo número de pares de documentos está entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de una consulta a otra. A continuación evaluamos las precisiones de AdaRank.MAP y RankBoost en términos de MAP para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Encontramos que el MAP promedio de AdaRank.MAP sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que AdaRank.MAP tiene un rendimiento particularmente mejor que RankBoost para consultas con un pequeño número de pares de documentos (por ejemplo, 0-1k, 1k-2k y 2k-3k). Los resultados indican que AdaRank.MAP puede evitar de manera efectiva la creación de un modelo sesgado hacia consultas con más pares de documentos. Para AdaRank.NDCG, se pueden observar resultados similares. 0.2 0.3 0.4 0.5 MAP grupo de consultas RankBoost AdaRank.MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas. 0.30 0.31 0.32 0.33 0.34 prueba 1 prueba 2 prueba 3 prueba 4 MAP AdaRank.MAP AdaRank.NDCG Figura 9: MAP en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. Llevamos a cabo un experimento adicional para ver si AdaRank tiene la capacidad de mejorar la precisión de clasificación en términos de una medida utilizando dicha medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación utilizando AdaRank.MAP y AdaRank.NDCG y evaluamos sus precisión en el conjunto de datos de entrenamiento en términos de MAP y NDCG@5. El experimento se llevó a cabo para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, AdaRank.MAP entrenado con MAP tiene un mejor rendimiento en términos de MAP, mientras que AdaRank.NDCG entrenado con NDCG@5 tiene un mejor rendimiento en términos de NDCG@5. Los resultados indican que AdaRank puede mejorar efectivamente el rendimiento de clasificación en términos de una medida al utilizar la medida en el entrenamiento. Finalmente, intentamos verificar la corrección del Teorema 1. Es decir, la precisión del ranking en términos de la medida de rendimiento puede mejorarse continuamente, siempre y cuando se cumpla que e−δt min 1 − ϕ(t)2 < 1. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de AdaRank.MAP en términos de MAP durante la fase de entrenamiento en una prueba de la validación cruzada. Desde la figura, podemos ver que la precisión de clasificación de AdaRank.MAP mejora constantemente a medida que avanza el entrenamiento, hasta que alcanza su punto máximo. El resultado concuerda bien con el Teorema 1.5. CONCLUSIÓN Y TRABAJO FUTURO En este artículo hemos propuesto un algoritmo novedoso para aprender modelos de clasificación en la recuperación de documentos, denominado AdaRank. A diferencia de los métodos existentes, AdaRank optimiza una función de pérdida que está directamente definida en las medidas de rendimiento. Emplea una técnica de aumento en el aprendizaje del modelo de clasificación. AdaRank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que AdaRank puede superar significativamente a los métodos base de BM25, Ranking SVM y RankBoost. 0.49 0.50 0.51 0.52 0.53 prueba 1 prueba 2 prueba 3 prueba 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo se entrena con MAP o NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP número de rondas Figura 11: Curva de aprendizaje de AdaRank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo AdaRank, y evaluaciones empíricas adicionales del algoritmo, incluyendo comparaciones con otros algoritmos que pueden optimizar directamente medidas de rendimiento. AGRADECIMIENTOS Agradecemos a Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias para este artículo. 7. REFERENCIAS [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Lo. Aprendizaje para clasificar con funciones de costo no suaves. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. \n\nMIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificar utilizando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W. Hon. Adaptando el SVM de clasificación para la recuperación de documentos. En SIGIR 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Clasificación de subconjuntos utilizando regresión. En COLT, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Resumen de la pista web TREC 2003. En TREC, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Métodos de aumento para regresión. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire y Y. Cantante. Un algoritmo de refuerzo eficiente para combinar preferencias. Revista de Investigación en Aprendizaje Automático, 4:933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización de la teoría de decisiones del aprendizaje en línea y una aplicación al boosting. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: Una perspectiva estadística del boosting. Los Anales de Estadística, 28(2):337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Aprendizaje de clasificaciones mediante separación de envolvente convexa. En Advances in Neural Information Processing Systems 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, y J. H. Friedman. Los Elementos del Aprendizaje Estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de gran margen para regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, y D. Hickam. Ohsumed: una evaluación interactiva de recuperación y una nueva colección de pruebas grande para la investigación. En SIGIR, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. Métodos de evaluación IR para recuperar documentos altamente relevantes. En SIGIR 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En SIGKDD 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En SIGIR 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rangos para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En SIGIR 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas de pagerank: Trayendo orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Un estudio de propagación de relevancia para la búsqueda en la web. En SIGIR 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Casco. El informe final de la pista de filtrado TREC-9. En TREC, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: Una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire y Y. Cantante. Algoritmos de mejora mejorados utilizando predicciones con calificación de confianza. I'm sorry, but \"Mach\" is not a complete sentence. Could you please provide more context or clarify the sentence you would like me to translate to Spanish? Aprender., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Microsoft Research Asia participó en la pista web y la pista de terabyte de TREC 2004. En TREC, 2004. [28] A. Trotman. Aprendizaje para clasificar. I'm sorry, but I need a complete sentence to provide an accurate translation. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.\nTraducción: Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li y Y. Huang. Aprendizaje sensible al costo de SVM para clasificación. En ECML, páginas 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En SIGIR 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo de SVM para clasificación con aplicación a la recuperación de datos. En SIGKDD 11, páginas 354-363, 2005. APÉNDICE Aquí presentamos la prueba del Teorema 1. PRUEBA. Establezca ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} y φ(t) = 1 2 (1 + ϕ(t)). Según la definición de αt, sabemos que eαt = φ(t) 1−φ(t). ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.\n\nZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Además, si E(π(qi, di, hT), yi) ∈ [−1, +1] entonces, ZT ≤ e−δT minZT−1 m i=1 PT(i) 1+E(π(qi, di, hT), yi) 2 e−αT + 1−E(π(qi, di, hT), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for <br>information retrieval</br> Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of <br>information retrieval</br> and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for <br>information retrieval</br>, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 <br>information retrieval</br> The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for <br>information retrieval</br>) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with <br>information retrieval</br> and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In <br>information retrieval</br>, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern <br>information retrieval</br>.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for <br>information retrieval</br>.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for <br>information retrieval</br>.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for <br>information retrieval</br>.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to <br>information retrieval</br>.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "AdaRank: A Boosting Algorithm for <br>information retrieval</br> Jun Xu Microsoft Research Asia No.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of <br>information retrieval</br> and machine learning.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for <br>information retrieval</br>, referred to as AdaRank.",
                "RELATED WORK 2.1 <br>information retrieval</br> The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for <br>information retrieval</br>) [18, 22] all have parameters to tune."
            ],
            "translated_annotated_samples": [
                "AdaRank: Un algoritmo de aumento para la <br>recuperación de información</br> Jun Xu Microsoft Research Asia No.",
                "INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de <br>recuperación de información</br> y aprendizaje automático.",
                "Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para <br>recuperación de información</br>, denominado AdaRank.",
                "TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada.",
                "Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la <br>recuperación de información</br> Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de boosting, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de <br>recuperación de información</br> y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando boosting, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de boosting para <br>recuperación de información</br>, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "learn to rank": {
            "translated_key": "aprender a clasificar",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "boost": {
            "translated_key": "boosting",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of <br>boost</br>ing, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using <br>boost</br>ing, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a <br>boost</br>ing algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, <br>boost</br>ing, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of <br>boost</br>ing is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is <br>boost</br>ed.",
                "Freund and Schapire have proposed the first well-known <br>boost</br>ing algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a <br>boost</br>ing method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a <br>boost</br>ing technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a <br>boost</br>ing algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a <br>boost</br>ing technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient <br>boost</br>ing algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to <br>boost</br>ing.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of <br>boost</br>ing.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved <br>boost</br>ing algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [
                "To deal with the problem, we propose a novel learning algorithm within the framework of <br>boost</br>ing, which can minimize a loss function directly defined on the performance measures.",
                "Freund et al. [8] take a similar approach and perform the learning by using <br>boost</br>ing, referred to as RankBoost.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a <br>boost</br>ing algorithm for information retrieval, referred to as AdaRank.",
                "They are learning to rank, <br>boost</br>ing, and direct optimization of performance measures.",
                "The basic idea of <br>boost</br>ing is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is <br>boost</br>ed."
            ],
            "translated_annotated_samples": [
                "Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de <br>boosting</br>, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento.",
                "Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando <br>boosting</br>, conocido como RankBoost.",
                "Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de <br>boosting</br> para recuperación de información, denominado AdaRank.",
                "Están aprendiendo a clasificar, <br>potenciar</br> y optimizar directamente las medidas de rendimiento.",
                "La idea básica del <br>boost</br>ing es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado."
            ],
            "translated_text": "AdaRank: Un algoritmo de aumento para la recuperación de información Jun Xu Microsoft Research Asia No. En este documento abordamos el tema del aprendizaje para clasificar en la recuperación de documentos. En la tarea, se crea automáticamente un modelo con algunos datos de entrenamiento y luego se utiliza para clasificar documentos. La bondad de un modelo suele evaluarse con medidas de rendimiento como MAP (Precisión Promedio Media) y NDCG (Ganancia Acumulada Descontada Normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que pudiera optimizar directamente las medidas de rendimiento con respecto a los datos de entrenamiento. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando funciones de pérdida vagamente relacionadas con las medidas de rendimiento. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. Para abordar el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de <br>boosting</br>, que puede minimizar una función de pérdida definida directamente en las medidas de rendimiento. Nuestro algoritmo, conocido como AdaRank, construye repetidamente clasificadores débiles sobre la base de datos de entrenamiento reponderada y finalmente combina linealmente los clasificadores débiles para hacer predicciones de clasificación. Demostramos que el proceso de entrenamiento de AdaRank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que AdaRank supera significativamente a los métodos base de BM25, Ranking SVM y RankBoost. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos Generales Algoritmos, Experimentación, Teoría 1. INTRODUCCIÓN Recientemente, el aprendizaje para clasificar ha ganado cada vez más atención en los campos de recuperación de información y aprendizaje automático. Cuando se aplica a la recuperación de documentos, aprender a clasificar se convierte en una tarea de la siguiente manera. En el entrenamiento, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia proporcionados por humanos. En el ranking, dado una nueva consulta, los documentos recuperados correspondientes son ordenados utilizando el modelo de ranking entrenado. En la recuperación de documentos, generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento como MAP (Precisión Promedio Media) [1] y NDCG (Ganancia Acumulativa Descontada Normalizada) [15]. Idealmente, la función de clasificación se crea de manera que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Se han desarrollado y aplicado varios métodos para aprender a clasificar en la recuperación de documentos. Por ejemplo, Herbrich et al. [13] proponen un algoritmo de aprendizaje para clasificación basado en Máquinas de Vectores de Soporte, llamado Ranking SVM. Freund et al. [8] siguen un enfoque similar y realizan el aprendizaje utilizando <br>boosting</br>, conocido como RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar funciones de pérdida vagamente relacionadas con las medidas de rendimiento de IR, no funciones de pérdida directamente basadas en las medidas. Por ejemplo, Ranking SVM y RankBoost entrenan modelos de ranking minimizando los errores de clasificación en pares de instancias. En este artículo, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de AdaBoost para clasificación [9], proponemos desarrollar un algoritmo de <br>boosting</br> para recuperación de información, denominado AdaRank. AdaRank utiliza una combinación lineal de clasificadores débiles como su modelo. En el aprendizaje, se repite el proceso de volver a ponderar la muestra de entrenamiento, crear un clasificador débil y calcular un peso para el clasificador. Mostramos que el algoritmo AdaRank puede optimizar de forma iterativa una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento de IR. Se proporciona un límite inferior del rendimiento en los datos de entrenamiento, lo que indica que la precisión de clasificación en términos de la medida de rendimiento puede mejorar continuamente durante el proceso de entrenamiento. AdaRank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en el entrenamiento y alta precisión en la clasificación. Los resultados experimentales indican que AdaRank puede superar a los métodos base de BM25, Ranking SVM y RankBoost, en cuatro conjuntos de datos de referencia que incluyen OHSUMED, WSJ, AP y .Gov. Ajustar modelos de clasificación utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en RI [1]. A medida que el número de características en el modelo de clasificación aumenta y la cantidad de datos de entrenamiento crece, el ajuste se vuelve más difícil. Desde el punto de vista de IR, AdaRank puede ser visto como un método de aprendizaje automático para ajuste de modelos de clasificación. Recientemente, la optimización directa de medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y el ranking [5, 19]. AdaRank se puede ver como un método de aprendizaje automático para la optimización directa de medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos en detalle el algoritmo propuesto AdaRank en la Sección 3. Los resultados experimentales y las discusiones se presentan en la Sección 4. La sección 5 concluye este artículo y presenta el trabajo futuro. TRABAJO RELACIONADO 2.1 Recuperación de Información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que pueda ordenar los documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (Modelos de Lenguaje para la Recuperación de Información) [18, 22] tienen todos parámetros para ajustar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y más datos etiquetados están disponibles, ajustar o entrenar los modelos de clasificación resulta ser un problema desafiante. Recientemente, se han aplicado métodos de aprendizaje para clasificar en la construcción de modelos de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica Ranking SVM a la recuperación de documentos. Él utiliza datos de clics para deducir datos de entrenamiento para la creación del modelo. Cao et al. [4] adaptan el Ranking SVM para la recuperación de documentos modificando la función de Pérdida Bisagra para cumplir mejor con los requisitos de RI. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de las consultas con menos documentos recuperados. Burges et al. [3] emplean la Entropía Relativa como función de pérdida y el Descenso de Gradiente como algoritmo para entrenar un modelo de Red Neuronal para clasificación en la recuperación de documentos. El método se conoce como RankNet. 2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, <br>potenciar</br> y optimizar directamente las medidas de rendimiento. Aprender a clasificar es crear automáticamente una función de clasificación que asigna puntuaciones a las instancias y luego clasificar las instancias utilizando esas puntuaciones. Se han propuesto varios enfoques para abordar el problema. Un enfoque principal para aprender a clasificar es transformarlo en una clasificación binaria en pares de instancias. Este enfoque de pares se ajusta bien a la recuperación de información y, por lo tanto, se utiliza ampliamente en IR. Los métodos típicos del enfoque incluyen Ranking SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques de aprendizaje para clasificar, consulte [2, 11, 31]. En el enfoque de clasificación por pares, la tarea de aprendizaje se formaliza como un problema de clasificar pares de instancias en dos categorías (correctamente clasificados e incorrectamente clasificados). De hecho, se sabe que reducir los errores de clasificación en pares de instancias es equivalente a maximizar un límite inferior de MAP [16]. En ese sentido, los métodos existentes de Ranking SVM, RankBoost y RankNet solo pueden minimizar funciones de pérdida que están vagamente relacionadas con las medidas de rendimiento de IR. El boosting es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica del <br>boost</br>ing es construir repetidamente aprendices débiles reponderando los datos de entrenamiento y formar un conjunto de aprendices débiles de tal manera que el rendimiento total del conjunto sea mejorado. ",
            "candidates": [],
            "error": [
                [
                    "boosting",
                    "boosting",
                    "boosting",
                    "potenciar",
                    "boost"
                ]
            ]
        }
    }
}